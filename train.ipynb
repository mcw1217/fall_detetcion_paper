{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "api_url = \"https://notify-api.line.me/api/notify\"\n",
    "token = \"LYy0yPmrqjMc3rmvdQR2WcbCCVZkmFlf6FZBZGEkpYQ\"\n",
    "headers = {'Authorization':'Bearer '+token}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (2756, 30, 335)\n",
      "2 (4406, 30, 335)\n",
      "3 (5999, 30, 335)\n",
      "4 (8410, 30, 335)\n",
      "(10487, 30, 335)\n"
     ]
    }
   ],
   "source": [
    "actions = [\n",
    "    'fall','stand','walking','lie','sit'\n",
    "]\n",
    "data = np.load(\"./dataset/test_vec_dataset/seq_fall-2023-1.npy\")\n",
    "datas = np.load(\"./dataset/test_vec_dataset/seq_stand-2023-1.npy\")\n",
    "datas2 = np.load(\"./dataset/test_vec_dataset/seq_walking-2023-1.npy\")\n",
    "datas3 = np.load(\"./dataset/test_vec_dataset/seq_lie-2023-1.npy\")\n",
    "datas4 = np.load(\"./dataset/test_vec_dataset/seq_sit-2023-1.npy\")\n",
    "for i in range(2,105):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/test_vec_dataset/seq_fall-2023-{i}.npy')\n",
    "    ], axis=0) \n",
    "print(\"1\",data.shape) \n",
    "# for i in range(2,3):\n",
    "#     data = np.concatenate([\n",
    "#         data,\n",
    "#         np.load(f'./dataset/test_vec_dataset/seq_stand-2023-{i}.npy')\n",
    "#     ], axis=0)\n",
    "for i in range(2,3):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/test_vec_dataset/seq_walking-2023-{i}.npy')\n",
    "    ], axis=0)\n",
    "print(\"2\",data.shape) \n",
    "for i in range(2,3):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/test_vec_dataset/seq_lie-2023-{i}.npy')\n",
    "    ], axis=0) \n",
    "print(\"3\",data.shape) \n",
    "for i in range(2,48):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/test_vec_dataset/seq_sit-2023-{i}.npy')\n",
    "    ], axis=0) \n",
    "print(\"4\",data.shape) \n",
    "        \n",
    "data = np.concatenate([data,datas2,datas3,datas4])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load(\"dataset/seq_fall-2023-1.npy\")\n",
    "# datas = np.load(\"dataset/seq_stand-2023-1.npy\")\n",
    "# datas2 = np.load(\"dataset/seq_walking-2023-1.npy\")\n",
    "# datas3 = np.load(\"dataset/seq_lie-2023-1.npy\")\n",
    "\n",
    "\n",
    "# for i in range(2,60):\n",
    "#     data = np.concatenate([\n",
    "#         data,\n",
    "#         np.load(f'dataset/seq_fall-2023-{i}.npy')\n",
    "#     ], axis=0)  \n",
    "# print(data.shape)\n",
    "\n",
    "# # fall = 1569\n",
    "# # walking = 2428\n",
    "# # lie = 5710\n",
    "# for i in range(2,5):\n",
    "#     datas3 = np.concatenate([\n",
    "#         datas3,\n",
    "#         np.load(f'dataset/seq_lie-2023-{i}.npy')\n",
    "#     ], axis=0) \n",
    "\n",
    "# print(datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(data[9000:11000,0,-1])\n",
    "# print(data.shape)\n",
    "data = np.nan_to_num(data)\n",
    "\n",
    "\n",
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "# print(x_data.shape)\n",
    "# print(x_data[0,0,:])\n",
    "# print(labels.shape)\n",
    "# print(labels[5000:10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10487, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8389, 30, 334) (8389, 5)\n",
      "(2098, 30, 334) (2098, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2,random_state=2022)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6292, 30, 334) (6292, 5)\n",
      "(4195, 30, 334) (4195, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=5213)\n",
    "\n",
    "for train_idx, test_idx in split.split(x_data,y_data):\n",
    "    x_train = x_data[train_idx]\n",
    "    y_train = y_data[train_idx]\n",
    "    x_val = x_data[test_idx]\n",
    "    y_val = y_data[test_idx]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 334)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_2 (GRU)                 (None, 30)                32940     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               3968      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,921\n",
      "Trainable params: 47,665\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(30, dropout=0.2, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.8041 - acc: 0.6904\n",
      "Epoch 1: val_loss improved from inf to 0.44183, saving model to models\\model.h5\n",
      "197/197 [==============================] - 3s 10ms/step - loss: 0.7971 - acc: 0.6936 - val_loss: 0.4418 - val_acc: 0.8448 - lr: 0.0010\n",
      "Epoch 2/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.3379 - acc: 0.8732\n",
      "Epoch 2: val_loss improved from 0.44183 to 0.33413, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.3354 - acc: 0.8736 - val_loss: 0.3341 - val_acc: 0.8746 - lr: 0.0010\n",
      "Epoch 3/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.1753 - acc: 0.9367\n",
      "Epoch 3: val_loss improved from 0.33413 to 0.19613, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.1753 - acc: 0.9367 - val_loss: 0.1961 - val_acc: 0.9263 - lr: 0.0010\n",
      "Epoch 4/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9603\n",
      "Epoch 4: val_loss improved from 0.19613 to 0.12672, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.1135 - acc: 0.9604 - val_loss: 0.1267 - val_acc: 0.9523 - lr: 0.0010\n",
      "Epoch 5/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0639 - acc: 0.9777\n",
      "Epoch 5: val_loss improved from 0.12672 to 0.06730, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0639 - acc: 0.9777 - val_loss: 0.0673 - val_acc: 0.9759 - lr: 0.0010\n",
      "Epoch 6/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9834\n",
      "Epoch 6: val_loss did not improve from 0.06730\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0467 - acc: 0.9833 - val_loss: 0.1119 - val_acc: 0.9723 - lr: 0.0010\n",
      "Epoch 7/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9845\n",
      "Epoch 7: val_loss improved from 0.06730 to 0.02030, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0459 - acc: 0.9849 - val_loss: 0.0203 - val_acc: 0.9924 - lr: 0.0010\n",
      "Epoch 8/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9909\n",
      "Epoch 8: val_loss did not improve from 0.02030\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0262 - acc: 0.9909 - val_loss: 0.0488 - val_acc: 0.9833 - lr: 0.0010\n",
      "Epoch 9/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9904\n",
      "Epoch 9: val_loss did not improve from 0.02030\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0300 - acc: 0.9905 - val_loss: 0.0221 - val_acc: 0.9931 - lr: 0.0010\n",
      "Epoch 10/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9901\n",
      "Epoch 10: val_loss did not improve from 0.02030\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0276 - acc: 0.9901 - val_loss: 0.0298 - val_acc: 0.9909 - lr: 0.0010\n",
      "Epoch 11/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9933\n",
      "Epoch 11: val_loss improved from 0.02030 to 0.00862, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0197 - acc: 0.9935 - val_loss: 0.0086 - val_acc: 0.9979 - lr: 0.0010\n",
      "Epoch 12/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9974\n",
      "Epoch 12: val_loss improved from 0.00862 to 0.00610, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.0061 - val_acc: 0.9979 - lr: 0.0010\n",
      "Epoch 13/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0132 - acc: 0.9955\n",
      "Epoch 13: val_loss did not improve from 0.00610\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0132 - acc: 0.9955 - val_loss: 0.0131 - val_acc: 0.9957 - lr: 0.0010\n",
      "Epoch 14/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9905\n",
      "Epoch 14: val_loss did not improve from 0.00610\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0101 - val_acc: 0.9964 - lr: 0.0010\n",
      "Epoch 15/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0144 - acc: 0.9959\n",
      "Epoch 15: val_loss did not improve from 0.00610\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0144 - acc: 0.9959 - val_loss: 0.0068 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 16/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9968\n",
      "Epoch 16: val_loss did not improve from 0.00610\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0107 - acc: 0.9968 - val_loss: 0.0099 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 17/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9966\n",
      "Epoch 17: val_loss did not improve from 0.00610\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0097 - acc: 0.9967 - val_loss: 0.0162 - val_acc: 0.9945 - lr: 0.0010\n",
      "Epoch 18/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9970\n",
      "Epoch 18: val_loss improved from 0.00610 to 0.00564, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0056 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 19/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0141 - acc: 0.9952\n",
      "Epoch 19: val_loss did not improve from 0.00564\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0141 - acc: 0.9952 - val_loss: 0.0090 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 20/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0136 - acc: 0.9955\n",
      "Epoch 20: val_loss did not improve from 0.00564\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0246 - val_acc: 0.9938 - lr: 0.0010\n",
      "Epoch 21/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9953\n",
      "Epoch 21: val_loss did not improve from 0.00564\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0128 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 22/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9903\n",
      "Epoch 22: val_loss did not improve from 0.00564\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0322 - acc: 0.9903 - val_loss: 0.0129 - val_acc: 0.9959 - lr: 0.0010\n",
      "Epoch 23/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9957\n",
      "Epoch 23: val_loss did not improve from 0.00564\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 0.0134 - acc: 0.9957 - val_loss: 0.0098 - val_acc: 0.9969 - lr: 0.0010\n",
      "Epoch 24/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9956\n",
      "Epoch 24: val_loss did not improve from 0.00564\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0150 - acc: 0.9957 - val_loss: 0.0075 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 25/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 25: val_loss did not improve from 0.00564\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0158 - acc: 0.9932 - val_loss: 0.0248 - val_acc: 0.9931 - lr: 0.0010\n",
      "Epoch 26/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9979\n",
      "Epoch 26: val_loss did not improve from 0.00564\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0919 - val_acc: 0.9778 - lr: 0.0010\n",
      "Epoch 27/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9951\n",
      "Epoch 27: val_loss improved from 0.00564 to 0.00325, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0108 - acc: 0.9952 - val_loss: 0.0032 - val_acc: 0.9986 - lr: 0.0010\n",
      "Epoch 28/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9961\n",
      "Epoch 28: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 0.0141 - acc: 0.9962 - val_loss: 0.0111 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 29/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9974\n",
      "Epoch 29: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0089 - acc: 0.9975 - val_loss: 0.0033 - val_acc: 0.9986 - lr: 0.0010\n",
      "Epoch 30/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9960\n",
      "Epoch 30: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0099 - acc: 0.9960 - val_loss: 0.0055 - val_acc: 0.9983 - lr: 0.0010\n",
      "Epoch 31/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0141 - acc: 0.9951\n",
      "Epoch 31: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0140 - acc: 0.9951 - val_loss: 0.0219 - val_acc: 0.9967 - lr: 0.0010\n",
      "Epoch 32/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9969\n",
      "Epoch 32: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 0.0078 - acc: 0.9970 - val_loss: 0.0146 - val_acc: 0.9979 - lr: 0.0010\n",
      "Epoch 33/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9940\n",
      "Epoch 33: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0218 - acc: 0.9941 - val_loss: 0.0238 - val_acc: 0.9979 - lr: 0.0010\n",
      "Epoch 34/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9972\n",
      "Epoch 34: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0068 - acc: 0.9973 - val_loss: 0.0216 - val_acc: 0.9995 - lr: 0.0010\n",
      "Epoch 35/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9982\n",
      "Epoch 35: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0139 - val_acc: 0.9988 - lr: 0.0010\n",
      "Epoch 36/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 36: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 0.0259 - val_acc: 0.9957 - lr: 0.0010\n",
      "Epoch 37/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0106 - acc: 0.9964\n",
      "Epoch 37: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0249 - val_acc: 0.9938 - lr: 0.0010\n",
      "Epoch 38/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0157 - acc: 0.9949\n",
      "Epoch 38: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0154 - acc: 0.9951 - val_loss: 0.0138 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 39/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0100 - acc: 0.9967\n",
      "Epoch 39: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0116 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 40/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 40: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0117 - val_acc: 0.9995 - lr: 0.0010\n",
      "Epoch 41/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9984\n",
      "Epoch 41: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0074 - acc: 0.9983 - val_loss: 0.3550 - val_acc: 0.9366 - lr: 0.0010\n",
      "Epoch 42/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0244 - acc: 0.9928\n",
      "Epoch 42: val_loss did not improve from 0.00325\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0244 - acc: 0.9928 - val_loss: 0.0084 - val_acc: 0.9993 - lr: 0.0010\n",
      "Epoch 43/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 43: val_loss improved from 0.00325 to 0.00189, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0019 - val_acc: 0.9995 - lr: 0.0010\n",
      "Epoch 44/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9982\n",
      "Epoch 44: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0074 - val_acc: 0.9979 - lr: 0.0010\n",
      "Epoch 45/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9986\n",
      "Epoch 45: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.0253 - val_acc: 0.9950 - lr: 0.0010\n",
      "Epoch 46/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9981\n",
      "Epoch 46: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0199 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 47/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9974\n",
      "Epoch 47: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0081 - acc: 0.9975 - val_loss: 0.0455 - val_acc: 0.9907 - lr: 0.0010\n",
      "Epoch 48/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0104 - acc: 0.9972\n",
      "Epoch 48: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0102 - acc: 0.9973 - val_loss: 0.0623 - val_acc: 0.9909 - lr: 0.0010\n",
      "Epoch 49/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9971\n",
      "Epoch 49: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0072 - acc: 0.9971 - val_loss: 0.0252 - val_acc: 0.9995 - lr: 0.0010\n",
      "Epoch 50/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 50: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0237 - val_acc: 0.9993 - lr: 0.0010\n",
      "Epoch 51/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9957\n",
      "Epoch 51: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1296 - val_acc: 0.9595 - lr: 0.0010\n",
      "Epoch 52/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 52: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0066 - acc: 0.9984 - val_loss: 0.0263 - val_acc: 0.9940 - lr: 0.0010\n",
      "Epoch 53/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9992\n",
      "Epoch 53: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0118 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 54/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9984\n",
      "Epoch 54: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0184 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 55/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9990\n",
      "Epoch 55: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0023 - acc: 0.9990 - val_loss: 0.0227 - val_acc: 0.9952 - lr: 0.0010\n",
      "Epoch 56/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 56: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0169 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 57/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 57: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0116 - val_acc: 0.9983 - lr: 0.0010\n",
      "Epoch 58/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9987\n",
      "Epoch 58: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0130 - val_acc: 0.9993 - lr: 0.0010\n",
      "Epoch 59/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9988\n",
      "Epoch 59: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0060 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 60/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992\n",
      "Epoch 60: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0299 - val_acc: 0.9936 - lr: 0.0010\n",
      "Epoch 61/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9925\n",
      "Epoch 61: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0279 - acc: 0.9927 - val_loss: 0.0279 - val_acc: 0.9936 - lr: 0.0010\n",
      "Epoch 62/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9969\n",
      "Epoch 62: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0308 - val_acc: 0.9914 - lr: 0.0010\n",
      "Epoch 63/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 63: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0251 - val_acc: 0.9914 - lr: 0.0010\n",
      "Epoch 64/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 64: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0020 - val_acc: 0.9993 - lr: 0.0010\n",
      "Epoch 65/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9969\n",
      "Epoch 65: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0088 - acc: 0.9968 - val_loss: 0.0265 - val_acc: 0.9897 - lr: 0.0010\n",
      "Epoch 66/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0148 - acc: 0.9951\n",
      "Epoch 66: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0143 - acc: 0.9952 - val_loss: 0.0690 - val_acc: 0.9855 - lr: 0.0010\n",
      "Epoch 67/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9969\n",
      "Epoch 67: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0061 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 68/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9964\n",
      "Epoch 68: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0126 - acc: 0.9965 - val_loss: 0.0100 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 69/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9971\n",
      "Epoch 69: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0096 - acc: 0.9971 - val_loss: 0.0025 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 70/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9992\n",
      "Epoch 70: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 0.0037 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 71/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9990\n",
      "Epoch 71: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0023 - acc: 0.9990 - val_loss: 0.0167 - val_acc: 0.9957 - lr: 0.0010\n",
      "Epoch 72/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9964\n",
      "Epoch 72: val_loss did not improve from 0.00189\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0028 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 73/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9992\n",
      "Epoch 73: val_loss improved from 0.00189 to 0.00058, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 5.7608e-04 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 74/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 74: val_loss improved from 0.00058 to 0.00037, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 3.7461e-04 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 75/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9995\n",
      "Epoch 75: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.0249 - val_acc: 0.9933 - lr: 0.0010\n",
      "Epoch 76/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
      "Epoch 76: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0074 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 77/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9963\n",
      "Epoch 77: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0162 - acc: 0.9963 - val_loss: 0.0052 - val_acc: 0.9993 - lr: 0.0010\n",
      "Epoch 78/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 78: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0012 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 79/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 79: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 4.6424e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 80/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9975\n",
      "Epoch 80: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0105 - acc: 0.9976 - val_loss: 4.4067e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 81/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 81: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0017 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 82/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9987\n",
      "Epoch 82: val_loss improved from 0.00037 to 0.00015, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 1.5034e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 83/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9990\n",
      "Epoch 83: val_loss did not improve from 0.00015\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0397 - val_acc: 0.9895 - lr: 0.0010\n",
      "Epoch 84/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 84: val_loss did not improve from 0.00015\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0081 - acc: 0.9978 - val_loss: 0.0198 - val_acc: 0.9952 - lr: 0.0010\n",
      "Epoch 85/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0078 - acc: 0.9982\n",
      "Epoch 85: val_loss did not improve from 0.00015\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0075 - acc: 0.9983 - val_loss: 3.0799e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 86/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.6369e-04 - acc: 0.9998\n",
      "Epoch 86: val_loss did not improve from 0.00015\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.4524e-04 - acc: 0.9998 - val_loss: 3.0241e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 87/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.7349e-04 - acc: 1.0000\n",
      "Epoch 87: val_loss did not improve from 0.00015\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.4596e-04 - acc: 0.9998 - val_loss: 0.0014 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 88/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0173 - acc: 0.9951\n",
      "Epoch 88: val_loss did not improve from 0.00015\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 0.0173 - acc: 0.9951 - val_loss: 0.0057 - val_acc: 0.9983 - lr: 0.0010\n",
      "Epoch 89/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9979\n",
      "Epoch 89: val_loss did not improve from 0.00015\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0078 - acc: 0.9978 - val_loss: 3.8860e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 90/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9980\n",
      "Epoch 90: val_loss did not improve from 0.00015\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0045 - val_acc: 0.9995 - lr: 0.0010\n",
      "Epoch 91/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 91: val_loss improved from 0.00015 to 0.00009, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 9.3395e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 92/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 5.6245e-04 - acc: 0.9998\n",
      "Epoch 92: val_loss did not improve from 0.00009\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.5817e-04 - acc: 0.9998 - val_loss: 4.2750e-04 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 93/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 93: val_loss improved from 0.00009 to 0.00006, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 5.5117e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 94/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 94: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 4.0417e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 95/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0072 - acc: 0.9985\n",
      "Epoch 95: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0070 - acc: 0.9986 - val_loss: 0.0020 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 96/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 96: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 1.8837e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 97/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9953\n",
      "Epoch 97: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0091 - val_acc: 0.9976 - lr: 0.0010\n",
      "Epoch 98/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 98: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.0048 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 99/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9984\n",
      "Epoch 99: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 0.0084 - acc: 0.9984 - val_loss: 0.0146 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 100/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9995\n",
      "Epoch 100: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0032 - acc: 0.9995 - val_loss: 0.0012 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 101/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 6.7315e-04 - acc: 0.9998\n",
      "Epoch 101: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.5933e-04 - acc: 0.9998 - val_loss: 0.0023 - val_acc: 0.9986 - lr: 0.0010\n",
      "Epoch 102/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9980\n",
      "Epoch 102: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.0168 - val_acc: 0.9964 - lr: 0.0010\n",
      "Epoch 103/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 103: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0023 - val_acc: 0.9995 - lr: 0.0010\n",
      "Epoch 104/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9992\n",
      "Epoch 104: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 6.5129e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 105/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.5690e-04 - acc: 0.9998\n",
      "Epoch 105: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.3609e-04 - acc: 0.9998 - val_loss: 0.0021 - val_acc: 0.9993 - lr: 0.0010\n",
      "Epoch 106/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.8706e-04 - acc: 0.9995\n",
      "Epoch 106: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.6441e-04 - acc: 0.9995 - val_loss: 0.2568 - val_acc: 0.9647 - lr: 0.0010\n",
      "Epoch 107/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 8.4044e-04 - acc: 0.9998\n",
      "Epoch 107: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.1773e-04 - acc: 0.9998 - val_loss: 0.0033 - val_acc: 0.9993 - lr: 0.0010\n",
      "Epoch 108/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.4142e-04 - acc: 1.0000\n",
      "Epoch 108: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3891e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 109/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9966\n",
      "Epoch 109: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0130 - acc: 0.9967 - val_loss: 0.0230 - val_acc: 0.9938 - lr: 0.0010\n",
      "Epoch 110/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0152 - acc: 0.9952\n",
      "Epoch 110: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0148 - acc: 0.9954 - val_loss: 0.0023 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 111/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9961\n",
      "Epoch 111: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0017 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 112/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992\n",
      "Epoch 112: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 8.4860e-04 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 113/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0011 - acc: 0.9995\n",
      "Epoch 113: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.0014 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 114/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0045 - acc: 0.9984\n",
      "Epoch 114: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0044 - acc: 0.9984 - val_loss: 5.6877e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 115/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9995\n",
      "Epoch 115: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0010 - acc: 0.9995 - val_loss: 1.3357e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 116/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9992\n",
      "Epoch 116: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0016 - acc: 0.9992 - val_loss: 7.4980e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 117/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9958\n",
      "Epoch 117: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0339 - val_acc: 0.9936 - lr: 0.0010\n",
      "Epoch 118/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0184 - acc: 0.9946\n",
      "Epoch 118: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0189 - acc: 0.9946 - val_loss: 0.0147 - val_acc: 0.9931 - lr: 0.0010\n",
      "Epoch 119/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 119: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.0121 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 120/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9992\n",
      "Epoch 120: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0084 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 121/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 9.7622e-04 - acc: 0.9997\n",
      "Epoch 121: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.7123e-04 - acc: 0.9997 - val_loss: 0.0102 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 122/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 3.3849e-04 - acc: 0.9998\n",
      "Epoch 122: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.1741e-04 - acc: 0.9998 - val_loss: 0.0142 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 123/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.1500e-04 - acc: 1.0000\n",
      "Epoch 123: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1400e-04 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 124/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 6.7250e-04 - acc: 0.9998\n",
      "Epoch 124: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.5459e-04 - acc: 0.9998 - val_loss: 0.0066 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 125/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.0232e-04 - acc: 1.0000\n",
      "Epoch 125: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 1.9924e-04 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 0.9998 - lr: 0.0010\n",
      "Epoch 126/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 5.6292e-04 - acc: 0.9998\n",
      "Epoch 126: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 5.6118e-04 - acc: 0.9998 - val_loss: 0.0148 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 127/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9976\n",
      "Epoch 127: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0117 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 128/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9964\n",
      "Epoch 128: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0020 - val_acc: 0.9988 - lr: 0.0010\n",
      "Epoch 129/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 129: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 2.6415e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 130/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9990\n",
      "Epoch 130: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0026 - acc: 0.9990 - val_loss: 5.7474e-04 - val_acc: 0.9995 - lr: 0.0010\n",
      "Epoch 131/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 9.2870e-04 - acc: 0.9997\n",
      "Epoch 131: val_loss did not improve from 0.00006\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.2870e-04 - acc: 0.9997 - val_loss: 0.0022 - val_acc: 0.9993 - lr: 0.0010\n",
      "Epoch 132/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9990\n",
      "Epoch 132: val_loss did not improve from 0.00006\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0157 - val_acc: 0.9943 - lr: 0.0010\n",
      "Epoch 133/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0029 - acc: 0.9987\n",
      "Epoch 133: val_loss improved from 0.00006 to 0.00003, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 3.2445e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 134/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 134: val_loss did not improve from 0.00003\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 4.2657e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 135/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.6508e-04 - acc: 0.9998\n",
      "Epoch 135: val_loss did not improve from 0.00003\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.6394e-04 - acc: 0.9998 - val_loss: 4.9253e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 136/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 4.3197e-04 - acc: 0.9998\n",
      "Epoch 136: val_loss did not improve from 0.00003\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.1600e-04 - acc: 0.9998 - val_loss: 1.3626e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 137/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.0506e-04 - acc: 0.9998\n",
      "Epoch 137: val_loss did not improve from 0.00003\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0238e-04 - acc: 0.9998 - val_loss: 7.5179e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 138/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9994\n",
      "Epoch 138: val_loss improved from 0.00003 to 0.00003, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 2.5542e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 139/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 139: val_loss did not improve from 0.00003\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0109 - val_acc: 0.9959 - lr: 5.0000e-04\n",
      "Epoch 140/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.4172e-04 - acc: 0.9998\n",
      "Epoch 140: val_loss improved from 0.00003 to 0.00002, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 6.2512e-04 - acc: 0.9998 - val_loss: 1.5292e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 141/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.4330e-04 - acc: 0.9998\n",
      "Epoch 141: val_loss did not improve from 0.00002\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.3964e-04 - acc: 0.9998 - val_loss: 0.0081 - val_acc: 0.9969 - lr: 5.0000e-04\n",
      "Epoch 142/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 9.4359e-04 - acc: 0.9997\n",
      "Epoch 142: val_loss did not improve from 0.00002\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.4359e-04 - acc: 0.9997 - val_loss: 1.8638e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 143/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 5.5212e-04 - acc: 0.9998\n",
      "Epoch 143: val_loss did not improve from 0.00002\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.4771e-04 - acc: 0.9998 - val_loss: 2.3645e-04 - val_acc: 0.9998 - lr: 5.0000e-04\n",
      "Epoch 144/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 7.6700e-04 - acc: 0.9997\n",
      "Epoch 144: val_loss improved from 0.00002 to 0.00001, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.6071e-04 - acc: 0.9997 - val_loss: 9.6956e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 145/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.6752e-04 - acc: 1.0000\n",
      "Epoch 145: val_loss did not improve from 0.00001\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6589e-04 - acc: 1.0000 - val_loss: 1.4592e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 146/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.1797e-04 - acc: 1.0000\n",
      "Epoch 146: val_loss improved from 0.00001 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 2.1192e-04 - acc: 1.0000 - val_loss: 3.2631e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 147/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 147: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 3.9673e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 148/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.1526e-04 - acc: 0.9998\n",
      "Epoch 148: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1327e-04 - acc: 0.9998 - val_loss: 5.4354e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 149/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.5337e-05 - acc: 1.0000\n",
      "Epoch 149: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.8252e-05 - acc: 1.0000 - val_loss: 1.8357e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 150/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 7.1671e-05 - acc: 1.0000\n",
      "Epoch 150: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.0442e-05 - acc: 1.0000 - val_loss: 7.4178e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 151/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 8.1170e-05 - acc: 1.0000\n",
      "Epoch 151: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.0552e-05 - acc: 1.0000 - val_loss: 1.0063e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 152/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 3.0912e-05 - acc: 1.0000\n",
      "Epoch 152: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.0506e-05 - acc: 1.0000 - val_loss: 3.9356e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 153/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.3050e-04 - acc: 1.0000\n",
      "Epoch 153: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2703e-04 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 0.9976 - lr: 5.0000e-04\n",
      "Epoch 154/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 5.4947e-05 - acc: 1.0000\n",
      "Epoch 154: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.3696e-05 - acc: 1.0000 - val_loss: 1.1100e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 155/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.2591e-04 - acc: 0.9998\n",
      "Epoch 155: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2293e-04 - acc: 0.9998 - val_loss: 0.0033 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 156/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 3.5773e-04 - acc: 0.9998\n",
      "Epoch 156: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 3.5114e-04 - acc: 0.9998 - val_loss: 4.9557e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 157/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.2540e-04 - acc: 1.0000\n",
      "Epoch 157: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2197e-04 - acc: 1.0000 - val_loss: 1.3270e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 158/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.3005e-05 - acc: 1.0000\n",
      "Epoch 158: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.2656e-05 - acc: 1.0000 - val_loss: 3.2285e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 159/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.4937e-05 - acc: 1.0000\n",
      "Epoch 159: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.4567e-05 - acc: 1.0000 - val_loss: 6.3405e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 160/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9989\n",
      "Epoch 160: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0061 - acc: 0.9989 - val_loss: 5.7644e-04 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 161/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.8787e-04 - acc: 0.9998\n",
      "Epoch 161: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.8570e-04 - acc: 0.9998 - val_loss: 4.8017e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 162/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9993\n",
      "Epoch 162: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 2.8851e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 163/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.1925e-04 - acc: 0.9998\n",
      "Epoch 163: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1522e-04 - acc: 0.9998 - val_loss: 2.7000e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 164/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.1473e-04 - acc: 1.0000\n",
      "Epoch 164: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1473e-04 - acc: 1.0000 - val_loss: 5.7552e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 165/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.9418e-05 - acc: 1.0000\n",
      "Epoch 165: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.9418e-05 - acc: 1.0000 - val_loss: 5.9886e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 166/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.8129e-04 - acc: 0.9998\n",
      "Epoch 166: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.8008e-04 - acc: 0.9998 - val_loss: 1.5158e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 167/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 167: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 2.1779e-04 - val_acc: 0.9998 - lr: 5.0000e-04\n",
      "Epoch 168/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.4184e-04 - acc: 1.0000\n",
      "Epoch 168: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3502e-04 - acc: 1.0000 - val_loss: 8.7003e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 169/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.3400e-04 - acc: 1.0000\n",
      "Epoch 169: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3092e-04 - acc: 1.0000 - val_loss: 9.2629e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 170/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 5.0854e-05 - acc: 1.0000\n",
      "Epoch 170: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.0692e-05 - acc: 1.0000 - val_loss: 8.6370e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 171/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.5130e-05 - acc: 1.0000\n",
      "Epoch 171: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5183e-05 - acc: 1.0000 - val_loss: 6.2327e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 172/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.0331e-05 - acc: 1.0000\n",
      "Epoch 172: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9736e-05 - acc: 1.0000 - val_loss: 7.1480e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 173/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.6166e-05 - acc: 1.0000\n",
      "Epoch 173: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 1.6115e-05 - acc: 1.0000 - val_loss: 5.1898e-06 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 174/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 174: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 1.4600e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 175/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9990\n",
      "Epoch 175: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0754 - val_acc: 0.9907 - lr: 5.0000e-04\n",
      "Epoch 176/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9978\n",
      "Epoch 176: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0107 - acc: 0.9978 - val_loss: 0.0017 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 177/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997   \n",
      "Epoch 177: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0020 - acc: 0.9997 - val_loss: 0.0037 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 178/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 178: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 2.9033e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 179/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 179: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 3.3540e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 180/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 7.8020e-04 - acc: 0.9998\n",
      "Epoch 180: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.8020e-04 - acc: 0.9998 - val_loss: 3.5405e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 181/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 7.9099e-05 - acc: 1.0000\n",
      "Epoch 181: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.0572e-05 - acc: 1.0000 - val_loss: 3.6967e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 182/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 182: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 5.0724e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 183/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.7836e-04 - acc: 0.9997\n",
      "Epoch 183: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.7456e-04 - acc: 0.9997 - val_loss: 1.7387e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 184/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.2807e-04 - acc: 1.0000\n",
      "Epoch 184: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2657e-04 - acc: 1.0000 - val_loss: 1.6439e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 185/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 5.8218e-05 - acc: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.6434e-05 - acc: 1.0000 - val_loss: 1.4861e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 186/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 2.9941e-04 - acc: 0.9998\n",
      "Epoch 186: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.9747e-04 - acc: 0.9998 - val_loss: 1.8287e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 187/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 187: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 5.1061e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 188/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.1626e-04 - acc: 0.9998\n",
      "Epoch 188: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.9615e-04 - acc: 0.9998 - val_loss: 7.5673e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 189/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 6.8267e-04 - acc: 0.9997\n",
      "Epoch 189: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.6404e-04 - acc: 0.9997 - val_loss: 8.3532e-04 - val_acc: 0.9998 - lr: 2.5000e-04\n",
      "Epoch 190/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.7545e-04 - acc: 0.9998\n",
      "Epoch 190: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6779e-04 - acc: 0.9998 - val_loss: 0.0019 - val_acc: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 191/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 191: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0022 - acc: 0.9997 - val_loss: 6.6513e-04 - val_acc: 0.9998 - lr: 2.5000e-04\n",
      "Epoch 192/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9997\n",
      "Epoch 192: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0023 - acc: 0.9997 - val_loss: 5.2739e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 193/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.9293e-04 - acc: 0.9998\n",
      "Epoch 193: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8714e-04 - acc: 0.9998 - val_loss: 4.1175e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 194/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.1485e-05 - acc: 1.0000\n",
      "Epoch 194: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.3128e-05 - acc: 1.0000 - val_loss: 2.0514e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 195/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.4911e-05 - acc: 1.0000\n",
      "Epoch 195: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5986e-05 - acc: 1.0000 - val_loss: 3.6996e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 196/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.7860e-05 - acc: 1.0000\n",
      "Epoch 196: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7860e-05 - acc: 1.0000 - val_loss: 5.0052e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 197/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.7839e-05 - acc: 1.0000\n",
      "Epoch 197: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.7839e-05 - acc: 1.0000 - val_loss: 1.9642e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 198/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 4.8279e-05 - acc: 1.0000\n",
      "Epoch 198: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.7329e-05 - acc: 1.0000 - val_loss: 9.4272e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 199/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 5.3594e-04 - acc: 0.9997\n",
      "Epoch 199: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.1876e-04 - acc: 0.9997 - val_loss: 5.5764e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 200/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 6.5296e-05 - acc: 1.0000\n",
      "Epoch 200: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.4150e-05 - acc: 1.0000 - val_loss: 9.2102e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 201/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.3398e-04 - acc: 0.9998\n",
      "Epoch 201: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3398e-04 - acc: 0.9998 - val_loss: 3.0014e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 202/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 4.7411e-05 - acc: 1.0000\n",
      "Epoch 202: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.7410e-05 - acc: 1.0000 - val_loss: 1.5340e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 203/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 6.5962e-05 - acc: 1.0000\n",
      "Epoch 203: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.9342e-05 - acc: 1.0000 - val_loss: 1.4283e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 204/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.0852e-05 - acc: 1.0000\n",
      "Epoch 204: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0539e-05 - acc: 1.0000 - val_loss: 7.0643e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 205/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.6980e-05 - acc: 1.0000\n",
      "Epoch 205: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6923e-05 - acc: 1.0000 - val_loss: 6.1788e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 206/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 206: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 3.5850e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 207/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.9555e-04 - acc: 1.0000\n",
      "Epoch 207: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9213e-04 - acc: 1.0000 - val_loss: 4.9325e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 208/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 208: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 1.7891e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 209/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.7481e-05 - acc: 1.0000\n",
      "Epoch 209: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7088e-05 - acc: 1.0000 - val_loss: 1.0846e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 210/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.1278e-05 - acc: 1.0000\n",
      "Epoch 210: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.0263e-05 - acc: 1.0000 - val_loss: 1.2147e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 211/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.3669e-05 - acc: 1.0000\n",
      "Epoch 211: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3430e-05 - acc: 1.0000 - val_loss: 9.9902e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 212/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.6336e-05 - acc: 1.0000\n",
      "Epoch 212: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6100e-05 - acc: 1.0000 - val_loss: 8.0392e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 213/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.2834e-05 - acc: 1.0000\n",
      "Epoch 213: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2625e-05 - acc: 1.0000 - val_loss: 6.3063e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 214/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 4.0549e-05 - acc: 1.0000\n",
      "Epoch 214: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.9855e-05 - acc: 1.0000 - val_loss: 8.0599e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 215/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 6.1137e-04 - acc: 0.9998\n",
      "Epoch 215: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.9414e-04 - acc: 0.9998 - val_loss: 7.5670e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 216/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 7.2282e-04 - acc: 0.9997\n",
      "Epoch 216: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.1685e-04 - acc: 0.9997 - val_loss: 1.0168e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 217/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.4884e-04 - acc: 0.9998\n",
      "Epoch 217: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4393e-04 - acc: 0.9998 - val_loss: 1.1856e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 218/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.8061e-05 - acc: 1.0000\n",
      "Epoch 218: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7674e-05 - acc: 1.0000 - val_loss: 8.2051e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 219/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.1658e-04 - acc: 1.0000\n",
      "Epoch 219: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1398e-04 - acc: 1.0000 - val_loss: 1.4131e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 220/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 6.3909e-05 - acc: 1.0000\n",
      "Epoch 220: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.3080e-05 - acc: 1.0000 - val_loss: 2.1617e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 221/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 2.2143e-05 - acc: 1.0000\n",
      "Epoch 221: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1994e-05 - acc: 1.0000 - val_loss: 2.7475e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 222/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.5068e-05 - acc: 1.0000\n",
      "Epoch 222: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4799e-05 - acc: 1.0000 - val_loss: 2.4680e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 223/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9998  \n",
      "Epoch 223: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0030 - acc: 0.9998 - val_loss: 6.3179e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 224/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 224: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0027 - acc: 0.9995 - val_loss: 2.0104e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 225/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 5.0614e-05 - acc: 1.0000\n",
      "Epoch 225: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.0156e-05 - acc: 1.0000 - val_loss: 1.0843e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 226/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 5.1722e-05 - acc: 1.0000\n",
      "Epoch 226: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.1455e-05 - acc: 1.0000 - val_loss: 6.0303e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 227/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 5.4185e-06 - acc: 1.0000\n",
      "Epoch 227: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.3750e-06 - acc: 1.0000 - val_loss: 6.0205e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 228/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.6751e-05 - acc: 1.0000\n",
      "Epoch 228: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6751e-05 - acc: 1.0000 - val_loss: 6.5094e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 229/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 6.7844e-06 - acc: 1.0000\n",
      "Epoch 229: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.5966e-06 - acc: 1.0000 - val_loss: 6.3546e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 230/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9993\n",
      "Epoch 230: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0049 - acc: 0.9994 - val_loss: 1.3020e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 231/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 4.3900e-04 - acc: 0.9997\n",
      "Epoch 231: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.3091e-04 - acc: 0.9997 - val_loss: 0.0019 - val_acc: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 232/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 5.4718e-05 - acc: 1.0000\n",
      "Epoch 232: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.3328e-05 - acc: 1.0000 - val_loss: 2.9581e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 233/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.8070e-05 - acc: 1.0000\n",
      "Epoch 233: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 233: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 5.8070e-05 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 234/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.3336e-05 - acc: 1.0000\n",
      "Epoch 234: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 2.2929e-05 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9990 - lr: 1.2500e-04\n",
      "Epoch 235/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 4.4831e-05 - acc: 1.0000\n",
      "Epoch 235: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 4.4049e-05 - acc: 1.0000 - val_loss: 4.5152e-04 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 236/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.1295e-04 - acc: 1.0000\n",
      "Epoch 236: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0718e-04 - acc: 1.0000 - val_loss: 1.1461e-04 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 237/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.9157e-04 - acc: 0.9998\n",
      "Epoch 237: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8925e-04 - acc: 0.9998 - val_loss: 1.9631e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 238/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.1622e-04 - acc: 0.9997\n",
      "Epoch 238: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.9563e-04 - acc: 0.9997 - val_loss: 3.6829e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 239/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.0702e-04 - acc: 1.0000\n",
      "Epoch 239: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0451e-04 - acc: 1.0000 - val_loss: 2.7784e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 240/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.6921e-06 - acc: 1.0000\n",
      "Epoch 240: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.6058e-06 - acc: 1.0000 - val_loss: 2.8170e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 241/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 7.7851e-06 - acc: 1.0000\n",
      "Epoch 241: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.2865e-06 - acc: 1.0000 - val_loss: 2.7413e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 242/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 3.5500e-06 - acc: 1.0000\n",
      "Epoch 242: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.5500e-06 - acc: 1.0000 - val_loss: 2.4924e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 243/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 4.8463e-06 - acc: 1.0000\n",
      "Epoch 243: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.8463e-06 - acc: 1.0000 - val_loss: 2.5935e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 244/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.1551e-05 - acc: 1.0000\n",
      "Epoch 244: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 1.1472e-05 - acc: 1.0000 - val_loss: 2.5565e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 245/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 7.5382e-06 - acc: 1.0000\n",
      "Epoch 245: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.7780e-06 - acc: 1.0000 - val_loss: 2.4042e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 246/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.4248e-06 - acc: 1.0000\n",
      "Epoch 246: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 4.3900e-06 - acc: 1.0000 - val_loss: 2.2995e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 247/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 4.9939e-06 - acc: 1.0000\n",
      "Epoch 247: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 4.8887e-06 - acc: 1.0000 - val_loss: 2.2767e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 248/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 7.5652e-06 - acc: 1.0000\n",
      "Epoch 248: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 7.4723e-06 - acc: 1.0000 - val_loss: 2.1153e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 249/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 5.7113e-06 - acc: 1.0000\n",
      "Epoch 249: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.5691e-06 - acc: 1.0000 - val_loss: 2.1744e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 250/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.2942e-05 - acc: 1.0000\n",
      "Epoch 250: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2838e-05 - acc: 1.0000 - val_loss: 2.4881e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 251/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 6.3969e-06 - acc: 1.0000\n",
      "Epoch 251: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.3969e-06 - acc: 1.0000 - val_loss: 2.2520e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 252/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.9860e-06 - acc: 1.0000\n",
      "Epoch 252: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.9860e-06 - acc: 1.0000 - val_loss: 2.0126e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 253/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 3.9488e-04 - acc: 0.9998\n",
      "Epoch 253: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.8977e-04 - acc: 0.9998 - val_loss: 1.7170e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 254/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.8681e-05 - acc: 1.0000\n",
      "Epoch 254: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8681e-05 - acc: 1.0000 - val_loss: 1.2748e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 255/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 9.4784e-06 - acc: 1.0000\n",
      "Epoch 255: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.2272e-06 - acc: 1.0000 - val_loss: 1.1136e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 256/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.3773e-04 - acc: 0.9998\n",
      "Epoch 256: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3384e-04 - acc: 0.9998 - val_loss: 9.9409e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 257/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.3048e-05 - acc: 1.0000\n",
      "Epoch 257: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6606e-05 - acc: 1.0000 - val_loss: 1.0005e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 258/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.3866e-05 - acc: 1.0000\n",
      "Epoch 258: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3866e-05 - acc: 1.0000 - val_loss: 3.5330e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 259/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.7806e-06 - acc: 1.0000\n",
      "Epoch 259: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.7432e-06 - acc: 1.0000 - val_loss: 3.1648e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 260/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.2324e-06 - acc: 1.0000\n",
      "Epoch 260: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1633e-06 - acc: 1.0000 - val_loss: 2.8207e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 261/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 7.2696e-05 - acc: 1.0000\n",
      "Epoch 261: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.2696e-05 - acc: 1.0000 - val_loss: 3.6283e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 262/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.8602e-06 - acc: 1.0000\n",
      "Epoch 262: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.7281e-06 - acc: 1.0000 - val_loss: 2.6192e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 263/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.6477e-06 - acc: 1.0000\n",
      "Epoch 263: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6072e-06 - acc: 1.0000 - val_loss: 2.5098e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 264/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.1716e-06 - acc: 1.0000\n",
      "Epoch 264: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1716e-06 - acc: 1.0000 - val_loss: 2.5296e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 265/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 5.9972e-06 - acc: 1.0000\n",
      "Epoch 265: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.7968e-06 - acc: 1.0000 - val_loss: 2.3119e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 266/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.3245e-06 - acc: 1.0000\n",
      "Epoch 266: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2786e-06 - acc: 1.0000 - val_loss: 2.0991e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 267/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.6773e-06 - acc: 1.0000\n",
      "Epoch 267: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.6333e-06 - acc: 1.0000 - val_loss: 1.7797e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 268/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.2347e-05 - acc: 1.0000\n",
      "Epoch 268: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2193e-05 - acc: 1.0000 - val_loss: 1.4784e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 269/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 7.6291e-06 - acc: 1.0000\n",
      "Epoch 269: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.7998e-06 - acc: 1.0000 - val_loss: 1.2513e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 270/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 5.0593e-06 - acc: 1.0000\n",
      "Epoch 270: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.9349e-06 - acc: 1.0000 - val_loss: 1.1976e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 271/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 5.9680e-07 - acc: 1.0000\n",
      "Epoch 271: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.7783e-07 - acc: 1.0000 - val_loss: 1.2163e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 272/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 8.3792e-07 - acc: 1.0000\n",
      "Epoch 272: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.3488e-07 - acc: 1.0000 - val_loss: 1.2547e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 273/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.5390e-05 - acc: 1.0000\n",
      "Epoch 273: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4953e-05 - acc: 1.0000 - val_loss: 1.1221e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 274/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.1746e-06 - acc: 1.0000\n",
      "Epoch 274: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1613e-06 - acc: 1.0000 - val_loss: 1.1194e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 275/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 2.0150e-05 - acc: 1.0000\n",
      "Epoch 275: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9825e-05 - acc: 1.0000 - val_loss: 8.5549e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 276/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 8.7838e-07 - acc: 1.0000\n",
      "Epoch 276: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.6387e-07 - acc: 1.0000 - val_loss: 8.4914e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 277/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.1499e-04 - acc: 1.0000\n",
      "Epoch 277: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1288e-04 - acc: 1.0000 - val_loss: 1.5128e-04 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 278/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 8.7235e-05 - acc: 1.0000\n",
      "Epoch 278: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.4946e-05 - acc: 1.0000 - val_loss: 5.6748e-04 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 279/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.7644e-05 - acc: 1.0000\n",
      "Epoch 279: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6854e-05 - acc: 1.0000 - val_loss: 9.6905e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 280/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 9.7096e-05 - acc: 1.0000\n",
      "Epoch 280: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.7096e-05 - acc: 1.0000 - val_loss: 4.3678e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 281/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.8925e-04 - acc: 0.9998\n",
      "Epoch 281: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8384e-04 - acc: 0.9998 - val_loss: 4.8601e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 282/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 9.4434e-07 - acc: 1.0000\n",
      "Epoch 282: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.1529e-07 - acc: 1.0000 - val_loss: 5.5914e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 283/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 9.5367e-06 - acc: 1.0000\n",
      "Epoch 283: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 283: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.2169e-06 - acc: 1.0000 - val_loss: 4.4718e-07 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 284/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.0539e-06 - acc: 1.0000\n",
      "Epoch 284: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2855e-06 - acc: 1.0000 - val_loss: 4.7829e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 285/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.2032e-04 - acc: 0.9998\n",
      "Epoch 285: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1872e-04 - acc: 0.9998 - val_loss: 3.3779e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 286/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 9.5308e-07 - acc: 1.0000\n",
      "Epoch 286: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.4083e-07 - acc: 1.0000 - val_loss: 3.2725e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 287/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.2307e-06 - acc: 1.0000\n",
      "Epoch 287: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2950e-06 - acc: 1.0000 - val_loss: 3.4509e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 288/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.7538e-06 - acc: 1.0000\n",
      "Epoch 288: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.7145e-06 - acc: 1.0000 - val_loss: 3.4336e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 289/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 6.0289e-07 - acc: 1.0000\n",
      "Epoch 289: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 5.9638e-07 - acc: 1.0000 - val_loss: 3.7947e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 290/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.2066e-06 - acc: 1.0000\n",
      "Epoch 290: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.1795e-06 - acc: 1.0000 - val_loss: 3.7760e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 291/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.0205e-06 - acc: 1.0000\n",
      "Epoch 291: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0174e-06 - acc: 1.0000 - val_loss: 3.2308e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 292/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.6231e-07 - acc: 1.0000\n",
      "Epoch 292: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.8292e-07 - acc: 1.0000 - val_loss: 3.3651e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 293/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 9.4987e-07 - acc: 1.0000\n",
      "Epoch 293: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 9.4329e-07 - acc: 1.0000 - val_loss: 3.5691e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 294/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.7593e-06 - acc: 1.0000\n",
      "Epoch 294: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.7547e-06 - acc: 1.0000 - val_loss: 3.8496e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 295/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.3785e-07 - acc: 1.0000\n",
      "Epoch 295: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.3775e-07 - acc: 1.0000 - val_loss: 3.1114e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 296/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.4742e-06 - acc: 1.0000\n",
      "Epoch 296: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4622e-06 - acc: 1.0000 - val_loss: 3.3748e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 297/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.0436e-07 - acc: 1.0000\n",
      "Epoch 297: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.6961e-07 - acc: 1.0000 - val_loss: 2.8637e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 298/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 7.7757e-07 - acc: 1.0000\n",
      "Epoch 298: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.7255e-07 - acc: 1.0000 - val_loss: 2.6886e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 299/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.5592e-06 - acc: 1.0000\n",
      "Epoch 299: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5151e-06 - acc: 1.0000 - val_loss: 2.4949e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 300/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.2697e-06 - acc: 1.0000\n",
      "Epoch 300: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2400e-06 - acc: 1.0000 - val_loss: 2.5093e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 301/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.4222e-07 - acc: 1.0000\n",
      "Epoch 301: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.3956e-07 - acc: 1.0000 - val_loss: 2.5770e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 302/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.0507e-07 - acc: 1.0000\n",
      "Epoch 302: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.0378e-07 - acc: 1.0000 - val_loss: 2.5940e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 303/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 3.0385e-06 - acc: 1.0000\n",
      "Epoch 303: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.9488e-06 - acc: 1.0000 - val_loss: 2.7119e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 304/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.6344e-07 - acc: 1.0000\n",
      "Epoch 304: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.6254e-07 - acc: 1.0000 - val_loss: 2.4738e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 305/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.0306e-05 - acc: 1.0000\n",
      "Epoch 305: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0169e-05 - acc: 1.0000 - val_loss: 1.1165e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 306/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 4.5236e-06 - acc: 1.0000\n",
      "Epoch 306: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.5236e-06 - acc: 1.0000 - val_loss: 1.3705e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 307/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.6997e-07 - acc: 1.0000\n",
      "Epoch 307: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 3.6880e-07 - acc: 1.0000 - val_loss: 1.4393e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 308/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.9999e-07 - acc: 1.0000\n",
      "Epoch 308: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.9999e-07 - acc: 1.0000 - val_loss: 1.0730e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 309/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.3726e-07 - acc: 1.0000\n",
      "Epoch 309: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.3658e-07 - acc: 1.0000 - val_loss: 1.0966e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 310/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.0058e-07 - acc: 1.0000\n",
      "Epoch 310: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.0136e-07 - acc: 1.0000 - val_loss: 1.2211e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 311/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 4.7998e-06 - acc: 1.0000\n",
      "Epoch 311: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.7119e-06 - acc: 1.0000 - val_loss: 2.2777e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 312/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 312: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0026 - acc: 0.9998 - val_loss: 2.2076e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 313/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.7306e-06 - acc: 1.0000\n",
      "Epoch 313: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6590e-06 - acc: 1.0000 - val_loss: 2.1536e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 314/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.8895e-07 - acc: 1.0000\n",
      "Epoch 314: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.8476e-07 - acc: 1.0000 - val_loss: 1.9874e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 315/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 7.5841e-07 - acc: 1.0000\n",
      "Epoch 315: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 7.4087e-07 - acc: 1.0000 - val_loss: 2.1042e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 316/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 6.4090e-06 - acc: 1.0000\n",
      "Epoch 316: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.3237e-06 - acc: 1.0000 - val_loss: 1.2483e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 317/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 4.9781e-07 - acc: 1.0000\n",
      "Epoch 317: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.8954e-07 - acc: 1.0000 - val_loss: 1.4421e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 318/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 7.4661e-07 - acc: 1.0000\n",
      "Epoch 318: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 7.4446e-07 - acc: 1.0000 - val_loss: 1.2853e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 319/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 4.9499e-07 - acc: 1.0000\n",
      "Epoch 319: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.9499e-07 - acc: 1.0000 - val_loss: 1.3623e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 320/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.2741e-06 - acc: 1.0000\n",
      "Epoch 320: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.2701e-06 - acc: 1.0000 - val_loss: 1.2117e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 321/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.4034e-06 - acc: 1.0000\n",
      "Epoch 321: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.3683e-06 - acc: 1.0000 - val_loss: 1.1168e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 322/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.3938e-06 - acc: 1.0000\n",
      "Epoch 322: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.3515e-06 - acc: 1.0000 - val_loss: 1.1753e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 323/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.7712e-06 - acc: 1.0000\n",
      "Epoch 323: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7302e-06 - acc: 1.0000 - val_loss: 1.0432e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 324/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 1.6325e-07 - acc: 1.0000\n",
      "Epoch 324: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5773e-07 - acc: 1.0000 - val_loss: 1.0290e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 325/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 3.7949e-06 - acc: 1.0000\n",
      "Epoch 325: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 3.6496e-06 - acc: 1.0000 - val_loss: 1.0329e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 326/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 7.5216e-08 - acc: 1.0000\n",
      "Epoch 326: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.4458e-08 - acc: 1.0000 - val_loss: 9.9827e-08 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 327/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 5.1583e-06 - acc: 1.0000\n",
      "Epoch 327: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.9663e-06 - acc: 1.0000 - val_loss: 8.7722e-08 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 328/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 4.3070e-07 - acc: 1.0000\n",
      "Epoch 328: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.2588e-07 - acc: 1.0000 - val_loss: 6.9137e-08 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 329/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.7161e-07 - acc: 1.0000\n",
      "Epoch 329: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.7161e-07 - acc: 1.0000 - val_loss: 8.0305e-08 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 330/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 7.0524e-07 - acc: 1.0000\n",
      "Epoch 330: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.9941e-07 - acc: 1.0000 - val_loss: 8.2607e-08 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 331/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.5151e-07 - acc: 1.0000\n",
      "Epoch 331: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.5045e-07 - acc: 1.0000 - val_loss: 8.3487e-08 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 332/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.0587e-06 - acc: 1.0000\n",
      "Epoch 332: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0340e-06 - acc: 1.0000 - val_loss: 8.5363e-08 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 333/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.8667e-07 - acc: 1.0000\n",
      "Epoch 333: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 333: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.8274e-07 - acc: 1.0000 - val_loss: 8.2265e-08 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 334/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 3.9451e-07 - acc: 1.0000\n",
      "Epoch 334: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 3.9201e-07 - acc: 1.0000 - val_loss: 8.5079e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 335/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 5.6198e-07 - acc: 1.0000\n",
      "Epoch 335: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.5186e-07 - acc: 1.0000 - val_loss: 6.9961e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 336/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 4.7314e-05 - acc: 1.0000\n",
      "Epoch 336: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.5498e-05 - acc: 1.0000 - val_loss: 8.4141e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 337/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 7.9814e-06 - acc: 1.0000\n",
      "Epoch 337: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 7.8533e-06 - acc: 1.0000 - val_loss: 9.5792e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 338/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.2202e-07 - acc: 1.0000\n",
      "Epoch 338: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1932e-07 - acc: 1.0000 - val_loss: 8.6102e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 339/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.3290e-07 - acc: 1.0000\n",
      "Epoch 339: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3029e-07 - acc: 1.0000 - val_loss: 8.4511e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 340/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 4.2263e-07 - acc: 1.0000\n",
      "Epoch 340: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.1488e-07 - acc: 1.0000 - val_loss: 8.5335e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 341/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 7.3904e-07 - acc: 1.0000\n",
      "Epoch 341: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.2665e-07 - acc: 1.0000 - val_loss: 7.6156e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 342/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.0493e-06 - acc: 1.0000\n",
      "Epoch 342: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0220e-06 - acc: 1.0000 - val_loss: 6.9734e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 343/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.7065e-07 - acc: 1.0000\n",
      "Epoch 343: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7011e-07 - acc: 1.0000 - val_loss: 6.4619e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 344/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.1089e-07 - acc: 1.0000\n",
      "Epoch 344: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0956e-07 - acc: 1.0000 - val_loss: 6.3540e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 345/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.4980e-07 - acc: 1.0000\n",
      "Epoch 345: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4858e-07 - acc: 1.0000 - val_loss: 6.2232e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 346/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 9.5023e-08 - acc: 1.0000\n",
      "Epoch 346: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.2077e-08 - acc: 1.0000 - val_loss: 5.9419e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 347/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.9445e-06 - acc: 1.0000\n",
      "Epoch 347: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.9187e-06 - acc: 1.0000 - val_loss: 6.3341e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 348/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.4250e-07 - acc: 1.0000\n",
      "Epoch 348: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6158e-07 - acc: 1.0000 - val_loss: 5.6464e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 349/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.2232e-07 - acc: 1.0000\n",
      "Epoch 349: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2879e-07 - acc: 1.0000 - val_loss: 5.3423e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 350/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.2953e-07 - acc: 1.0000\n",
      "Epoch 350: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 8.0187e-07 - acc: 1.0000 - val_loss: 5.4191e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 351/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.9360e-07 - acc: 1.0000\n",
      "Epoch 351: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.9372e-07 - acc: 1.0000 - val_loss: 4.7399e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 352/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.4045e-07 - acc: 1.0000\n",
      "Epoch 352: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3932e-07 - acc: 1.0000 - val_loss: 4.5865e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 353/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 2.3989e-07 - acc: 1.0000\n",
      "Epoch 353: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3791e-07 - acc: 1.0000 - val_loss: 4.0551e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 354/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.8157e-07 - acc: 1.0000\n",
      "Epoch 354: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8099e-07 - acc: 1.0000 - val_loss: 4.3392e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 355/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.5099e-07 - acc: 1.0000\n",
      "Epoch 355: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.5019e-07 - acc: 1.0000 - val_loss: 4.0323e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 356/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.8794e-07 - acc: 1.0000\n",
      "Epoch 356: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8403e-07 - acc: 1.0000 - val_loss: 3.9954e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 357/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.0723e-07 - acc: 1.0000\n",
      "Epoch 357: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0723e-07 - acc: 1.0000 - val_loss: 3.9641e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 358/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 9.0105e-08 - acc: 1.0000\n",
      "Epoch 358: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.0105e-08 - acc: 1.0000 - val_loss: 3.6544e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 359/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 5.8672e-08 - acc: 1.0000\n",
      "Epoch 359: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 7.1824e-08 - acc: 1.0000 - val_loss: 3.8647e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 360/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.2606e-07 - acc: 1.0000\n",
      "Epoch 360: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.2502e-07 - acc: 1.0000 - val_loss: 3.7084e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 361/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.3246e-07 - acc: 1.0000\n",
      "Epoch 361: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2830e-07 - acc: 1.0000 - val_loss: 3.1600e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 362/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.1629e-06 - acc: 1.0000\n",
      "Epoch 362: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.5991e-06 - acc: 1.0000 - val_loss: 2.8360e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 363/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.9842e-08 - acc: 1.0000\n",
      "Epoch 363: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.9468e-08 - acc: 1.0000 - val_loss: 3.1429e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 364/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 3.9074e-07 - acc: 1.0000\n",
      "Epoch 364: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.8698e-07 - acc: 1.0000 - val_loss: 2.6826e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 365/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.4374e-07 - acc: 1.0000\n",
      "Epoch 365: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.3595e-07 - acc: 1.0000 - val_loss: 2.4694e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 366/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 8.8010e-08 - acc: 1.0000\n",
      "Epoch 366: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.7623e-08 - acc: 1.0000 - val_loss: 2.5973e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 367/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.4370e-06 - acc: 1.0000\n",
      "Epoch 367: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.4370e-06 - acc: 1.0000 - val_loss: 2.4069e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 368/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 6.2659e-08 - acc: 1.0000\n",
      "Epoch 368: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.0646e-08 - acc: 1.0000 - val_loss: 2.5746e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 369/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.8547e-07 - acc: 1.0000\n",
      "Epoch 369: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8224e-07 - acc: 1.0000 - val_loss: 2.3785e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 370/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.9841e-07 - acc: 1.0000\n",
      "Epoch 370: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.9537e-07 - acc: 1.0000 - val_loss: 2.4154e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 371/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.2762e-07 - acc: 1.0000\n",
      "Epoch 371: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2866e-07 - acc: 1.0000 - val_loss: 2.2677e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 372/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.6425e-08 - acc: 1.0000\n",
      "Epoch 372: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.3835e-08 - acc: 1.0000 - val_loss: 2.1881e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 373/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 9.7398e-06 - acc: 1.0000\n",
      "Epoch 373: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.0051e-05 - acc: 1.0000 - val_loss: 2.2392e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 374/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.6545e-07 - acc: 1.0000\n",
      "Epoch 374: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6322e-07 - acc: 1.0000 - val_loss: 1.9579e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 375/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.5167e-07 - acc: 1.0000\n",
      "Epoch 375: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.5089e-07 - acc: 1.0000 - val_loss: 2.0204e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 376/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 7.0667e-08 - acc: 1.0000\n",
      "Epoch 376: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.0667e-08 - acc: 1.0000 - val_loss: 1.9437e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 377/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.3831e-07 - acc: 1.0000\n",
      "Epoch 377: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3450e-07 - acc: 1.0000 - val_loss: 1.8414e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 378/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.6077e-08 - acc: 1.0000\n",
      "Epoch 378: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.5880e-08 - acc: 1.0000 - val_loss: 1.9323e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 379/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 5.0662e-08 - acc: 1.0000\n",
      "Epoch 379: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.1210e-08 - acc: 1.0000 - val_loss: 1.8357e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 380/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 3.7547e-08 - acc: 1.0000\n",
      "Epoch 380: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.0771e-08 - acc: 1.0000 - val_loss: 2.0943e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 381/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 6.4804e-08 - acc: 1.0000\n",
      "Epoch 381: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.2366e-08 - acc: 1.0000 - val_loss: 1.9096e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 382/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.4015e-07 - acc: 1.0000\n",
      "Epoch 382: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.5820e-07 - acc: 1.0000 - val_loss: 2.0460e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 383/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 2.0432e-07 - acc: 1.0000\n",
      "Epoch 383: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "\n",
      "Epoch 383: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9732e-07 - acc: 1.0000 - val_loss: 1.6084e-08 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 384/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 2.4454e-07 - acc: 1.0000\n",
      "Epoch 384: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4299e-07 - acc: 1.0000 - val_loss: 1.4521e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 385/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 3.6126e-07 - acc: 1.0000\n",
      "Epoch 385: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.5655e-07 - acc: 1.0000 - val_loss: 1.3782e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 386/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 2.7623e-05 - acc: 1.0000\n",
      "Epoch 386: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6553e-05 - acc: 1.0000 - val_loss: 1.2532e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 387/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 2.3156e-06 - acc: 1.0000\n",
      "Epoch 387: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.2965e-06 - acc: 1.0000 - val_loss: 1.3725e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 388/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.3433e-07 - acc: 1.0000\n",
      "Epoch 388: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.3392e-07 - acc: 1.0000 - val_loss: 1.4265e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 389/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 4.1765e-08 - acc: 1.0000\n",
      "Epoch 389: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.1283e-08 - acc: 1.0000 - val_loss: 1.4322e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 390/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.4686e-05 - acc: 1.0000\n",
      "Epoch 390: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.4191e-05 - acc: 1.0000 - val_loss: 1.2702e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 391/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.0414e-07 - acc: 1.0000\n",
      "Epoch 391: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0414e-07 - acc: 1.0000 - val_loss: 1.1338e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 392/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.9621e-07 - acc: 1.0000\n",
      "Epoch 392: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.9621e-07 - acc: 1.0000 - val_loss: 1.3498e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 393/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.9471e-07 - acc: 1.0000\n",
      "Epoch 393: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.9377e-07 - acc: 1.0000 - val_loss: 1.2219e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 394/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 5.5250e-08 - acc: 1.0000\n",
      "Epoch 394: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.5074e-08 - acc: 1.0000 - val_loss: 1.0941e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 395/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.8432e-05 - acc: 1.0000\n",
      "Epoch 395: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.8189e-05 - acc: 1.0000 - val_loss: 1.3498e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 396/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.6943e-07 - acc: 1.0000\n",
      "Epoch 396: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.0753e-07 - acc: 1.0000 - val_loss: 1.2844e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 397/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.2178e-08 - acc: 1.0000\n",
      "Epoch 397: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 3.2076e-08 - acc: 1.0000 - val_loss: 1.2873e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 398/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 7.1487e-08 - acc: 1.0000\n",
      "Epoch 398: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.8848e-08 - acc: 1.0000 - val_loss: 1.2816e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 399/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.7733e-07 - acc: 1.0000\n",
      "Epoch 399: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.7585e-07 - acc: 1.0000 - val_loss: 1.2674e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 400/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.1078e-07 - acc: 1.0000\n",
      "Epoch 400: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 8.3750e-07 - acc: 1.0000 - val_loss: 1.2844e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 401/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 1.0278e-07 - acc: 1.0000\n",
      "Epoch 401: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.0002e-07 - acc: 1.0000 - val_loss: 1.1139e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 402/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 8.3725e-08 - acc: 1.0000\n",
      "Epoch 402: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.1500e-08 - acc: 1.0000 - val_loss: 1.1935e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 403/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 6.1934e-08 - acc: 1.0000\n",
      "Epoch 403: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.1441e-08 - acc: 1.0000 - val_loss: 1.1765e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 404/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 5.7191e-07 - acc: 1.0000\n",
      "Epoch 404: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 5.6719e-07 - acc: 1.0000 - val_loss: 1.1253e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 405/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.6299e-07 - acc: 1.0000\n",
      "Epoch 405: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.6131e-07 - acc: 1.0000 - val_loss: 1.3129e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 406/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.6293e-08 - acc: 1.0000\n",
      "Epoch 406: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.6082e-08 - acc: 1.0000 - val_loss: 1.1765e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 407/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.0625e-07 - acc: 1.0000\n",
      "Epoch 407: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0595e-07 - acc: 1.0000 - val_loss: 1.1651e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 408/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 6.9397e-08 - acc: 1.0000\n",
      "Epoch 408: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.9397e-08 - acc: 1.0000 - val_loss: 1.2560e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 409/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 5.0655e-08 - acc: 1.0000\n",
      "Epoch 409: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.9979e-08 - acc: 1.0000 - val_loss: 1.2844e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 410/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.3637e-07 - acc: 1.0000\n",
      "Epoch 410: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 5.3637e-07 - acc: 1.0000 - val_loss: 1.1850e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 411/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 2.2864e-08 - acc: 1.0000\n",
      "Epoch 411: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.3209e-08 - acc: 1.0000 - val_loss: 1.3924e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 412/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.9165e-07 - acc: 1.0000\n",
      "Epoch 412: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 4.9032e-07 - acc: 1.0000 - val_loss: 1.3214e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 413/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.9171e-08 - acc: 1.0000\n",
      "Epoch 413: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.6916e-08 - acc: 1.0000 - val_loss: 1.2503e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 414/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 3.2852e-08 - acc: 1.0000\n",
      "Epoch 414: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2455e-08 - acc: 1.0000 - val_loss: 1.2532e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 415/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 5.9316e-08 - acc: 1.0000\n",
      "Epoch 415: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 5.8826e-08 - acc: 1.0000 - val_loss: 1.3043e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 416/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.6490e-06 - acc: 1.0000\n",
      "Epoch 416: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 2.6137e-06 - acc: 1.0000 - val_loss: 1.2248e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 417/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.0599e-04 - acc: 0.9998\n",
      "Epoch 417: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 3.9231e-04 - acc: 0.9998 - val_loss: 1.0401e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 418/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.3623e-06 - acc: 1.0000\n",
      "Epoch 418: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3548e-06 - acc: 1.0000 - val_loss: 9.6049e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 419/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.8473e-07 - acc: 1.0000\n",
      "Epoch 419: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8057e-07 - acc: 1.0000 - val_loss: 1.0884e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 420/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.4605e-07 - acc: 1.0000\n",
      "Epoch 420: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.4559e-07 - acc: 1.0000 - val_loss: 1.0060e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 421/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.6458e-04 - acc: 0.9998\n",
      "Epoch 421: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 5.6458e-04 - acc: 0.9998 - val_loss: 1.0287e-08 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 422/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.3024e-06 - acc: 1.0000\n",
      "Epoch 422: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 3.2919e-06 - acc: 1.0000 - val_loss: 9.7186e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 423/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.0303e-07 - acc: 1.0000\n",
      "Epoch 423: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 1.0303e-07 - acc: 1.0000 - val_loss: 9.8323e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 424/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 9.2381e-08 - acc: 1.0000\n",
      "Epoch 424: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.9458e-08 - acc: 1.0000 - val_loss: 9.6902e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 425/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.3017e-04 - acc: 0.9998\n",
      "Epoch 425: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2943e-04 - acc: 0.9998 - val_loss: 8.1557e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 426/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.9760e-07 - acc: 1.0000\n",
      "Epoch 426: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9597e-07 - acc: 1.0000 - val_loss: 8.0988e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 427/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 6.8621e-07 - acc: 1.0000\n",
      "Epoch 427: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 6.7470e-07 - acc: 1.0000 - val_loss: 8.2409e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 428/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 5.1177e-05 - acc: 1.0000\n",
      "Epoch 428: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.1015e-05 - acc: 1.0000 - val_loss: 8.6672e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 429/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.1531e-08 - acc: 1.0000\n",
      "Epoch 429: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 11ms/step - loss: 4.1775e-08 - acc: 1.0000 - val_loss: 8.5535e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 430/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 3.4796e-07 - acc: 1.0000\n",
      "Epoch 430: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 3.4524e-07 - acc: 1.0000 - val_loss: 8.5819e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 431/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 4.0149e-06 - acc: 1.0000\n",
      "Epoch 431: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.9209e-06 - acc: 1.0000 - val_loss: 8.2693e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 432/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.6211e-08 - acc: 1.0000\n",
      "Epoch 432: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 5.6211e-08 - acc: 1.0000 - val_loss: 8.3830e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 433/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.3646e-07 - acc: 1.0000\n",
      "Epoch 433: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 433: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3192e-07 - acc: 1.0000 - val_loss: 8.0988e-09 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 434/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 4.0314e-08 - acc: 1.0000\n",
      "Epoch 434: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 4.0960e-08 - acc: 1.0000 - val_loss: 8.7524e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 435/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 2.2829e-08 - acc: 1.0000\n",
      "Epoch 435: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3001e-08 - acc: 1.0000 - val_loss: 9.0082e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 436/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 3.5159e-08 - acc: 1.0000\n",
      "Epoch 436: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.4728e-08 - acc: 1.0000 - val_loss: 8.6388e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 437/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.0900e-07 - acc: 1.0000\n",
      "Epoch 437: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.9583e-07 - acc: 1.0000 - val_loss: 9.4344e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 438/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.4095e-08 - acc: 1.0000\n",
      "Epoch 438: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.2818e-08 - acc: 1.0000 - val_loss: 9.5765e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 439/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 8.4862e-08 - acc: 1.0000\n",
      "Epoch 439: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.2941e-08 - acc: 1.0000 - val_loss: 8.7524e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 440/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.8149e-08 - acc: 1.0000\n",
      "Epoch 440: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7999e-08 - acc: 1.0000 - val_loss: 8.4398e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 441/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.6312e-08 - acc: 1.0000\n",
      "Epoch 441: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.6120e-08 - acc: 1.0000 - val_loss: 7.9283e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 442/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.4129e-06 - acc: 1.0000\n",
      "Epoch 442: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4084e-06 - acc: 1.0000 - val_loss: 8.7524e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 443/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.6369e-08 - acc: 1.0000\n",
      "Epoch 443: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.5314e-08 - acc: 1.0000 - val_loss: 8.1273e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 444/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 5.0895e-07 - acc: 1.0000\n",
      "Epoch 444: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.9448e-07 - acc: 1.0000 - val_loss: 7.8147e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 445/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 5.5782e-08 - acc: 1.0000\n",
      "Epoch 445: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.5038e-08 - acc: 1.0000 - val_loss: 7.9852e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 446/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 6.0891e-08 - acc: 1.0000\n",
      "Epoch 446: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.0513e-08 - acc: 1.0000 - val_loss: 9.0650e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 447/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 6.7688e-08 - acc: 1.0000\n",
      "Epoch 447: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.7688e-08 - acc: 1.0000 - val_loss: 8.5251e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 448/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.0313e-08 - acc: 1.0000\n",
      "Epoch 448: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9799e-08 - acc: 1.0000 - val_loss: 8.8661e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 449/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.5849e-04 - acc: 0.9998\n",
      "Epoch 449: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5395e-04 - acc: 0.9998 - val_loss: 9.0366e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 450/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.0580e-08 - acc: 1.0000\n",
      "Epoch 450: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.1403e-08 - acc: 1.0000 - val_loss: 8.5819e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 451/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.0867e-07 - acc: 1.0000\n",
      "Epoch 451: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.0496e-07 - acc: 1.0000 - val_loss: 8.7240e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 452/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 8.9965e-08 - acc: 1.0000\n",
      "Epoch 452: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.9411e-08 - acc: 1.0000 - val_loss: 9.5481e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 453/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.5507e-08 - acc: 1.0000\n",
      "Epoch 453: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.4201e-08 - acc: 1.0000 - val_loss: 9.1787e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 454/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.1604e-07 - acc: 1.0000\n",
      "Epoch 454: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1236e-07 - acc: 1.0000 - val_loss: 9.7754e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 455/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 9.5761e-07 - acc: 1.0000\n",
      "Epoch 455: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.5516e-07 - acc: 1.0000 - val_loss: 7.7862e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 456/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.2274e-08 - acc: 1.0000\n",
      "Epoch 456: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2072e-08 - acc: 1.0000 - val_loss: 8.4398e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 457/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.5301e-07 - acc: 1.0000\n",
      "Epoch 457: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5301e-07 - acc: 1.0000 - val_loss: 8.2977e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 458/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.5841e-08 - acc: 1.0000\n",
      "Epoch 458: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.5331e-08 - acc: 1.0000 - val_loss: 7.8147e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 459/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 5.2191e-08 - acc: 1.0000\n",
      "Epoch 459: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.0945e-08 - acc: 1.0000 - val_loss: 7.9852e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 460/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.1978e-08 - acc: 1.0000\n",
      "Epoch 460: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.9142e-08 - acc: 1.0000 - val_loss: 7.9567e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 461/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.7908e-08 - acc: 1.0000\n",
      "Epoch 461: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.5003e-08 - acc: 1.0000 - val_loss: 8.6672e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 462/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 8.2696e-08 - acc: 1.0000\n",
      "Epoch 462: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.2373e-08 - acc: 1.0000 - val_loss: 9.0366e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 463/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.0616e-06 - acc: 1.0000\n",
      "Epoch 463: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.0487e-06 - acc: 1.0000 - val_loss: 9.7186e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 464/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 3.7616e-08 - acc: 1.0000\n",
      "Epoch 464: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.7152e-08 - acc: 1.0000 - val_loss: 8.1841e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 465/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.3151e-08 - acc: 1.0000\n",
      "Epoch 465: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0280e-07 - acc: 1.0000 - val_loss: 9.9743e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 466/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.8311e-07 - acc: 1.0000\n",
      "Epoch 466: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.7929e-07 - acc: 1.0000 - val_loss: 9.1787e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 467/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.7171e-07 - acc: 1.0000\n",
      "Epoch 467: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.7089e-07 - acc: 1.0000 - val_loss: 8.0136e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 468/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 8.2773e-08 - acc: 1.0000\n",
      "Epoch 468: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.2183e-08 - acc: 1.0000 - val_loss: 8.2125e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 469/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 5.4938e-08 - acc: 1.0000\n",
      "Epoch 469: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.3537e-08 - acc: 1.0000 - val_loss: 8.7524e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 470/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.9083e-07 - acc: 1.0000\n",
      "Epoch 470: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9022e-07 - acc: 1.0000 - val_loss: 8.4114e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 471/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.3898e-08 - acc: 1.0000\n",
      "Epoch 471: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.2495e-08 - acc: 1.0000 - val_loss: 9.0650e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 472/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 4.9147e-06 - acc: 1.0000\n",
      "Epoch 472: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.9147e-06 - acc: 1.0000 - val_loss: 9.0934e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 473/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 9.1694e-08 - acc: 1.0000\n",
      "Epoch 473: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.1694e-08 - acc: 1.0000 - val_loss: 8.1557e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 474/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 7.0785e-07 - acc: 1.0000\n",
      "Epoch 474: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.8402e-07 - acc: 1.0000 - val_loss: 7.6157e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 475/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 5.1512e-08 - acc: 1.0000\n",
      "Epoch 475: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.0490e-08 - acc: 1.0000 - val_loss: 8.0704e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 476/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.5003e-07 - acc: 1.0000\n",
      "Epoch 476: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.3842e-07 - acc: 1.0000 - val_loss: 8.5251e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 477/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.9461e-07 - acc: 1.0000\n",
      "Epoch 477: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9399e-07 - acc: 1.0000 - val_loss: 9.6902e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 478/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 4.5146e-08 - acc: 1.0000\n",
      "Epoch 478: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.4314e-08 - acc: 1.0000 - val_loss: 9.2071e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 479/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.5998e-08 - acc: 1.0000\n",
      "Epoch 479: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.5429e-08 - acc: 1.0000 - val_loss: 9.0082e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 480/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 7.6770e-08 - acc: 1.0000\n",
      "Epoch 480: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.4964e-08 - acc: 1.0000 - val_loss: 8.9798e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 481/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.2095e-08 - acc: 1.0000\n",
      "Epoch 481: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.0771e-08 - acc: 1.0000 - val_loss: 8.8377e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 482/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.7705e-05 - acc: 1.0000\n",
      "Epoch 482: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7648e-05 - acc: 1.0000 - val_loss: 7.9852e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 483/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 2.0092e-07 - acc: 1.0000\n",
      "Epoch 483: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "\n",
      "Epoch 483: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9926e-07 - acc: 1.0000 - val_loss: 7.5021e-09 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 484/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 9.4187e-08 - acc: 1.0000\n",
      "Epoch 484: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.3906e-08 - acc: 1.0000 - val_loss: 7.8715e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 485/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.8684e-08 - acc: 1.0000\n",
      "Epoch 485: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.8684e-08 - acc: 1.0000 - val_loss: 8.5535e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 486/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 6.7464e-08 - acc: 1.0000\n",
      "Epoch 486: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.7464e-08 - acc: 1.0000 - val_loss: 7.8147e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 487/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.2151e-07 - acc: 1.0000\n",
      "Epoch 487: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2060e-07 - acc: 1.0000 - val_loss: 7.8147e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 488/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 3.8372e-08 - acc: 1.0000\n",
      "Epoch 488: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.7892e-08 - acc: 1.0000 - val_loss: 8.6956e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 489/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 7.9500e-07 - acc: 1.0000\n",
      "Epoch 489: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.9247e-07 - acc: 1.0000 - val_loss: 8.0704e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 490/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.2736e-08 - acc: 1.0000\n",
      "Epoch 490: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2584e-08 - acc: 1.0000 - val_loss: 8.0136e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 491/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.0631e-07 - acc: 1.0000\n",
      "Epoch 491: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1081e-07 - acc: 1.0000 - val_loss: 8.3546e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 492/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.5376e-08 - acc: 1.0000\n",
      "Epoch 492: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.5376e-08 - acc: 1.0000 - val_loss: 7.3884e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 493/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.4848e-08 - acc: 1.0000\n",
      "Epoch 493: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4876e-08 - acc: 1.0000 - val_loss: 7.2747e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 494/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.1941e-07 - acc: 1.0000\n",
      "Epoch 494: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1909e-07 - acc: 1.0000 - val_loss: 7.1895e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 495/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 495: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 8.4398e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 496/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 3.6928e-08 - acc: 1.0000\n",
      "Epoch 496: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.6641e-08 - acc: 1.0000 - val_loss: 7.5305e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 497/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.3697e-08 - acc: 1.0000\n",
      "Epoch 497: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3796e-08 - acc: 1.0000 - val_loss: 7.4168e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 498/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 7.2878e-08 - acc: 1.0000\n",
      "Epoch 498: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 7.2275e-08 - acc: 1.0000 - val_loss: 7.1611e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 499/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.1583e-08 - acc: 1.0000\n",
      "Epoch 499: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.9925e-08 - acc: 1.0000 - val_loss: 7.3600e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 500/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.6695e-08 - acc: 1.0000\n",
      "Epoch 500: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6695e-08 - acc: 1.0000 - val_loss: 6.9337e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 501/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.4331e-08 - acc: 1.0000\n",
      "Epoch 501: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4285e-08 - acc: 1.0000 - val_loss: 7.8715e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 502/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.2868e-08 - acc: 1.0000\n",
      "Epoch 502: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2868e-08 - acc: 1.0000 - val_loss: 7.2463e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 503/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.1608e-08 - acc: 1.0000\n",
      "Epoch 503: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.1507e-08 - acc: 1.0000 - val_loss: 7.4452e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 504/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 9.5462e-08 - acc: 1.0000\n",
      "Epoch 504: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.5462e-08 - acc: 1.0000 - val_loss: 7.5305e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 505/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.4559e-07 - acc: 1.0000\n",
      "Epoch 505: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4559e-07 - acc: 1.0000 - val_loss: 7.1895e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 506/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 7.0504e-08 - acc: 1.0000\n",
      "Epoch 506: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.0761e-08 - acc: 1.0000 - val_loss: 7.1611e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 507/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 3.2454e-08 - acc: 1.0000\n",
      "Epoch 507: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2454e-08 - acc: 1.0000 - val_loss: 7.4452e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 508/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 6.9758e-08 - acc: 1.0000\n",
      "Epoch 508: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.9187e-08 - acc: 1.0000 - val_loss: 7.2179e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 509/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 9.4258e-08 - acc: 1.0000\n",
      "Epoch 509: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.2539e-08 - acc: 1.0000 - val_loss: 7.4168e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 510/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.7357e-07 - acc: 1.0000\n",
      "Epoch 510: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 1.7357e-07 - acc: 1.0000 - val_loss: 7.1042e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 511/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.2990e-08 - acc: 1.0000\n",
      "Epoch 511: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2732e-08 - acc: 1.0000 - val_loss: 6.7064e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 512/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 4.0411e-08 - acc: 1.0000\n",
      "Epoch 512: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.0032e-08 - acc: 1.0000 - val_loss: 8.3546e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 513/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 3.3275e-08 - acc: 1.0000\n",
      "Epoch 513: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2492e-08 - acc: 1.0000 - val_loss: 7.2179e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 514/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 3.4260e-08 - acc: 1.0000\n",
      "Epoch 514: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.4462e-08 - acc: 1.0000 - val_loss: 6.9622e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 515/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.1447e-06 - acc: 1.0000\n",
      "Epoch 515: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1130e-06 - acc: 1.0000 - val_loss: 7.6157e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 516/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.4077e-06 - acc: 1.0000\n",
      "Epoch 516: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2930e-06 - acc: 1.0000 - val_loss: 7.0758e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 517/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.7296e-07 - acc: 1.0000\n",
      "Epoch 517: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7241e-07 - acc: 1.0000 - val_loss: 7.0190e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 518/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.4359e-08 - acc: 1.0000\n",
      "Epoch 518: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.3992e-08 - acc: 1.0000 - val_loss: 7.1895e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 519/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 6.5025e-08 - acc: 1.0000\n",
      "Epoch 519: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.3771e-08 - acc: 1.0000 - val_loss: 7.1042e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 520/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.4835e-08 - acc: 1.0000\n",
      "Epoch 520: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4835e-08 - acc: 1.0000 - val_loss: 7.3032e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 521/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 3.7798e-08 - acc: 1.0000\n",
      "Epoch 521: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.6717e-08 - acc: 1.0000 - val_loss: 6.7917e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 522/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 5.9735e-08 - acc: 1.0000\n",
      "Epoch 522: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.9640e-08 - acc: 1.0000 - val_loss: 7.3316e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 523/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.3538e-08 - acc: 1.0000\n",
      "Epoch 523: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 1.3395e-08 - acc: 1.0000 - val_loss: 7.2747e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 524/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.8232e-07 - acc: 1.0000\n",
      "Epoch 524: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 1.8004e-07 - acc: 1.0000 - val_loss: 7.4452e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 525/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.0322e-07 - acc: 1.0000\n",
      "Epoch 525: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0051e-07 - acc: 1.0000 - val_loss: 7.3884e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 526/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 9.3047e-08 - acc: 1.0000\n",
      "Epoch 526: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.2751e-08 - acc: 1.0000 - val_loss: 6.8485e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 527/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.0010e-07 - acc: 1.0000\n",
      "Epoch 527: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9745e-07 - acc: 1.0000 - val_loss: 8.3830e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 528/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.2975e-06 - acc: 1.0000\n",
      "Epoch 528: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 1.2868e-06 - acc: 1.0000 - val_loss: 7.0190e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 529/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.4090e-06 - acc: 1.0000\n",
      "Epoch 529: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 4.3950e-06 - acc: 1.0000 - val_loss: 7.1327e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 530/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.8877e-07 - acc: 1.0000\n",
      "Epoch 530: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 3.7589e-07 - acc: 1.0000 - val_loss: 7.2747e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 531/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 6.2640e-08 - acc: 1.0000\n",
      "Epoch 531: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.2141e-08 - acc: 1.0000 - val_loss: 7.0474e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 532/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 9.9651e-08 - acc: 1.0000\n",
      "Epoch 532: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.9812e-08 - acc: 1.0000 - val_loss: 7.2747e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 533/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 4.7923e-08 - acc: 1.0000\n",
      "Epoch 533: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 533: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.7345e-08 - acc: 1.0000 - val_loss: 7.3032e-09 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 534/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.2034e-08 - acc: 1.0000\n",
      "Epoch 534: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2034e-08 - acc: 1.0000 - val_loss: 6.7632e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 535/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.2018e-07 - acc: 1.0000\n",
      "Epoch 535: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1792e-07 - acc: 1.0000 - val_loss: 7.1042e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 536/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.6568e-07 - acc: 1.0000\n",
      "Epoch 536: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.6183e-07 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 537/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.0264e-08 - acc: 1.0000\n",
      "Epoch 537: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.7654e-08 - acc: 1.0000 - val_loss: 6.7064e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 538/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.4898e-06 - acc: 1.0000\n",
      "Epoch 538: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4570e-06 - acc: 1.0000 - val_loss: 7.5305e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 539/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 9.0106e-08 - acc: 1.0000\n",
      "Epoch 539: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.0503e-08 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 540/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.6102e-07 - acc: 1.0000\n",
      "Epoch 540: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5735e-07 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 541/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.0156e-08 - acc: 1.0000\n",
      "Epoch 541: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 2.0670e-08 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 542/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.0782e-06 - acc: 1.0000\n",
      "Epoch 542: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0089e-06 - acc: 1.0000 - val_loss: 6.8201e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 543/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 9.7978e-08 - acc: 1.0000\n",
      "Epoch 543: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.8064e-08 - acc: 1.0000 - val_loss: 7.1611e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 544/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.5433e-08 - acc: 1.0000\n",
      "Epoch 544: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6013e-08 - acc: 1.0000 - val_loss: 6.7064e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 545/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.6350e-07 - acc: 1.0000\n",
      "Epoch 545: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.6214e-07 - acc: 1.0000 - val_loss: 7.2747e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 546/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.6910e-07 - acc: 1.0000\n",
      "Epoch 546: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6440e-07 - acc: 1.0000 - val_loss: 6.6780e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 547/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 3.7787e-08 - acc: 1.0000\n",
      "Epoch 547: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.9331e-08 - acc: 1.0000 - val_loss: 6.9337e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 548/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.8300e-08 - acc: 1.0000\n",
      "Epoch 548: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.7661e-08 - acc: 1.0000 - val_loss: 6.7064e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 549/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.8805e-06 - acc: 1.0000\n",
      "Epoch 549: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8458e-06 - acc: 1.0000 - val_loss: 6.6212e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 550/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.3249e-07 - acc: 1.0000\n",
      "Epoch 550: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2128e-07 - acc: 1.0000 - val_loss: 6.6780e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 551/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.0527e-08 - acc: 1.0000\n",
      "Epoch 551: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1125e-08 - acc: 1.0000 - val_loss: 5.7971e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 552/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 9.0013e-08 - acc: 1.0000\n",
      "Epoch 552: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.8812e-08 - acc: 1.0000 - val_loss: 6.4507e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 553/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 1.8153e-04 - acc: 0.9998\n",
      "Epoch 553: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7449e-04 - acc: 0.9998 - val_loss: 5.9676e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 554/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.8880e-08 - acc: 1.0000\n",
      "Epoch 554: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9173e-08 - acc: 1.0000 - val_loss: 6.2517e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 555/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 5.8720e-08 - acc: 1.0000\n",
      "Epoch 555: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.7955e-08 - acc: 1.0000 - val_loss: 7.2463e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 556/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.5794e-08 - acc: 1.0000\n",
      "Epoch 556: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5744e-08 - acc: 1.0000 - val_loss: 6.8485e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 557/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.8566e-08 - acc: 1.0000\n",
      "Epoch 557: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.7570e-08 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 558/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 9.2628e-08 - acc: 1.0000\n",
      "Epoch 558: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.2628e-08 - acc: 1.0000 - val_loss: 7.3316e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 559/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.4319e-07 - acc: 1.0000\n",
      "Epoch 559: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3979e-07 - acc: 1.0000 - val_loss: 6.9622e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 560/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.8511e-08 - acc: 1.0000\n",
      "Epoch 560: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.8294e-08 - acc: 1.0000 - val_loss: 6.3370e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 561/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 9.4794e-08 - acc: 1.0000\n",
      "Epoch 561: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.1657e-08 - acc: 1.0000 - val_loss: 6.7917e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 562/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 8.6657e-08 - acc: 1.0000\n",
      "Epoch 562: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.5632e-08 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 563/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.1996e-07 - acc: 1.0000\n",
      "Epoch 563: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1926e-07 - acc: 1.0000 - val_loss: 6.4507e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 564/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 3.9163e-08 - acc: 1.0000\n",
      "Epoch 564: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.8478e-08 - acc: 1.0000 - val_loss: 6.1096e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 565/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 5.2497e-08 - acc: 1.0000\n",
      "Epoch 565: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.2119e-08 - acc: 1.0000 - val_loss: 6.9337e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 566/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.5097e-08 - acc: 1.0000\n",
      "Epoch 566: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4645e-08 - acc: 1.0000 - val_loss: 6.4222e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 567/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.4993e-07 - acc: 1.0000\n",
      "Epoch 567: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4795e-07 - acc: 1.0000 - val_loss: 6.5075e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 568/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.8978e-08 - acc: 1.0000\n",
      "Epoch 568: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.8822e-08 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 569/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.1817e-08 - acc: 1.0000\n",
      "Epoch 569: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.9924e-08 - acc: 1.0000 - val_loss: 6.5075e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 570/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 5.0030e-08 - acc: 1.0000\n",
      "Epoch 570: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.1473e-08 - acc: 1.0000 - val_loss: 6.7917e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 571/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.3400e-07 - acc: 1.0000\n",
      "Epoch 571: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3326e-07 - acc: 1.0000 - val_loss: 6.6212e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 572/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.1489e-08 - acc: 1.0000\n",
      "Epoch 572: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0784e-08 - acc: 1.0000 - val_loss: 6.4791e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 573/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 6.1263e-08 - acc: 1.0000\n",
      "Epoch 573: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.0456e-08 - acc: 1.0000 - val_loss: 6.4791e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 574/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.2855e-08 - acc: 1.0000\n",
      "Epoch 574: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2959e-08 - acc: 1.0000 - val_loss: 6.8769e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 575/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.0755e-07 - acc: 1.0000\n",
      "Epoch 575: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0560e-07 - acc: 1.0000 - val_loss: 6.2801e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 576/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 7.7674e-08 - acc: 1.0000\n",
      "Epoch 576: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.7427e-08 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 577/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.5567e-08 - acc: 1.0000\n",
      "Epoch 577: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4762e-08 - acc: 1.0000 - val_loss: 6.7917e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 578/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.1461e-08 - acc: 1.0000\n",
      "Epoch 578: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1425e-08 - acc: 1.0000 - val_loss: 6.3654e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 579/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 9.7771e-08 - acc: 1.0000\n",
      "Epoch 579: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.7771e-08 - acc: 1.0000 - val_loss: 6.4222e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 580/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.6650e-07 - acc: 1.0000\n",
      "Epoch 580: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.5899e-07 - acc: 1.0000 - val_loss: 6.3654e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 581/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.3272e-08 - acc: 1.0000\n",
      "Epoch 581: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2151e-08 - acc: 1.0000 - val_loss: 7.2463e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 582/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.0622e-07 - acc: 1.0000\n",
      "Epoch 582: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0366e-07 - acc: 1.0000 - val_loss: 7.2747e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 583/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 9.3019e-06 - acc: 1.0000\n",
      "Epoch 583: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 583: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.9412e-06 - acc: 1.0000 - val_loss: 6.9622e-09 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 584/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.3383e-07 - acc: 1.0000\n",
      "Epoch 584: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3342e-07 - acc: 1.0000 - val_loss: 7.0758e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 585/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.5939e-08 - acc: 1.0000\n",
      "Epoch 585: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.5884e-08 - acc: 1.0000 - val_loss: 6.3654e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 586/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 7.1423e-06 - acc: 1.0000\n",
      "Epoch 586: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.0833e-06 - acc: 1.0000 - val_loss: 6.7632e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 587/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.1653e-08 - acc: 1.0000\n",
      "Epoch 587: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1557e-08 - acc: 1.0000 - val_loss: 6.9622e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 588/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.4126e-08 - acc: 1.0000\n",
      "Epoch 588: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3455e-08 - acc: 1.0000 - val_loss: 6.5075e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 589/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.3625e-08 - acc: 1.0000\n",
      "Epoch 589: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3550e-08 - acc: 1.0000 - val_loss: 7.0474e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 590/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 3.4602e-07 - acc: 1.0000\n",
      "Epoch 590: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.4602e-07 - acc: 1.0000 - val_loss: 6.7917e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 591/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.5993e-08 - acc: 1.0000\n",
      "Epoch 591: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6521e-08 - acc: 1.0000 - val_loss: 6.3086e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 592/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.6998e-08 - acc: 1.0000\n",
      "Epoch 592: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.5789e-08 - acc: 1.0000 - val_loss: 6.2517e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 593/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 8.4647e-08 - acc: 1.0000\n",
      "Epoch 593: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.4378e-08 - acc: 1.0000 - val_loss: 6.0244e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 594/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.5548e-08 - acc: 1.0000\n",
      "Epoch 594: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5100e-08 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 595/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.8347e-07 - acc: 1.0000\n",
      "Epoch 595: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8347e-07 - acc: 1.0000 - val_loss: 6.6212e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 596/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 5.2965e-07 - acc: 1.0000\n",
      "Epoch 596: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.2830e-07 - acc: 1.0000 - val_loss: 6.3370e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 597/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 8.7443e-04 - acc: 0.9998\n",
      "Epoch 597: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.4942e-04 - acc: 0.9998 - val_loss: 6.7632e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 598/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 9.7795e-07 - acc: 1.0000\n",
      "Epoch 598: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.7484e-07 - acc: 1.0000 - val_loss: 6.7917e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 599/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 5.7441e-06 - acc: 1.0000\n",
      "Epoch 599: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.7259e-06 - acc: 1.0000 - val_loss: 6.3086e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 600/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 4.7338e-06 - acc: 1.0000\n",
      "Epoch 600: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.5991e-06 - acc: 1.0000 - val_loss: 6.8201e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 601/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.0601e-07 - acc: 1.0000\n",
      "Epoch 601: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0567e-07 - acc: 1.0000 - val_loss: 6.9053e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 602/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 5.2524e-08 - acc: 1.0000\n",
      "Epoch 602: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.1797e-08 - acc: 1.0000 - val_loss: 6.8201e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 603/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 3.5360e-08 - acc: 1.0000\n",
      "Epoch 603: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.8536e-08 - acc: 1.0000 - val_loss: 6.3086e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 604/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.6483e-08 - acc: 1.0000\n",
      "Epoch 604: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.6272e-08 - acc: 1.0000 - val_loss: 6.9053e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 605/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 3.8456e-08 - acc: 1.0000\n",
      "Epoch 605: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.8138e-08 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 606/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 3.7352e-07 - acc: 1.0000\n",
      "Epoch 606: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.6564e-07 - acc: 1.0000 - val_loss: 6.2233e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 607/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.1577e-07 - acc: 1.0000\n",
      "Epoch 607: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1541e-07 - acc: 1.0000 - val_loss: 6.3938e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 608/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 4.4163e-07 - acc: 1.0000\n",
      "Epoch 608: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.4163e-07 - acc: 1.0000 - val_loss: 6.1381e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 609/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.3430e-08 - acc: 1.0000\n",
      "Epoch 609: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2659e-08 - acc: 1.0000 - val_loss: 6.6496e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 610/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 3.3815e-07 - acc: 1.0000\n",
      "Epoch 610: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2847e-07 - acc: 1.0000 - val_loss: 6.3086e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 611/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 2.3345e-08 - acc: 1.0000\n",
      "Epoch 611: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3171e-08 - acc: 1.0000 - val_loss: 6.0244e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 612/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 8.5392e-08 - acc: 1.0000\n",
      "Epoch 612: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.5121e-08 - acc: 1.0000 - val_loss: 6.4222e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 613/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 7.2542e-08 - acc: 1.0000\n",
      "Epoch 613: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.0344e-08 - acc: 1.0000 - val_loss: 6.3654e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 614/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.9387e-08 - acc: 1.0000\n",
      "Epoch 614: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9325e-08 - acc: 1.0000 - val_loss: 6.7632e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 615/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.4649e-08 - acc: 1.0000\n",
      "Epoch 615: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.4614e-08 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 616/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 5.4074e-07 - acc: 1.0000\n",
      "Epoch 616: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.2258e-07 - acc: 1.0000 - val_loss: 7.1042e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 617/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 7.7938e-08 - acc: 1.0000\n",
      "Epoch 617: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.7938e-08 - acc: 1.0000 - val_loss: 6.4791e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 618/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.2657e-07 - acc: 1.0000\n",
      "Epoch 618: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2585e-07 - acc: 1.0000 - val_loss: 6.8769e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 619/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.2253e-08 - acc: 1.0000\n",
      "Epoch 619: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1541e-08 - acc: 1.0000 - val_loss: 6.1381e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 620/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.1233e-08 - acc: 1.0000\n",
      "Epoch 620: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.0579e-08 - acc: 1.0000 - val_loss: 6.1949e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 621/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 9.0284e-08 - acc: 1.0000\n",
      "Epoch 621: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.9643e-08 - acc: 1.0000 - val_loss: 6.2233e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 622/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 4.9258e-08 - acc: 1.0000\n",
      "Epoch 622: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.9258e-08 - acc: 1.0000 - val_loss: 6.0528e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 623/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 9.7826e-08 - acc: 1.0000\n",
      "Epoch 623: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.9120e-08 - acc: 1.0000 - val_loss: 6.9622e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 624/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.4965e-08 - acc: 1.0000\n",
      "Epoch 624: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4384e-08 - acc: 1.0000 - val_loss: 6.5075e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 625/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 2.9629e-08 - acc: 1.0000\n",
      "Epoch 625: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.9233e-08 - acc: 1.0000 - val_loss: 6.7632e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 626/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.3125e-08 - acc: 1.0000\n",
      "Epoch 626: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.2988e-08 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 627/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.3956e-07 - acc: 1.0000\n",
      "Epoch 627: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3539e-07 - acc: 1.0000 - val_loss: 6.7917e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 628/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.1469e-08 - acc: 1.0000\n",
      "Epoch 628: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0746e-08 - acc: 1.0000 - val_loss: 6.3370e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 629/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.2618e-07 - acc: 1.0000\n",
      "Epoch 629: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2618e-07 - acc: 1.0000 - val_loss: 6.7917e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 630/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 4.2514e-08 - acc: 1.0000\n",
      "Epoch 630: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.2514e-08 - acc: 1.0000 - val_loss: 6.0528e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 631/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 9.2176e-08 - acc: 1.0000\n",
      "Epoch 631: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.1921e-08 - acc: 1.0000 - val_loss: 5.9960e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 632/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 7.7681e-08 - acc: 1.0000\n",
      "Epoch 632: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1710e-07 - acc: 1.0000 - val_loss: 5.7118e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 633/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.3918e-07 - acc: 1.0000\n",
      "Epoch 633: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 633: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3488e-07 - acc: 1.0000 - val_loss: 6.4222e-09 - val_acc: 1.0000 - lr: 9.7656e-07\n",
      "Epoch 634/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.7733e-08 - acc: 1.0000\n",
      "Epoch 634: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7734e-08 - acc: 1.0000 - val_loss: 6.0528e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 635/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.8871e-07 - acc: 1.0000\n",
      "Epoch 635: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.8748e-07 - acc: 1.0000 - val_loss: 6.3086e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 636/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.3831e-08 - acc: 1.0000\n",
      "Epoch 636: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3831e-08 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 637/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.1562e-07 - acc: 1.0000\n",
      "Epoch 637: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1246e-07 - acc: 1.0000 - val_loss: 6.0244e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 638/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 7.3140e-07 - acc: 1.0000\n",
      "Epoch 638: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.3140e-07 - acc: 1.0000 - val_loss: 7.1042e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 639/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 2.4196e-07 - acc: 1.0000\n",
      "Epoch 639: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4000e-07 - acc: 1.0000 - val_loss: 6.0244e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 640/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.4822e-08 - acc: 1.0000\n",
      "Epoch 640: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4232e-08 - acc: 1.0000 - val_loss: 6.2801e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 641/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 5.6709e-07 - acc: 1.0000\n",
      "Epoch 641: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.6548e-07 - acc: 1.0000 - val_loss: 6.3938e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 642/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.2768e-07 - acc: 1.0000\n",
      "Epoch 642: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2663e-07 - acc: 1.0000 - val_loss: 6.3938e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 643/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.2875e-06 - acc: 1.0000\n",
      "Epoch 643: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2875e-06 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 644/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 9.7568e-08 - acc: 1.0000\n",
      "Epoch 644: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.4910e-08 - acc: 1.0000 - val_loss: 6.5643e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 645/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.2122e-06 - acc: 1.0000\n",
      "Epoch 645: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1775e-06 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 646/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.8350e-08 - acc: 1.0000\n",
      "Epoch 646: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.9048e-08 - acc: 1.0000 - val_loss: 6.5643e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 647/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.4236e-08 - acc: 1.0000\n",
      "Epoch 647: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4191e-08 - acc: 1.0000 - val_loss: 6.1096e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 648/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 1.4125e-07 - acc: 1.0000\n",
      "Epoch 648: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3874e-07 - acc: 1.0000 - val_loss: 6.6212e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 649/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.7553e-07 - acc: 1.0000\n",
      "Epoch 649: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7012e-07 - acc: 1.0000 - val_loss: 6.4791e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 650/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 6.0817e-09 - acc: 1.0000\n",
      "Epoch 650: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.0817e-09 - acc: 1.0000 - val_loss: 7.0190e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 651/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.4426e-06 - acc: 1.0000\n",
      "Epoch 651: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3731e-06 - acc: 1.0000 - val_loss: 7.3600e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 652/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 6.0190e-08 - acc: 1.0000\n",
      "Epoch 652: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.8275e-08 - acc: 1.0000 - val_loss: 6.5075e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 653/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.2468e-08 - acc: 1.0000\n",
      "Epoch 653: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.3452e-08 - acc: 1.0000 - val_loss: 7.5021e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 654/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 7.0436e-08 - acc: 1.0000\n",
      "Epoch 654: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.0212e-08 - acc: 1.0000 - val_loss: 6.8769e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 655/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.3961e-08 - acc: 1.0000\n",
      "Epoch 655: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.3821e-08 - acc: 1.0000 - val_loss: 6.9906e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 656/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.3502e-08 - acc: 1.0000\n",
      "Epoch 656: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3436e-08 - acc: 1.0000 - val_loss: 6.3654e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 657/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.8550e-08 - acc: 1.0000\n",
      "Epoch 657: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8567e-08 - acc: 1.0000 - val_loss: 6.4506e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 658/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.5486e-08 - acc: 1.0000\n",
      "Epoch 658: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.0224e-08 - acc: 1.0000 - val_loss: 6.1381e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 659/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.3960e-07 - acc: 1.0000\n",
      "Epoch 659: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.3820e-07 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 660/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.0725e-07 - acc: 1.0000\n",
      "Epoch 660: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0424e-07 - acc: 1.0000 - val_loss: 6.9053e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 661/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.2483e-07 - acc: 1.0000\n",
      "Epoch 661: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.1404e-07 - acc: 1.0000 - val_loss: 6.0812e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 662/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.2501e-08 - acc: 1.0000\n",
      "Epoch 662: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2397e-08 - acc: 1.0000 - val_loss: 6.6212e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 663/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.7103e-08 - acc: 1.0000\n",
      "Epoch 663: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.7017e-08 - acc: 1.0000 - val_loss: 6.0528e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 664/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.4587e-08 - acc: 1.0000\n",
      "Epoch 664: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.5539e-08 - acc: 1.0000 - val_loss: 6.3086e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 665/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.0106e-08 - acc: 1.0000\n",
      "Epoch 665: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.0010e-08 - acc: 1.0000 - val_loss: 6.0244e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 666/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.5131e-06 - acc: 1.0000\n",
      "Epoch 666: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5131e-06 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 667/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.5104e-08 - acc: 1.0000\n",
      "Epoch 667: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.5182e-08 - acc: 1.0000 - val_loss: 7.0474e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 668/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.8027e-08 - acc: 1.0000\n",
      "Epoch 668: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.7630e-08 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 669/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.9055e-08 - acc: 1.0000\n",
      "Epoch 669: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8776e-08 - acc: 1.0000 - val_loss: 6.6212e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 670/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.2882e-08 - acc: 1.0000\n",
      "Epoch 670: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.4134e-08 - acc: 1.0000 - val_loss: 6.7348e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 671/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.2012e-05 - acc: 1.0000\n",
      "Epoch 671: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1942e-05 - acc: 1.0000 - val_loss: 6.8769e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 672/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 3.2252e-06 - acc: 1.0000\n",
      "Epoch 672: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2252e-06 - acc: 1.0000 - val_loss: 6.3938e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 673/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.6351e-08 - acc: 1.0000\n",
      "Epoch 673: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.0698e-08 - acc: 1.0000 - val_loss: 6.0528e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 674/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.6456e-07 - acc: 1.0000\n",
      "Epoch 674: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.7178e-07 - acc: 1.0000 - val_loss: 7.0190e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 675/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.0805e-07 - acc: 1.0000\n",
      "Epoch 675: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0545e-07 - acc: 1.0000 - val_loss: 6.1949e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 676/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 1.2684e-07 - acc: 1.0000\n",
      "Epoch 676: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2515e-07 - acc: 1.0000 - val_loss: 6.2233e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 677/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.6847e-08 - acc: 1.0000\n",
      "Epoch 677: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.6847e-08 - acc: 1.0000 - val_loss: 6.1949e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 678/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.0803e-08 - acc: 1.0000\n",
      "Epoch 678: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0648e-08 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 679/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.9967e-07 - acc: 1.0000\n",
      "Epoch 679: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.6960e-07 - acc: 1.0000 - val_loss: 6.3654e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 680/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.2835e-08 - acc: 1.0000\n",
      "Epoch 680: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.2635e-08 - acc: 1.0000 - val_loss: 6.5643e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 681/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.1198e-06 - acc: 1.0000\n",
      "Epoch 681: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1106e-06 - acc: 1.0000 - val_loss: 7.1327e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 682/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.4615e-07 - acc: 1.0000\n",
      "Epoch 682: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.4615e-07 - acc: 1.0000 - val_loss: 6.5075e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 683/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 9.4014e-08 - acc: 1.0000\n",
      "Epoch 683: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 683: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.1381e-08 - acc: 1.0000 - val_loss: 6.7632e-09 - val_acc: 1.0000 - lr: 4.8828e-07\n",
      "Epoch 684/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.5393e-06 - acc: 1.0000\n",
      "Epoch 684: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5393e-06 - acc: 1.0000 - val_loss: 6.5359e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 685/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.9063e-08 - acc: 1.0000\n",
      "Epoch 685: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.9003e-08 - acc: 1.0000 - val_loss: 6.2233e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 686/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 1.8705e-08 - acc: 1.0000\n",
      "Epoch 686: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.8150e-08 - acc: 1.0000 - val_loss: 6.5643e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 687/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.9333e-08 - acc: 1.0000\n",
      "Epoch 687: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.9113e-08 - acc: 1.0000 - val_loss: 6.6212e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 688/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 7.2201e-08 - acc: 1.0000\n",
      "Epoch 688: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.2201e-08 - acc: 1.0000 - val_loss: 6.4507e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 689/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.5784e-07 - acc: 1.0000\n",
      "Epoch 689: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5575e-07 - acc: 1.0000 - val_loss: 6.3938e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 690/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.2580e-08 - acc: 1.0000\n",
      "Epoch 690: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2508e-08 - acc: 1.0000 - val_loss: 6.5643e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 691/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 4.3888e-07 - acc: 1.0000\n",
      "Epoch 691: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.3748e-07 - acc: 1.0000 - val_loss: 6.3086e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 692/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 3.3666e-08 - acc: 1.0000\n",
      "Epoch 692: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.3666e-08 - acc: 1.0000 - val_loss: 7.2747e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 693/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.5858e-08 - acc: 1.0000\n",
      "Epoch 693: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5858e-08 - acc: 1.0000 - val_loss: 5.8539e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 694/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.5835e-07 - acc: 1.0000\n",
      "Epoch 694: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.5382e-07 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 695/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 9.5868e-07 - acc: 1.0000\n",
      "Epoch 695: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.3670e-07 - acc: 1.0000 - val_loss: 6.6496e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 696/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.0747e-08 - acc: 1.0000\n",
      "Epoch 696: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.0505e-08 - acc: 1.0000 - val_loss: 7.2179e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 697/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 7.6168e-04 - acc: 0.9998\n",
      "Epoch 697: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 7.6168e-04 - acc: 0.9998 - val_loss: 6.4507e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 698/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.1458e-08 - acc: 1.0000\n",
      "Epoch 698: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1390e-08 - acc: 1.0000 - val_loss: 6.7064e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 699/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 3.5713e-08 - acc: 1.0000\n",
      "Epoch 699: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.5713e-08 - acc: 1.0000 - val_loss: 7.5021e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 700/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.6900e-08 - acc: 1.0000\n",
      "Epoch 700: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6900e-08 - acc: 1.0000 - val_loss: 6.7064e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 701/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.8066e-08 - acc: 1.0000\n",
      "Epoch 701: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.7410e-08 - acc: 1.0000 - val_loss: 6.8201e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 702/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 9.1024e-08 - acc: 1.0000\n",
      "Epoch 702: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.0310e-08 - acc: 1.0000 - val_loss: 5.9107e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 703/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.6745e-08 - acc: 1.0000\n",
      "Epoch 703: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.6881e-08 - acc: 1.0000 - val_loss: 6.1096e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 704/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 3.8514e-08 - acc: 1.0000\n",
      "Epoch 704: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.8403e-08 - acc: 1.0000 - val_loss: 6.7632e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 705/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 4.3702e-08 - acc: 1.0000\n",
      "Epoch 705: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.0442e-07 - acc: 1.0000 - val_loss: 6.8485e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 706/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 3.2321e-08 - acc: 1.0000\n",
      "Epoch 706: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2321e-08 - acc: 1.0000 - val_loss: 6.5927e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 707/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.1427e-05 - acc: 1.0000\n",
      "Epoch 707: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1391e-05 - acc: 1.0000 - val_loss: 6.4222e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 708/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 9.2904e-08 - acc: 1.0000\n",
      "Epoch 708: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 9.7387e-08 - acc: 1.0000 - val_loss: 6.1665e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 709/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.9057e-08 - acc: 1.0000\n",
      "Epoch 709: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 8.6194e-08 - acc: 1.0000 - val_loss: 6.5643e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 710/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 3.6302e-08 - acc: 1.0000\n",
      "Epoch 710: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.6206e-08 - acc: 1.0000 - val_loss: 6.5075e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 711/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.3777e-08 - acc: 1.0000\n",
      "Epoch 711: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.3702e-08 - acc: 1.0000 - val_loss: 6.3938e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 712/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.9251e-08 - acc: 1.0000\n",
      "Epoch 712: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.9158e-08 - acc: 1.0000 - val_loss: 6.9053e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 713/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.4461e-08 - acc: 1.0000\n",
      "Epoch 713: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4383e-08 - acc: 1.0000 - val_loss: 6.8201e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 714/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.2281e-08 - acc: 1.0000\n",
      "Epoch 714: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.2281e-08 - acc: 1.0000 - val_loss: 6.6780e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 715/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 3.3202e-07 - acc: 1.0000\n",
      "Epoch 715: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.2767e-07 - acc: 1.0000 - val_loss: 6.6212e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 716/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 2.6601e-08 - acc: 1.0000\n",
      "Epoch 716: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.5994e-08 - acc: 1.0000 - val_loss: 6.7064e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 717/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 7.6581e-06 - acc: 1.0000\n",
      "Epoch 717: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 7.4002e-06 - acc: 1.0000 - val_loss: 6.6212e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 718/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 2.5087e-08 - acc: 1.0000\n",
      "Epoch 718: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4516e-08 - acc: 1.0000 - val_loss: 6.1096e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 719/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 4.5952e-06 - acc: 1.0000\n",
      "Epoch 719: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 4.5573e-06 - acc: 1.0000 - val_loss: 6.1096e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 720/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.0048e-06 - acc: 1.0000\n",
      "Epoch 720: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.0048e-06 - acc: 1.0000 - val_loss: 6.1665e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 721/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 5.1885e-07 - acc: 1.0000\n",
      "Epoch 721: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.0672e-07 - acc: 1.0000 - val_loss: 6.6496e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 722/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.1061e-06 - acc: 1.0000\n",
      "Epoch 722: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.1102e-06 - acc: 1.0000 - val_loss: 6.7917e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 723/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 6.8781e-06 - acc: 1.0000\n",
      "Epoch 723: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.8213e-06 - acc: 1.0000 - val_loss: 6.8485e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 724/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 3.1337e-08 - acc: 1.0000\n",
      "Epoch 724: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 3.1337e-08 - acc: 1.0000 - val_loss: 6.9053e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 725/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 2.8410e-08 - acc: 1.0000\n",
      "Epoch 725: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.8040e-08 - acc: 1.0000 - val_loss: 6.8769e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 726/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 2.5199e-08 - acc: 1.0000\n",
      "Epoch 726: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.4516e-08 - acc: 1.0000 - val_loss: 7.0190e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 727/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 6.1685e-05 - acc: 1.0000\n",
      "Epoch 727: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.1489e-05 - acc: 1.0000 - val_loss: 6.8201e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 728/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.1819e-08 - acc: 1.0000\n",
      "Epoch 728: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 2.1750e-08 - acc: 1.0000 - val_loss: 6.8201e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 729/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 1.8444e-06 - acc: 1.0000\n",
      "Epoch 729: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 1.8196e-06 - acc: 1.0000 - val_loss: 6.9622e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 730/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.2100e-07 - acc: 1.0000\n",
      "Epoch 730: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2100e-07 - acc: 1.0000 - val_loss: 6.4791e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 731/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 5.7064e-08 - acc: 1.0000\n",
      "Epoch 731: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 5.7064e-08 - acc: 1.0000 - val_loss: 7.0758e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 732/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.3341e-08 - acc: 1.0000\n",
      "Epoch 732: val_loss did not improve from 0.00000\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 1.2959e-08 - acc: 1.0000 - val_loss: 6.7632e-09 - val_acc: 1.0000 - lr: 2.4414e-07\n",
      "Epoch 732: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=10000,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")\n",
    "message = {\n",
    "    \"message\" : \"[낙상감지 알림] : 낙상감지 모델의 학습이 성공적으로 완료되었습니다!\"\n",
    "}\n",
    "requests.post(api_url, headers = headers, data = message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[3093,    0],\n",
       "        [   0, 1102]],\n",
       "\n",
       "       [[3227,    0],\n",
       "        [   0,  968]],\n",
       "\n",
       "       [[3052,    0],\n",
       "        [   0, 1143]],\n",
       "\n",
       "       [[3213,    0],\n",
       "        [   0,  982]]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "# print(y_pred.shape)\n",
    "# print(y_val)\n",
    "\n",
    "confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 104ms/step\n",
      "[0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0]\n",
      "(7, 5)\n",
      "(7, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "test_data = np.load(\"dataset/confusion_matrix/seq_fall-2023-1.npy\")\n",
    "\n",
    "test_data = np.nan_to_num(test_data)\n",
    "test_x = test_data[:, :, :-1]\n",
    "test_y = test_data[:, 0, -1]\n",
    "\n",
    "\n",
    "test_y = to_categorical(test_y, num_classes=len(actions))\n",
    "\n",
    "y_pred = model.predict(test_x)\n",
    "print(np.argmax(y_pred, axis=1))\n",
    "print(np.argmax(test_y, axis=1))\n",
    "\n",
    "print(test_y.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "confusion_matrix(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "api_url = \"https://notify-api.line.me/api/notify\"\n",
    "token = \"LYy0yPmrqjMc3rmvdQR2WcbCCVZkmFlf6FZBZGEkpYQ\"\n",
    "headers = {'Authorization':'Bearer '+token}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (2756, 30, 137)\n",
      "2 (4406, 30, 137)\n",
      "3 (5999, 30, 137)\n",
      "4 (8410, 30, 137)\n",
      "(10487, 30, 137)\n"
     ]
    }
   ],
   "source": [
    "actions = [\n",
    "    'fall','stand','walking','lie','sit'\n",
    "]\n",
    "data = np.load(\"./dataset/joint_dataset/seq_fall-2023-1.npy\")\n",
    "datas = np.load(\"./dataset/joint_dataset/seq_stand-2023-1.npy\")\n",
    "datas2 = np.load(\"./dataset/joint_dataset/seq_walking-2023-1.npy\")\n",
    "datas3 = np.load(\"./dataset/joint_dataset/seq_lie-2023-1.npy\")\n",
    "datas4 = np.load(\"./dataset/joint_dataset/seq_sit-2023-1.npy\")\n",
    "for i in range(2,105):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/joint_dataset/seq_fall-2023-{i}.npy')\n",
    "    ], axis=0) \n",
    "print(\"1\",data.shape) \n",
    "# for i in range(2,3):\n",
    "#     data = np.concatenate([\n",
    "#         data,\n",
    "#         np.load(f'./dataset/joint_dataset/seq_stand-2023-{i}.npy')\n",
    "#     ], axis=0)\n",
    "for i in range(2,3):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/joint_dataset/seq_walking-2023-{i}.npy')\n",
    "    ], axis=0)\n",
    "print(\"2\",data.shape) \n",
    "for i in range(2,3):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/joint_dataset/seq_lie-2023-{i}.npy')\n",
    "    ], axis=0) \n",
    "print(\"3\",data.shape) \n",
    "for i in range(2,48):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/joint_dataset/seq_sit-2023-{i}.npy')\n",
    "    ], axis=0) \n",
    "print(\"4\",data.shape) \n",
    "        \n",
    "data = np.concatenate([data,datas2,datas3,datas4])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load(\"dataset/seq_fall-2023-1.npy\")\n",
    "# datas = np.load(\"dataset/seq_stand-2023-1.npy\")\n",
    "# datas2 = np.load(\"dataset/seq_walking-2023-1.npy\")\n",
    "# datas3 = np.load(\"dataset/seq_lie-2023-1.npy\")\n",
    "\n",
    "\n",
    "# for i in range(2,60):\n",
    "#     data = np.concatenate([\n",
    "#         data,\n",
    "#         np.load(f'dataset/seq_fall-2023-{i}.npy')\n",
    "#     ], axis=0)  \n",
    "# print(data.shape)\n",
    "\n",
    "# # fall = 1569\n",
    "# # walking = 2428\n",
    "# # lie = 5710\n",
    "# for i in range(2,5):\n",
    "#     datas3 = np.concatenate([\n",
    "#         datas3,\n",
    "#         np.load(f'dataset/seq_lie-2023-{i}.npy')\n",
    "#     ], axis=0) \n",
    "\n",
    "# print(datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(data[9000:11000,0,-1])\n",
    "# print(data.shape)\n",
    "data = np.nan_to_num(data)\n",
    "\n",
    "\n",
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "# print(x_data.shape)\n",
    "# print(x_data[0,0,:])\n",
    "# print(labels.shape)\n",
    "# print(labels[5000:10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10487, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8389, 30, 136) (8389, 5)\n",
      "(2098, 30, 136) (2098, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2,random_state=2022)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6292, 30, 136) (6292, 5)\n",
      "(4195, 30, 136) (4195, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=5213)\n",
    "\n",
    "for train_idx, test_idx in split.split(x_data,y_data):\n",
    "    x_train = x_data[train_idx]\n",
    "    y_train = y_data[train_idx]\n",
    "    x_val = x_data[test_idx]\n",
    "    y_val = y_data[test_idx]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 334)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 8)                 3504      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1152      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,669\n",
      "Trainable params: 15,413\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(8, dropout=0.2, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.9609 - acc: 0.5952\n",
      "Epoch 1: val_loss improved from inf to 0.68577, saving model to models\\model.h5\n",
      "197/197 [==============================] - 2s 7ms/step - loss: 0.9554 - acc: 0.5969 - val_loss: 0.6858 - val_acc: 0.7354 - lr: 0.0010\n",
      "Epoch 2/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.7023 - acc: 0.7060\n",
      "Epoch 2: val_loss improved from 0.68577 to 0.58778, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.7017 - acc: 0.7061 - val_loss: 0.5878 - val_acc: 0.7383 - lr: 0.0010\n",
      "Epoch 3/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.5327 - acc: 0.7901\n",
      "Epoch 3: val_loss improved from 0.58778 to 0.51191, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.5260 - acc: 0.7931 - val_loss: 0.5119 - val_acc: 0.8010 - lr: 0.0010\n",
      "Epoch 4/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.4105 - acc: 0.8431\n",
      "Epoch 4: val_loss improved from 0.51191 to 0.50056, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.4103 - acc: 0.8425 - val_loss: 0.5006 - val_acc: 0.8021 - lr: 0.0010\n",
      "Epoch 5/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.3251 - acc: 0.8818\n",
      "Epoch 5: val_loss improved from 0.50056 to 0.34927, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.3251 - acc: 0.8818 - val_loss: 0.3493 - val_acc: 0.8567 - lr: 0.0010\n",
      "Epoch 6/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.2719 - acc: 0.8999\n",
      "Epoch 6: val_loss improved from 0.34927 to 0.27705, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2719 - acc: 0.8999 - val_loss: 0.2771 - val_acc: 0.8887 - lr: 0.0010\n",
      "Epoch 7/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.2286 - acc: 0.9166\n",
      "Epoch 7: val_loss improved from 0.27705 to 0.22940, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2263 - acc: 0.9178 - val_loss: 0.2294 - val_acc: 0.9008 - lr: 0.0010\n",
      "Epoch 8/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9189\n",
      "Epoch 8: val_loss did not improve from 0.22940\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2073 - acc: 0.9194 - val_loss: 0.3234 - val_acc: 0.8629 - lr: 0.0010\n",
      "Epoch 9/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9437\n",
      "Epoch 9: val_loss did not improve from 0.22940\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1566 - acc: 0.9436 - val_loss: 0.2497 - val_acc: 0.9092 - lr: 0.0010\n",
      "Epoch 10/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9427\n",
      "Epoch 10: val_loss did not improve from 0.22940\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1570 - acc: 0.9434 - val_loss: 0.2695 - val_acc: 0.8777 - lr: 0.0010\n",
      "Epoch 11/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9482\n",
      "Epoch 11: val_loss improved from 0.22940 to 0.18518, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 0.1385 - acc: 0.9477 - val_loss: 0.1852 - val_acc: 0.9330 - lr: 0.0010\n",
      "Epoch 12/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9607\n",
      "Epoch 12: val_loss did not improve from 0.18518\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1148 - acc: 0.9599 - val_loss: 0.2057 - val_acc: 0.9302 - lr: 0.0010\n",
      "Epoch 13/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.1140 - acc: 0.9634\n",
      "Epoch 13: val_loss did not improve from 0.18518\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1161 - acc: 0.9623 - val_loss: 0.2290 - val_acc: 0.9104 - lr: 0.0010\n",
      "Epoch 14/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9652\n",
      "Epoch 14: val_loss improved from 0.18518 to 0.14603, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1051 - acc: 0.9657 - val_loss: 0.1460 - val_acc: 0.9442 - lr: 0.0010\n",
      "Epoch 15/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9707\n",
      "Epoch 15: val_loss did not improve from 0.14603\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0851 - acc: 0.9706 - val_loss: 0.1923 - val_acc: 0.9228 - lr: 0.0010\n",
      "Epoch 16/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0786 - acc: 0.9716\n",
      "Epoch 16: val_loss improved from 0.14603 to 0.11640, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0791 - acc: 0.9716 - val_loss: 0.1164 - val_acc: 0.9585 - lr: 0.0010\n",
      "Epoch 17/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9711\n",
      "Epoch 17: val_loss did not improve from 0.11640\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0809 - acc: 0.9712 - val_loss: 0.1411 - val_acc: 0.9380 - lr: 0.0010\n",
      "Epoch 18/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9734\n",
      "Epoch 18: val_loss improved from 0.11640 to 0.09338, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0743 - acc: 0.9735 - val_loss: 0.0934 - val_acc: 0.9616 - lr: 0.0010\n",
      "Epoch 19/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9636\n",
      "Epoch 19: val_loss improved from 0.09338 to 0.09280, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1022 - acc: 0.9641 - val_loss: 0.0928 - val_acc: 0.9688 - lr: 0.0010\n",
      "Epoch 20/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9747\n",
      "Epoch 20: val_loss did not improve from 0.09280\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0679 - acc: 0.9749 - val_loss: 0.1406 - val_acc: 0.9578 - lr: 0.0010\n",
      "Epoch 21/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0573 - acc: 0.9803\n",
      "Epoch 21: val_loss improved from 0.09280 to 0.05458, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0573 - acc: 0.9803 - val_loss: 0.0546 - val_acc: 0.9778 - lr: 0.0010\n",
      "Epoch 22/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9802\n",
      "Epoch 22: val_loss did not improve from 0.05458\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0551 - acc: 0.9805 - val_loss: 0.0860 - val_acc: 0.9692 - lr: 0.0010\n",
      "Epoch 23/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9759\n",
      "Epoch 23: val_loss did not improve from 0.05458\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0634 - acc: 0.9758 - val_loss: 0.0762 - val_acc: 0.9685 - lr: 0.0010\n",
      "Epoch 24/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0667 - acc: 0.9762\n",
      "Epoch 24: val_loss did not improve from 0.05458\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0654 - acc: 0.9768 - val_loss: 0.1005 - val_acc: 0.9659 - lr: 0.0010\n",
      "Epoch 25/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9786\n",
      "Epoch 25: val_loss did not improve from 0.05458\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0578 - acc: 0.9787 - val_loss: 0.1186 - val_acc: 0.9609 - lr: 0.0010\n",
      "Epoch 26/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0421 - acc: 0.9859\n",
      "Epoch 26: val_loss did not improve from 0.05458\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0421 - acc: 0.9859 - val_loss: 0.0929 - val_acc: 0.9664 - lr: 0.0010\n",
      "Epoch 27/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0636 - acc: 0.9797\n",
      "Epoch 27: val_loss did not improve from 0.05458\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0617 - acc: 0.9805 - val_loss: 0.1277 - val_acc: 0.9490 - lr: 0.0010\n",
      "Epoch 28/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9848\n",
      "Epoch 28: val_loss did not improve from 0.05458\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0463 - acc: 0.9846 - val_loss: 0.0913 - val_acc: 0.9616 - lr: 0.0010\n",
      "Epoch 29/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0463 - acc: 0.9838\n",
      "Epoch 29: val_loss did not improve from 0.05458\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0463 - acc: 0.9838 - val_loss: 0.0654 - val_acc: 0.9714 - lr: 0.0010\n",
      "Epoch 30/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9849\n",
      "Epoch 30: val_loss did not improve from 0.05458\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0419 - acc: 0.9851 - val_loss: 0.0901 - val_acc: 0.9683 - lr: 0.0010\n",
      "Epoch 31/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0457 - acc: 0.9844\n",
      "Epoch 31: val_loss improved from 0.05458 to 0.03182, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0457 - acc: 0.9844 - val_loss: 0.0318 - val_acc: 0.9886 - lr: 0.0010\n",
      "Epoch 32/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0389 - acc: 0.9879\n",
      "Epoch 32: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0389 - acc: 0.9879 - val_loss: 0.0551 - val_acc: 0.9802 - lr: 0.0010\n",
      "Epoch 33/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9815\n",
      "Epoch 33: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0513 - acc: 0.9814 - val_loss: 0.2547 - val_acc: 0.9216 - lr: 0.0010\n",
      "Epoch 34/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0418 - acc: 0.9860\n",
      "Epoch 34: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0418 - acc: 0.9860 - val_loss: 0.0767 - val_acc: 0.9743 - lr: 0.0010\n",
      "Epoch 35/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0350 - acc: 0.9884\n",
      "Epoch 35: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0350 - acc: 0.9884 - val_loss: 0.1220 - val_acc: 0.9518 - lr: 0.0010\n",
      "Epoch 36/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0480 - acc: 0.9846\n",
      "Epoch 36: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0465 - acc: 0.9851 - val_loss: 0.0497 - val_acc: 0.9766 - lr: 0.0010\n",
      "Epoch 37/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0279 - acc: 0.9905\n",
      "Epoch 37: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0279 - acc: 0.9905 - val_loss: 0.1014 - val_acc: 0.9669 - lr: 0.0010\n",
      "Epoch 38/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9880\n",
      "Epoch 38: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0360 - acc: 0.9881 - val_loss: 0.0536 - val_acc: 0.9785 - lr: 0.0010\n",
      "Epoch 39/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0364 - acc: 0.9866\n",
      "Epoch 39: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0366 - acc: 0.9866 - val_loss: 0.0724 - val_acc: 0.9704 - lr: 0.0010\n",
      "Epoch 40/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9885\n",
      "Epoch 40: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0321 - acc: 0.9886 - val_loss: 0.0667 - val_acc: 0.9814 - lr: 0.0010\n",
      "Epoch 41/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0415 - acc: 0.9870\n",
      "Epoch 41: val_loss did not improve from 0.03182\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0407 - acc: 0.9876 - val_loss: 0.0802 - val_acc: 0.9654 - lr: 0.0010\n",
      "Epoch 42/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9901\n",
      "Epoch 42: val_loss improved from 0.03182 to 0.02344, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0262 - acc: 0.9901 - val_loss: 0.0234 - val_acc: 0.9900 - lr: 0.0010\n",
      "Epoch 43/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0336 - acc: 0.9879\n",
      "Epoch 43: val_loss did not improve from 0.02344\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0336 - acc: 0.9878 - val_loss: 0.0415 - val_acc: 0.9847 - lr: 0.0010\n",
      "Epoch 44/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0299 - acc: 0.9889\n",
      "Epoch 44: val_loss did not improve from 0.02344\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0299 - acc: 0.9889 - val_loss: 0.0423 - val_acc: 0.9833 - lr: 0.0010\n",
      "Epoch 45/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9884\n",
      "Epoch 45: val_loss did not improve from 0.02344\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0288 - acc: 0.9887 - val_loss: 0.2851 - val_acc: 0.9297 - lr: 0.0010\n",
      "Epoch 46/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9861\n",
      "Epoch 46: val_loss did not improve from 0.02344\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0455 - acc: 0.9862 - val_loss: 0.0423 - val_acc: 0.9836 - lr: 0.0010\n",
      "Epoch 47/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9916\n",
      "Epoch 47: val_loss did not improve from 0.02344\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0257 - acc: 0.9917 - val_loss: 0.0263 - val_acc: 0.9905 - lr: 0.0010\n",
      "Epoch 48/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9932\n",
      "Epoch 48: val_loss did not improve from 0.02344\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0184 - acc: 0.9932 - val_loss: 0.0338 - val_acc: 0.9864 - lr: 0.0010\n",
      "Epoch 49/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0361 - acc: 0.9880\n",
      "Epoch 49: val_loss did not improve from 0.02344\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0351 - acc: 0.9884 - val_loss: 0.0266 - val_acc: 0.9912 - lr: 0.0010\n",
      "Epoch 50/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9928\n",
      "Epoch 50: val_loss did not improve from 0.02344\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0199 - acc: 0.9927 - val_loss: 0.1733 - val_acc: 0.9604 - lr: 0.0010\n",
      "Epoch 51/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0371 - acc: 0.9876\n",
      "Epoch 51: val_loss did not improve from 0.02344\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0364 - acc: 0.9878 - val_loss: 0.0269 - val_acc: 0.9905 - lr: 0.0010\n",
      "Epoch 52/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9887\n",
      "Epoch 52: val_loss improved from 0.02344 to 0.02242, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0298 - acc: 0.9886 - val_loss: 0.0224 - val_acc: 0.9914 - lr: 0.0010\n",
      "Epoch 53/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0233 - acc: 0.9920\n",
      "Epoch 53: val_loss improved from 0.02242 to 0.01932, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0233 - acc: 0.9922 - val_loss: 0.0193 - val_acc: 0.9926 - lr: 0.0010\n",
      "Epoch 54/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0280 - acc: 0.9898\n",
      "Epoch 54: val_loss did not improve from 0.01932\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0273 - acc: 0.9903 - val_loss: 0.0236 - val_acc: 0.9902 - lr: 0.0010\n",
      "Epoch 55/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9932\n",
      "Epoch 55: val_loss did not improve from 0.01932\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0202 - acc: 0.9932 - val_loss: 0.0498 - val_acc: 0.9812 - lr: 0.0010\n",
      "Epoch 56/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9904\n",
      "Epoch 56: val_loss improved from 0.01932 to 0.01455, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0274 - acc: 0.9905 - val_loss: 0.0146 - val_acc: 0.9931 - lr: 0.0010\n",
      "Epoch 57/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9919\n",
      "Epoch 57: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0221 - acc: 0.9921 - val_loss: 0.0172 - val_acc: 0.9938 - lr: 0.0010\n",
      "Epoch 58/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0229 - acc: 0.9935\n",
      "Epoch 58: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0229 - acc: 0.9935 - val_loss: 0.0775 - val_acc: 0.9766 - lr: 0.0010\n",
      "Epoch 59/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0391 - acc: 0.9871\n",
      "Epoch 59: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0391 - acc: 0.9871 - val_loss: 0.0303 - val_acc: 0.9928 - lr: 0.0010\n",
      "Epoch 60/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9913\n",
      "Epoch 60: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0241 - acc: 0.9913 - val_loss: 0.0357 - val_acc: 0.9874 - lr: 0.0010\n",
      "Epoch 61/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9924\n",
      "Epoch 61: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0218 - acc: 0.9925 - val_loss: 0.0165 - val_acc: 0.9940 - lr: 0.0010\n",
      "Epoch 62/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0277 - acc: 0.9890\n",
      "Epoch 62: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0275 - acc: 0.9887 - val_loss: 0.0282 - val_acc: 0.9897 - lr: 0.0010\n",
      "Epoch 63/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0252 - acc: 0.9913\n",
      "Epoch 63: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0251 - acc: 0.9911 - val_loss: 0.0167 - val_acc: 0.9940 - lr: 0.0010\n",
      "Epoch 64/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9950\n",
      "Epoch 64: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0158 - acc: 0.9951 - val_loss: 0.0485 - val_acc: 0.9847 - lr: 0.0010\n",
      "Epoch 65/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0213 - acc: 0.9932\n",
      "Epoch 65: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0213 - acc: 0.9932 - val_loss: 0.0256 - val_acc: 0.9878 - lr: 0.0010\n",
      "Epoch 66/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9933\n",
      "Epoch 66: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0201 - acc: 0.9933 - val_loss: 0.0210 - val_acc: 0.9931 - lr: 0.0010\n",
      "Epoch 67/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0235 - acc: 0.9906\n",
      "Epoch 67: val_loss did not improve from 0.01455\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0238 - acc: 0.9908 - val_loss: 0.0165 - val_acc: 0.9933 - lr: 0.0010\n",
      "Epoch 68/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9966\n",
      "Epoch 68: val_loss improved from 0.01455 to 0.01195, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0159 - acc: 0.9967 - val_loss: 0.0119 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 69/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0191 - acc: 0.9935\n",
      "Epoch 69: val_loss did not improve from 0.01195\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0187 - acc: 0.9936 - val_loss: 0.0131 - val_acc: 0.9964 - lr: 0.0010\n",
      "Epoch 70/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9901\n",
      "Epoch 70: val_loss did not improve from 0.01195\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0252 - acc: 0.9900 - val_loss: 0.0386 - val_acc: 0.9838 - lr: 0.0010\n",
      "Epoch 71/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0221 - acc: 0.9919\n",
      "Epoch 71: val_loss did not improve from 0.01195\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0221 - acc: 0.9919 - val_loss: 0.0255 - val_acc: 0.9931 - lr: 0.0010\n",
      "Epoch 72/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9944\n",
      "Epoch 72: val_loss did not improve from 0.01195\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0227 - acc: 0.9943 - val_loss: 0.0194 - val_acc: 0.9928 - lr: 0.0010\n",
      "Epoch 73/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0129 - acc: 0.9955\n",
      "Epoch 73: val_loss did not improve from 0.01195\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0129 - acc: 0.9955 - val_loss: 0.0145 - val_acc: 0.9940 - lr: 0.0010\n",
      "Epoch 74/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0173 - acc: 0.9951\n",
      "Epoch 74: val_loss improved from 0.01195 to 0.00970, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0173 - acc: 0.9951 - val_loss: 0.0097 - val_acc: 0.9957 - lr: 0.0010\n",
      "Epoch 75/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0190 - acc: 0.9935\n",
      "Epoch 75: val_loss did not improve from 0.00970\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0190 - acc: 0.9933 - val_loss: 0.0637 - val_acc: 0.9776 - lr: 0.0010\n",
      "Epoch 76/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0245 - acc: 0.9932\n",
      "Epoch 76: val_loss did not improve from 0.00970\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0245 - acc: 0.9932 - val_loss: 0.0167 - val_acc: 0.9936 - lr: 0.0010\n",
      "Epoch 77/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0136 - acc: 0.9948\n",
      "Epoch 77: val_loss did not improve from 0.00970\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0136 - acc: 0.9948 - val_loss: 0.0257 - val_acc: 0.9928 - lr: 0.0010\n",
      "Epoch 78/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0245 - acc: 0.9934\n",
      "Epoch 78: val_loss did not improve from 0.00970\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0256 - acc: 0.9932 - val_loss: 0.4802 - val_acc: 0.9333 - lr: 0.0010\n",
      "Epoch 79/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9929\n",
      "Epoch 79: val_loss did not improve from 0.00970\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0211 - acc: 0.9930 - val_loss: 0.0992 - val_acc: 0.9731 - lr: 0.0010\n",
      "Epoch 80/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0208 - acc: 0.9926\n",
      "Epoch 80: val_loss did not improve from 0.00970\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0204 - acc: 0.9925 - val_loss: 0.1526 - val_acc: 0.9647 - lr: 0.0010\n",
      "Epoch 81/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0347 - acc: 0.9879\n",
      "Epoch 81: val_loss did not improve from 0.00970\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0347 - acc: 0.9879 - val_loss: 0.0291 - val_acc: 0.9897 - lr: 0.0010\n",
      "Epoch 82/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9923\n",
      "Epoch 82: val_loss improved from 0.00970 to 0.00943, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0265 - acc: 0.9924 - val_loss: 0.0094 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 83/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9936\n",
      "Epoch 83: val_loss did not improve from 0.00943\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0175 - acc: 0.9936 - val_loss: 0.0154 - val_acc: 0.9950 - lr: 0.0010\n",
      "Epoch 84/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0114 - acc: 0.9955\n",
      "Epoch 84: val_loss did not improve from 0.00943\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0112 - acc: 0.9955 - val_loss: 0.0102 - val_acc: 0.9979 - lr: 0.0010\n",
      "Epoch 85/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9960\n",
      "Epoch 85: val_loss did not improve from 0.00943\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0098 - acc: 0.9960 - val_loss: 0.0112 - val_acc: 0.9967 - lr: 0.0010\n",
      "Epoch 86/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9958\n",
      "Epoch 86: val_loss improved from 0.00943 to 0.00629, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0104 - acc: 0.9959 - val_loss: 0.0063 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 87/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9960\n",
      "Epoch 87: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0087 - acc: 0.9960 - val_loss: 0.0259 - val_acc: 0.9933 - lr: 0.0010\n",
      "Epoch 88/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0198 - acc: 0.9924\n",
      "Epoch 88: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0198 - acc: 0.9924 - val_loss: 0.0097 - val_acc: 0.9969 - lr: 0.0010\n",
      "Epoch 89/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9920\n",
      "Epoch 89: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0197 - acc: 0.9921 - val_loss: 0.0534 - val_acc: 0.9807 - lr: 0.0010\n",
      "Epoch 90/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0182 - acc: 0.9942\n",
      "Epoch 90: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0179 - acc: 0.9941 - val_loss: 0.0112 - val_acc: 0.9959 - lr: 0.0010\n",
      "Epoch 91/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9957\n",
      "Epoch 91: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0103 - acc: 0.9957 - val_loss: 0.0158 - val_acc: 0.9950 - lr: 0.0010\n",
      "Epoch 92/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0125 - acc: 0.9963\n",
      "Epoch 92: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0122 - acc: 0.9965 - val_loss: 0.0339 - val_acc: 0.9878 - lr: 0.0010\n",
      "Epoch 93/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0209 - acc: 0.9933\n",
      "Epoch 93: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0312 - val_acc: 0.9931 - lr: 0.0010\n",
      "Epoch 94/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9968\n",
      "Epoch 94: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0101 - acc: 0.9968 - val_loss: 0.0391 - val_acc: 0.9867 - lr: 0.0010\n",
      "Epoch 95/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0204 - acc: 0.9945\n",
      "Epoch 95: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0209 - acc: 0.9941 - val_loss: 0.0797 - val_acc: 0.9809 - lr: 0.0010\n",
      "Epoch 96/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0142 - acc: 0.9952\n",
      "Epoch 96: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0136 - acc: 0.9954 - val_loss: 0.0231 - val_acc: 0.9921 - lr: 0.0010\n",
      "Epoch 97/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0078 - acc: 0.9971\n",
      "Epoch 97: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0078 - acc: 0.9971 - val_loss: 0.0152 - val_acc: 0.9931 - lr: 0.0010\n",
      "Epoch 98/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9950\n",
      "Epoch 98: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0130 - acc: 0.9951 - val_loss: 0.0135 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 99/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0140 - acc: 0.9948\n",
      "Epoch 99: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0137 - acc: 0.9948 - val_loss: 0.0355 - val_acc: 0.9878 - lr: 0.0010\n",
      "Epoch 100/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9951\n",
      "Epoch 100: val_loss did not improve from 0.00629\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0065 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 101/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0111 - acc: 0.9962\n",
      "Epoch 101: val_loss improved from 0.00629 to 0.00404, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0040 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 102/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9970\n",
      "Epoch 102: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0090 - val_acc: 0.9967 - lr: 0.0010\n",
      "Epoch 103/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0095 - acc: 0.9970\n",
      "Epoch 103: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.0342 - val_acc: 0.9893 - lr: 0.0010\n",
      "Epoch 104/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9919\n",
      "Epoch 104: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0203 - acc: 0.9919 - val_loss: 0.0594 - val_acc: 0.9805 - lr: 0.0010\n",
      "Epoch 105/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0224 - acc: 0.9930\n",
      "Epoch 105: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0224 - acc: 0.9930 - val_loss: 0.0072 - val_acc: 0.9969 - lr: 0.0010\n",
      "Epoch 106/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9974\n",
      "Epoch 106: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0086 - acc: 0.9971 - val_loss: 0.0629 - val_acc: 0.9797 - lr: 0.0010\n",
      "Epoch 107/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0121 - acc: 0.9952\n",
      "Epoch 107: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0132 - acc: 0.9946 - val_loss: 0.0102 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 108/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9970\n",
      "Epoch 108: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0128 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 109/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9936\n",
      "Epoch 109: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0677 - val_acc: 0.9795 - lr: 0.0010\n",
      "Epoch 110/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0275 - acc: 0.9911\n",
      "Epoch 110: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0275 - acc: 0.9911 - val_loss: 0.0791 - val_acc: 0.9766 - lr: 0.0010\n",
      "Epoch 111/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9965\n",
      "Epoch 111: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0092 - acc: 0.9965 - val_loss: 0.0395 - val_acc: 0.9917 - lr: 0.0010\n",
      "Epoch 112/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9965\n",
      "Epoch 112: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0197 - val_acc: 0.9952 - lr: 0.0010\n",
      "Epoch 113/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9971\n",
      "Epoch 113: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.0212 - val_acc: 0.9921 - lr: 0.0010\n",
      "Epoch 114/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9973\n",
      "Epoch 114: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0070 - acc: 0.9973 - val_loss: 0.0772 - val_acc: 0.9766 - lr: 0.0010\n",
      "Epoch 115/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 115: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0131 - acc: 0.9962 - val_loss: 0.0096 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 116/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9952\n",
      "Epoch 116: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0159 - acc: 0.9952 - val_loss: 0.4598 - val_acc: 0.9004 - lr: 0.0010\n",
      "Epoch 117/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9931\n",
      "Epoch 117: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0232 - acc: 0.9932 - val_loss: 0.0089 - val_acc: 0.9969 - lr: 0.0010\n",
      "Epoch 118/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9939\n",
      "Epoch 118: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0208 - acc: 0.9940 - val_loss: 0.0210 - val_acc: 0.9919 - lr: 0.0010\n",
      "Epoch 119/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9976\n",
      "Epoch 119: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0065 - acc: 0.9976 - val_loss: 0.0075 - val_acc: 0.9969 - lr: 0.0010\n",
      "Epoch 120/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9971\n",
      "Epoch 120: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0100 - acc: 0.9971 - val_loss: 0.0076 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 121/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9971\n",
      "Epoch 121: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0084 - acc: 0.9971 - val_loss: 0.0203 - val_acc: 0.9919 - lr: 0.0010\n",
      "Epoch 122/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9957\n",
      "Epoch 122: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0124 - acc: 0.9957 - val_loss: 0.0078 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 123/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9941\n",
      "Epoch 123: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0189 - acc: 0.9941 - val_loss: 0.0302 - val_acc: 0.9924 - lr: 0.0010\n",
      "Epoch 124/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9934\n",
      "Epoch 124: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0213 - acc: 0.9930 - val_loss: 0.0323 - val_acc: 0.9883 - lr: 0.0010\n",
      "Epoch 125/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9958\n",
      "Epoch 125: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0129 - acc: 0.9959 - val_loss: 0.0051 - val_acc: 0.9983 - lr: 0.0010\n",
      "Epoch 126/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9955\n",
      "Epoch 126: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0135 - acc: 0.9955 - val_loss: 0.0077 - val_acc: 0.9983 - lr: 0.0010\n",
      "Epoch 127/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9984\n",
      "Epoch 127: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0054 - val_acc: 0.9976 - lr: 0.0010\n",
      "Epoch 128/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9953\n",
      "Epoch 128: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0180 - acc: 0.9954 - val_loss: 0.0376 - val_acc: 0.9902 - lr: 0.0010\n",
      "Epoch 129/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9973\n",
      "Epoch 129: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0065 - acc: 0.9973 - val_loss: 0.0269 - val_acc: 0.9926 - lr: 0.0010\n",
      "Epoch 130/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0070 - acc: 0.9976\n",
      "Epoch 130: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0070 - acc: 0.9976 - val_loss: 0.0143 - val_acc: 0.9945 - lr: 0.0010\n",
      "Epoch 131/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9967\n",
      "Epoch 131: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0109 - acc: 0.9968 - val_loss: 0.0259 - val_acc: 0.9912 - lr: 0.0010\n",
      "Epoch 132/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9956\n",
      "Epoch 132: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0135 - acc: 0.9955 - val_loss: 0.0693 - val_acc: 0.9759 - lr: 0.0010\n",
      "Epoch 133/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9967\n",
      "Epoch 133: val_loss did not improve from 0.00404\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0109 - acc: 0.9967 - val_loss: 0.0336 - val_acc: 0.9890 - lr: 0.0010\n",
      "Epoch 134/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9965\n",
      "Epoch 134: val_loss improved from 0.00404 to 0.00401, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.0040 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 135/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0145 - acc: 0.9951\n",
      "Epoch 135: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0145 - acc: 0.9951 - val_loss: 0.0323 - val_acc: 0.9888 - lr: 0.0010\n",
      "Epoch 136/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9893\n",
      "Epoch 136: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0347 - acc: 0.9894 - val_loss: 0.0092 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 137/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 137: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0065 - val_acc: 0.9976 - lr: 0.0010\n",
      "Epoch 138/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9965\n",
      "Epoch 138: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0207 - val_acc: 0.9940 - lr: 0.0010\n",
      "Epoch 139/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0164 - acc: 0.9952\n",
      "Epoch 139: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0164 - acc: 0.9952 - val_loss: 0.0056 - val_acc: 0.9986 - lr: 0.0010\n",
      "Epoch 140/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9973\n",
      "Epoch 140: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0068 - val_acc: 0.9986 - lr: 0.0010\n",
      "Epoch 141/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9982\n",
      "Epoch 141: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0052 - acc: 0.9983 - val_loss: 0.0276 - val_acc: 0.9921 - lr: 0.0010\n",
      "Epoch 142/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0084 - acc: 0.9967\n",
      "Epoch 142: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0084 - acc: 0.9967 - val_loss: 0.0041 - val_acc: 0.9988 - lr: 0.0010\n",
      "Epoch 143/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9984\n",
      "Epoch 143: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0051 - val_acc: 0.9986 - lr: 0.0010\n",
      "Epoch 144/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0075 - acc: 0.9981\n",
      "Epoch 144: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0052 - val_acc: 0.9986 - lr: 0.0010\n",
      "Epoch 145/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0336 - acc: 0.9897\n",
      "Epoch 145: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0336 - acc: 0.9897 - val_loss: 0.0281 - val_acc: 0.9902 - lr: 0.0010\n",
      "Epoch 146/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0148 - acc: 0.9949\n",
      "Epoch 146: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0148 - acc: 0.9949 - val_loss: 0.0122 - val_acc: 0.9969 - lr: 0.0010\n",
      "Epoch 147/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0095 - acc: 0.9971\n",
      "Epoch 147: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0095 - acc: 0.9971 - val_loss: 0.0053 - val_acc: 0.9986 - lr: 0.0010\n",
      "Epoch 148/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9953\n",
      "Epoch 148: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0141 - acc: 0.9954 - val_loss: 0.0160 - val_acc: 0.9948 - lr: 0.0010\n",
      "Epoch 149/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9969\n",
      "Epoch 149: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0165 - val_acc: 0.9952 - lr: 0.0010\n",
      "Epoch 150/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9947\n",
      "Epoch 150: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0148 - acc: 0.9948 - val_loss: 0.1579 - val_acc: 0.9585 - lr: 0.0010\n",
      "Epoch 151/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9963\n",
      "Epoch 151: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.0046 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 152/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9981\n",
      "Epoch 152: val_loss did not improve from 0.00401\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.0043 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 153/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 153: val_loss improved from 0.00401 to 0.00373, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.0037 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 154/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9984\n",
      "Epoch 154: val_loss improved from 0.00373 to 0.00318, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0032 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 155/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 155: val_loss did not improve from 0.00318\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0058 - val_acc: 0.9976 - lr: 5.0000e-04\n",
      "Epoch 156/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9987\n",
      "Epoch 156: val_loss did not improve from 0.00318\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0071 - val_acc: 0.9969 - lr: 5.0000e-04\n",
      "Epoch 157/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 157: val_loss did not improve from 0.00318\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0118 - val_acc: 0.9948 - lr: 5.0000e-04\n",
      "Epoch 158/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9982\n",
      "Epoch 158: val_loss improved from 0.00318 to 0.00311, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0052 - acc: 0.9983 - val_loss: 0.0031 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 159/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9981\n",
      "Epoch 159: val_loss did not improve from 0.00311\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0057 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 160/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9990\n",
      "Epoch 160: val_loss did not improve from 0.00311\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0046 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 161/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0078 - acc: 0.9979\n",
      "Epoch 161: val_loss did not improve from 0.00311\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0397 - val_acc: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 162/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0047 - acc: 0.9985\n",
      "Epoch 162: val_loss did not improve from 0.00311\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0049 - val_acc: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 163/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9982\n",
      "Epoch 163: val_loss did not improve from 0.00311\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0285 - val_acc: 0.9869 - lr: 5.0000e-04\n",
      "Epoch 164/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9978\n",
      "Epoch 164: val_loss did not improve from 0.00311\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0063 - acc: 0.9978 - val_loss: 0.0289 - val_acc: 0.9933 - lr: 5.0000e-04\n",
      "Epoch 165/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9986\n",
      "Epoch 165: val_loss did not improve from 0.00311\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0074 - val_acc: 0.9976 - lr: 5.0000e-04\n",
      "Epoch 166/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9985\n",
      "Epoch 166: val_loss did not improve from 0.00311\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0107 - val_acc: 0.9959 - lr: 5.0000e-04\n",
      "Epoch 167/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9982\n",
      "Epoch 167: val_loss improved from 0.00311 to 0.00156, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 0.0016 - val_acc: 0.9998 - lr: 5.0000e-04\n",
      "Epoch 168/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9984\n",
      "Epoch 168: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0070 - acc: 0.9983 - val_loss: 0.0035 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 169/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0044 - acc: 0.9983\n",
      "Epoch 169: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0044 - acc: 0.9983 - val_loss: 0.0034 - val_acc: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 170/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9981\n",
      "Epoch 170: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0042 - acc: 0.9981 - val_loss: 0.0057 - val_acc: 0.9986 - lr: 5.0000e-04\n",
      "Epoch 171/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9984\n",
      "Epoch 171: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0086 - val_acc: 0.9976 - lr: 5.0000e-04\n",
      "Epoch 172/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 172: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0031 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 173/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9990\n",
      "Epoch 173: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0022 - acc: 0.9990 - val_loss: 0.0023 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 174/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0037 - acc: 0.9986\n",
      "Epoch 174: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0021 - val_acc: 0.9998 - lr: 5.0000e-04\n",
      "Epoch 175/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9976\n",
      "Epoch 175: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0285 - val_acc: 0.9878 - lr: 5.0000e-04\n",
      "Epoch 176/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9986\n",
      "Epoch 176: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0059 - val_acc: 0.9974 - lr: 5.0000e-04\n",
      "Epoch 177/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 177: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0031 - val_acc: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 178/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9990\n",
      "Epoch 178: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.0091 - val_acc: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 179/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9992\n",
      "Epoch 179: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0059 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 180/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 180: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.0063 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 181/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9985\n",
      "Epoch 181: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.0094 - val_acc: 0.9969 - lr: 5.0000e-04\n",
      "Epoch 182/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0064 - acc: 0.9981\n",
      "Epoch 182: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0190 - val_acc: 0.9928 - lr: 5.0000e-04\n",
      "Epoch 183/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9989\n",
      "Epoch 183: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.0069 - val_acc: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 184/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 184: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0155 - val_acc: 0.9943 - lr: 5.0000e-04\n",
      "Epoch 185/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9981\n",
      "Epoch 185: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.0035 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 186/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9979\n",
      "Epoch 186: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0055 - acc: 0.9978 - val_loss: 0.0140 - val_acc: 0.9950 - lr: 5.0000e-04\n",
      "Epoch 187/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9974\n",
      "Epoch 187: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0124 - val_acc: 0.9950 - lr: 5.0000e-04\n",
      "Epoch 188/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0071 - acc: 0.9978\n",
      "Epoch 188: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0044 - val_acc: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 189/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 189: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0149 - val_acc: 0.9938 - lr: 5.0000e-04\n",
      "Epoch 190/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 190: val_loss did not improve from 0.00156\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.0063 - val_acc: 0.9969 - lr: 5.0000e-04\n",
      "Epoch 191/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9976\n",
      "Epoch 191: val_loss improved from 0.00156 to 0.00154, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0015 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 192/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9982\n",
      "Epoch 192: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0036 - acc: 0.9983 - val_loss: 0.0781 - val_acc: 0.9766 - lr: 5.0000e-04\n",
      "Epoch 193/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 193: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 0.0068 - val_acc: 0.9971 - lr: 5.0000e-04\n",
      "Epoch 194/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9979\n",
      "Epoch 194: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0056 - acc: 0.9979 - val_loss: 0.0017 - val_acc: 0.9998 - lr: 5.0000e-04\n",
      "Epoch 195/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 195: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0018 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 196/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9987\n",
      "Epoch 196: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0028 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 197/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 197: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0222 - val_acc: 0.9945 - lr: 5.0000e-04\n",
      "Epoch 198/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9989\n",
      "Epoch 198: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0037 - val_acc: 0.9986 - lr: 5.0000e-04\n",
      "Epoch 199/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9984\n",
      "Epoch 199: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0052 - val_acc: 0.9976 - lr: 5.0000e-04\n",
      "Epoch 200/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9978\n",
      "Epoch 200: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0052 - acc: 0.9978 - val_loss: 0.0036 - val_acc: 0.9986 - lr: 5.0000e-04\n",
      "Epoch 201/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9979\n",
      "Epoch 201: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.0135 - val_acc: 0.9959 - lr: 5.0000e-04\n",
      "Epoch 202/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9988\n",
      "Epoch 202: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0136 - val_acc: 0.9945 - lr: 5.0000e-04\n",
      "Epoch 203/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9992\n",
      "Epoch 203: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0044 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 204/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 204: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0078 - val_acc: 0.9971 - lr: 5.0000e-04\n",
      "Epoch 205/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9984\n",
      "Epoch 205: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0044 - acc: 0.9984 - val_loss: 0.0077 - val_acc: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 206/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0020 - acc: 0.9992\n",
      "Epoch 206: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0044 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 207/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0080 - acc: 0.9978\n",
      "Epoch 207: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0057 - val_acc: 0.9986 - lr: 5.0000e-04\n",
      "Epoch 208/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9987\n",
      "Epoch 208: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0037 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 209/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9987\n",
      "Epoch 209: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0300 - val_acc: 0.9919 - lr: 5.0000e-04\n",
      "Epoch 210/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9955\n",
      "Epoch 210: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0162 - acc: 0.9955 - val_loss: 0.0097 - val_acc: 0.9967 - lr: 5.0000e-04\n",
      "Epoch 211/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9985\n",
      "Epoch 211: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0040 - acc: 0.9986 - val_loss: 0.0155 - val_acc: 0.9945 - lr: 5.0000e-04\n",
      "Epoch 212/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 212: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0037 - val_acc: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 213/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9986\n",
      "Epoch 213: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0027 - val_acc: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 214/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 214: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0020 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 215/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 215: val_loss did not improve from 0.00154\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0033 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 216/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
      "Epoch 216: val_loss improved from 0.00154 to 0.00141, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0014 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 217/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9982\n",
      "Epoch 217: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0022 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 218/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9992\n",
      "Epoch 218: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0032 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 219/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 219: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0099 - val_acc: 0.9959 - lr: 5.0000e-04\n",
      "Epoch 220/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 220: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0024 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 221/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9984\n",
      "Epoch 221: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0048 - val_acc: 0.9986 - lr: 5.0000e-04\n",
      "Epoch 222/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9988\n",
      "Epoch 222: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0031 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 223/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9990\n",
      "Epoch 223: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0037 - val_acc: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 224/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9990\n",
      "Epoch 224: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0028 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 225/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9989\n",
      "Epoch 225: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 0.0140 - val_acc: 0.9943 - lr: 5.0000e-04\n",
      "Epoch 226/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9981\n",
      "Epoch 226: val_loss did not improve from 0.00141\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0078 - acc: 0.9981 - val_loss: 0.0697 - val_acc: 0.9833 - lr: 5.0000e-04\n",
      "Epoch 227/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9985\n",
      "Epoch 227: val_loss improved from 0.00141 to 0.00121, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0012 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 228/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 228: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0019 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 229/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 229: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0060 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 230/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9993 \n",
      "Epoch 230: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0016 - acc: 0.9992 - val_loss: 0.0079 - val_acc: 0.9971 - lr: 5.0000e-04\n",
      "Epoch 231/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9995\n",
      "Epoch 231: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.0017 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 232/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 232: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0102 - val_acc: 0.9957 - lr: 5.0000e-04\n",
      "Epoch 233/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9971\n",
      "Epoch 233: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0061 - acc: 0.9971 - val_loss: 0.0162 - val_acc: 0.9936 - lr: 5.0000e-04\n",
      "Epoch 234/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9985\n",
      "Epoch 234: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.0058 - val_acc: 0.9974 - lr: 5.0000e-04\n",
      "Epoch 235/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0049 - acc: 0.9981\n",
      "Epoch 235: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.0023 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 236/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9989\n",
      "Epoch 236: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0029 - acc: 0.9989 - val_loss: 0.0475 - val_acc: 0.9838 - lr: 5.0000e-04\n",
      "Epoch 237/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
      "Epoch 237: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9989 - val_loss: 0.0314 - val_acc: 0.9945 - lr: 5.0000e-04\n",
      "Epoch 238/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9993\n",
      "Epoch 238: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0028 - val_acc: 0.9986 - lr: 5.0000e-04\n",
      "Epoch 239/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9985\n",
      "Epoch 239: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0098 - val_acc: 0.9974 - lr: 5.0000e-04\n",
      "Epoch 240/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 240: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 0.0051 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 241/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9987\n",
      "Epoch 241: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0843 - val_acc: 0.9721 - lr: 5.0000e-04\n",
      "Epoch 242/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995  \n",
      "Epoch 242: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0096 - val_acc: 0.9967 - lr: 5.0000e-04\n",
      "Epoch 243/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9982\n",
      "Epoch 243: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0042 - acc: 0.9983 - val_loss: 0.0060 - val_acc: 0.9976 - lr: 5.0000e-04\n",
      "Epoch 244/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9974\n",
      "Epoch 244: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0060 - acc: 0.9975 - val_loss: 0.0025 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 245/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9995\n",
      "Epoch 245: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0039 - acc: 0.9995 - val_loss: 0.0028 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 246/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9966\n",
      "Epoch 246: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0136 - acc: 0.9967 - val_loss: 0.0188 - val_acc: 0.9974 - lr: 5.0000e-04\n",
      "Epoch 247/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 247: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0023 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 248/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9984\n",
      "Epoch 248: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0042 - acc: 0.9984 - val_loss: 0.0014 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 249/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9992\n",
      "Epoch 249: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0022 - acc: 0.9990 - val_loss: 0.0015 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 250/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.9989\n",
      "Epoch 250: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0018 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 251/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9990\n",
      "Epoch 251: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0019 - val_acc: 0.9998 - lr: 5.0000e-04\n",
      "Epoch 252/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9993\n",
      "Epoch 252: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 253/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 253: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0035 - acc: 0.9992 - val_loss: 0.0036 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 254/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9987\n",
      "Epoch 254: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 0.0078 - val_acc: 0.9976 - lr: 5.0000e-04\n",
      "Epoch 255/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9993\n",
      "Epoch 255: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0018 - acc: 0.9992 - val_loss: 0.0045 - val_acc: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 256/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 256: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0013 - val_acc: 0.9998 - lr: 5.0000e-04\n",
      "Epoch 257/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0046 - acc: 0.9983\n",
      "Epoch 257: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0044 - acc: 0.9984 - val_loss: 0.0053 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 258/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9985\n",
      "Epoch 258: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0047 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 259/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 259: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0019 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 260/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 260: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0276 - val_acc: 0.9928 - lr: 5.0000e-04\n",
      "Epoch 261/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 261: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0047 - val_acc: 0.9974 - lr: 5.0000e-04\n",
      "Epoch 262/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0048 - acc: 0.9982\n",
      "Epoch 262: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0046 - acc: 0.9983 - val_loss: 0.0045 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 263/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 263: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0075 - acc: 0.9978 - val_loss: 0.0044 - val_acc: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 264/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9987\n",
      "Epoch 264: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0047 - val_acc: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 265/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 265: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0103 - val_acc: 0.9962 - lr: 5.0000e-04\n",
      "Epoch 266/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9982\n",
      "Epoch 266: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.0059 - val_acc: 0.9979 - lr: 5.0000e-04\n",
      "Epoch 267/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0055 - acc: 0.9987\n",
      "Epoch 267: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.0268 - val_acc: 0.9917 - lr: 5.0000e-04\n",
      "Epoch 268/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0054 - acc: 0.9988\n",
      "Epoch 268: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0052 - acc: 0.9989 - val_loss: 0.0174 - val_acc: 0.9938 - lr: 5.0000e-04\n",
      "Epoch 269/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9990\n",
      "Epoch 269: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0103 - val_acc: 0.9950 - lr: 5.0000e-04\n",
      "Epoch 270/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 270: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0048 - val_acc: 0.9981 - lr: 5.0000e-04\n",
      "Epoch 271/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0193 - acc: 0.9944\n",
      "Epoch 271: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0193 - acc: 0.9944 - val_loss: 0.0105 - val_acc: 0.9967 - lr: 5.0000e-04\n",
      "Epoch 272/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9985\n",
      "Epoch 272: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0038 - acc: 0.9984 - val_loss: 0.0026 - val_acc: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 273/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9990\n",
      "Epoch 273: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 0.0032 - val_acc: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 274/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9992\n",
      "Epoch 274: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0025 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 275/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 275: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 0.0045 - val_acc: 0.9983 - lr: 5.0000e-04\n",
      "Epoch 276/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9995\n",
      "Epoch 276: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0040 - acc: 0.9995 - val_loss: 0.0028 - val_acc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 277/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9997 \n",
      "Epoch 277: val_loss did not improve from 0.00121\n",
      "\n",
      "Epoch 277: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0038 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 278/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0018 - acc: 0.9994\n",
      "Epoch 278: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0020 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 279/10000\n",
      "186/197 [===========================>..] - ETA: 0s - loss: 0.0013 - acc: 0.9992    \n",
      "Epoch 279: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0012 - acc: 0.9992 - val_loss: 0.0026 - val_acc: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 280/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9993\n",
      "Epoch 280: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 281/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 281: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0036 - val_acc: 0.9986 - lr: 2.5000e-04\n",
      "Epoch 282/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 282: val_loss did not improve from 0.00121\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0033 - val_acc: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 283/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9995\n",
      "Epoch 283: val_loss improved from 0.00121 to 0.00037, saving model to models\\model.h5\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0028 - acc: 0.9995 - val_loss: 3.6602e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 284/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9988\n",
      "Epoch 284: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9989 - val_loss: 0.0030 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 285/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 8.8449e-04 - acc: 0.9997\n",
      "Epoch 285: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 8.5278e-04 - acc: 0.9997 - val_loss: 0.0020 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 286/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9974\n",
      "Epoch 286: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0078 - acc: 0.9975 - val_loss: 0.0034 - val_acc: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 287/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 287: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0031 - val_acc: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 288/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 288: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0025 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 289/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 9.0302e-04 - acc: 0.9998\n",
      "Epoch 289: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 8.6393e-04 - acc: 0.9998 - val_loss: 0.0016 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 290/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 8.6720e-04 - acc: 0.9997\n",
      "Epoch 290: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 8.3448e-04 - acc: 0.9997 - val_loss: 0.0021 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 291/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9988\n",
      "Epoch 291: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0049 - val_acc: 0.9986 - lr: 2.5000e-04\n",
      "Epoch 292/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 292: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 8.5433e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 293/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 293: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0021 - val_acc: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 294/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9997\n",
      "Epoch 294: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9997 - val_loss: 0.0017 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 295/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 295: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0020 - val_acc: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 296/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 7.6711e-04 - acc: 0.9997\n",
      "Epoch 296: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.4642e-04 - acc: 0.9997 - val_loss: 0.0021 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 297/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 3.6441e-04 - acc: 1.0000\n",
      "Epoch 297: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 3.7137e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 298/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 7.4023e-04 - acc: 0.9997\n",
      "Epoch 298: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.1785e-04 - acc: 0.9997 - val_loss: 0.0017 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 299/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989\n",
      "Epoch 299: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0054 - val_acc: 0.9979 - lr: 2.5000e-04\n",
      "Epoch 300/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 7.7983e-04 - acc: 0.9997\n",
      "Epoch 300: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 8.2562e-04 - acc: 0.9997 - val_loss: 0.0013 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 301/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 301: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0015 - val_acc: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 302/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 302: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0011 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 303/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9992\n",
      "Epoch 303: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0012 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 304/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0015 - acc: 0.9993\n",
      "Epoch 304: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0020 - val_acc: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 305/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0016 - acc: 0.9997 \n",
      "Epoch 305: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0020 - val_acc: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 306/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 6.9907e-04 - acc: 0.9998\n",
      "Epoch 306: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 6.9309e-04 - acc: 0.9998 - val_loss: 0.0017 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 307/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 307: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.0069 - val_acc: 0.9971 - lr: 2.5000e-04\n",
      "Epoch 308/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 308: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0035 - acc: 0.9992 - val_loss: 0.0030 - val_acc: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 309/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 7.6839e-04 - acc: 0.9997\n",
      "Epoch 309: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.4897e-04 - acc: 0.9997 - val_loss: 0.0028 - val_acc: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 310/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9992\n",
      "Epoch 310: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0019 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 311/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9998 \n",
      "Epoch 311: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.0038 - val_acc: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 312/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 312: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.0035 - val_acc: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 313/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9987\n",
      "Epoch 313: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 314/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 314: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0040 - val_acc: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 315/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 315: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.0021 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 316/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0028 - acc: 0.9990\n",
      "Epoch 316: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0015 - val_acc: 0.9998 - lr: 2.5000e-04\n",
      "Epoch 317/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 8.0633e-04 - acc: 0.9997\n",
      "Epoch 317: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.9064e-04 - acc: 0.9997 - val_loss: 0.0022 - val_acc: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 318/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9992\n",
      "Epoch 318: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0049 - val_acc: 0.9981 - lr: 2.5000e-04\n",
      "Epoch 319/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9992\n",
      "Epoch 319: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.0176 - val_acc: 0.9955 - lr: 2.5000e-04\n",
      "Epoch 320/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9990\n",
      "Epoch 320: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0024 - acc: 0.9990 - val_loss: 0.0028 - val_acc: 0.9986 - lr: 2.5000e-04\n",
      "Epoch 321/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 6.5562e-04 - acc: 0.9998\n",
      "Epoch 321: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 6.3890e-04 - acc: 0.9998 - val_loss: 0.0040 - val_acc: 0.9979 - lr: 2.5000e-04\n",
      "Epoch 322/10000\n",
      "186/197 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 322: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0031 - val_acc: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 323/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 6.1243e-04 - acc: 0.9997\n",
      "Epoch 323: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 5.9373e-04 - acc: 0.9997 - val_loss: 7.9407e-04 - val_acc: 0.9998 - lr: 2.5000e-04\n",
      "Epoch 324/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9994\n",
      "Epoch 324: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0042 - val_acc: 0.9979 - lr: 2.5000e-04\n",
      "Epoch 325/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9992\n",
      "Epoch 325: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0118 - val_acc: 0.9959 - lr: 2.5000e-04\n",
      "Epoch 326/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0017 - acc: 0.9994\n",
      "Epoch 326: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0099 - val_acc: 0.9967 - lr: 2.5000e-04\n",
      "Epoch 327/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 9.2654e-04 - acc: 0.9998\n",
      "Epoch 327: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 9.1426e-04 - acc: 0.9998 - val_loss: 0.0018 - val_acc: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 328/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9994\n",
      "Epoch 328: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.0019 - val_acc: 0.9998 - lr: 2.5000e-04\n",
      "Epoch 329/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.4165e-04 - acc: 0.9998\n",
      "Epoch 329: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 8.1401e-04 - acc: 0.9998 - val_loss: 0.0079 - val_acc: 0.9981 - lr: 2.5000e-04\n",
      "Epoch 330/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0016 - acc: 0.9994\n",
      "Epoch 330: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0029 - val_acc: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 331/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 7.1044e-04 - acc: 0.9998\n",
      "Epoch 331: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.4224e-04 - acc: 0.9998 - val_loss: 0.0034 - val_acc: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 332/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9993\n",
      "Epoch 332: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0041 - val_acc: 0.9986 - lr: 2.5000e-04\n",
      "Epoch 333/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 2.0830e-04 - acc: 1.0000\n",
      "Epoch 333: val_loss did not improve from 0.00037\n",
      "\n",
      "Epoch 333: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 1.9941e-04 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9986 - lr: 2.5000e-04\n",
      "Epoch 334/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 334: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0026 - val_acc: 0.9990 - lr: 1.2500e-04\n",
      "Epoch 335/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 9.5773e-04 - acc: 0.9997\n",
      "Epoch 335: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 9.4983e-04 - acc: 0.9997 - val_loss: 0.0030 - val_acc: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 336/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 7.1710e-04 - acc: 0.9998\n",
      "Epoch 336: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.0451e-04 - acc: 0.9998 - val_loss: 0.0018 - val_acc: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 337/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 337: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0015 - val_acc: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 338/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997   \n",
      "Epoch 338: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.0071 - val_acc: 0.9981 - lr: 1.2500e-04\n",
      "Epoch 339/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 8.9863e-04 - acc: 0.9998\n",
      "Epoch 339: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 8.7968e-04 - acc: 0.9998 - val_loss: 0.0014 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 340/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.1504e-04 - acc: 1.0000\n",
      "Epoch 340: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 2.1504e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 341/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 1.9069e-04 - acc: 1.0000\n",
      "Epoch 341: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 1.9006e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 342/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 2.8748e-04 - acc: 0.9998\n",
      "Epoch 342: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 2.8406e-04 - acc: 0.9998 - val_loss: 0.0014 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 343/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 343: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 7.9310e-04 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 344/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.9717e-04 - acc: 1.0000\n",
      "Epoch 344: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 1.9717e-04 - acc: 1.0000 - val_loss: 6.7536e-04 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 345/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 7.6179e-04 - acc: 0.9998\n",
      "Epoch 345: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.5167e-04 - acc: 0.9998 - val_loss: 6.2417e-04 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 346/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 5.2338e-04 - acc: 0.9997\n",
      "Epoch 346: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 5.2190e-04 - acc: 0.9997 - val_loss: 7.3898e-04 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 347/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 6.4305e-04 - acc: 0.9997\n",
      "Epoch 347: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 6.4305e-04 - acc: 0.9997 - val_loss: 0.0025 - val_acc: 0.9986 - lr: 1.2500e-04\n",
      "Epoch 348/10000\n",
      "195/197 [============================>.] - ETA: 0s - loss: 5.7270e-04 - acc: 0.9998\n",
      "Epoch 348: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 5.6806e-04 - acc: 0.9998 - val_loss: 0.0036 - val_acc: 0.9990 - lr: 1.2500e-04\n",
      "Epoch 349/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 4.8527e-04 - acc: 0.9997\n",
      "Epoch 349: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 4.7681e-04 - acc: 0.9997 - val_loss: 0.0029 - val_acc: 0.9990 - lr: 1.2500e-04\n",
      "Epoch 350/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 1.8813e-04 - acc: 1.0000\n",
      "Epoch 350: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 1.9773e-04 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9983 - lr: 1.2500e-04\n",
      "Epoch 351/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 1.2851e-04 - acc: 1.0000\n",
      "Epoch 351: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 1.2851e-04 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 0.9979 - lr: 1.2500e-04\n",
      "Epoch 352/10000\n",
      "194/197 [============================>.] - ETA: 0s - loss: 4.1860e-04 - acc: 0.9997\n",
      "Epoch 352: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 4.5715e-04 - acc: 0.9997 - val_loss: 0.0064 - val_acc: 0.9979 - lr: 1.2500e-04\n",
      "Epoch 353/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 2.8318e-04 - acc: 0.9998\n",
      "Epoch 353: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 2.8238e-04 - acc: 0.9998 - val_loss: 0.0076 - val_acc: 0.9964 - lr: 1.2500e-04\n",
      "Epoch 354/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 354: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0171 - val_acc: 0.9940 - lr: 1.2500e-04\n",
      "Epoch 355/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 6.3467e-04 - acc: 0.9997\n",
      "Epoch 355: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 6.1080e-04 - acc: 0.9997 - val_loss: 0.0078 - val_acc: 0.9971 - lr: 1.2500e-04\n",
      "Epoch 356/10000\n",
      "188/197 [===========================>..] - ETA: 0s - loss: 2.2131e-04 - acc: 1.0000\n",
      "Epoch 356: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 2.4899e-04 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9983 - lr: 1.2500e-04\n",
      "Epoch 357/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9993\n",
      "Epoch 357: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0026 - val_acc: 0.9990 - lr: 1.2500e-04\n",
      "Epoch 358/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9995\n",
      "Epoch 358: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0010 - acc: 0.9995 - val_loss: 0.0026 - val_acc: 0.9990 - lr: 1.2500e-04\n",
      "Epoch 359/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 6.9403e-04 - acc: 0.9997\n",
      "Epoch 359: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 6.6853e-04 - acc: 0.9997 - val_loss: 0.0021 - val_acc: 0.9988 - lr: 1.2500e-04\n",
      "Epoch 360/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 9.6751e-04 - acc: 0.9995\n",
      "Epoch 360: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 9.2400e-04 - acc: 0.9995 - val_loss: 0.0034 - val_acc: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 361/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9993\n",
      "Epoch 361: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0011 - acc: 0.9994 - val_loss: 0.0014 - val_acc: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 362/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 6.6275e-04 - acc: 0.9997\n",
      "Epoch 362: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 6.6275e-04 - acc: 0.9997 - val_loss: 0.0020 - val_acc: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 363/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 4.4195e-04 - acc: 1.0000\n",
      "Epoch 363: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 4.3320e-04 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 0.9979 - lr: 1.2500e-04\n",
      "Epoch 364/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 7.1624e-04 - acc: 0.9997\n",
      "Epoch 364: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.0480e-04 - acc: 0.9997 - val_loss: 6.6288e-04 - val_acc: 0.9995 - lr: 1.2500e-04\n",
      "Epoch 365/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 6.2177e-04 - acc: 0.9998\n",
      "Epoch 365: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 6.0403e-04 - acc: 0.9998 - val_loss: 6.8678e-04 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 366/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 2.3499e-04 - acc: 0.9998\n",
      "Epoch 366: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 2.3499e-04 - acc: 0.9998 - val_loss: 6.6047e-04 - val_acc: 0.9998 - lr: 1.2500e-04\n",
      "Epoch 367/10000\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 367: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0099 - val_acc: 0.9971 - lr: 1.2500e-04\n",
      "Epoch 368/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 6.2043e-04 - acc: 0.9995\n",
      "Epoch 368: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 6.2043e-04 - acc: 0.9995 - val_loss: 0.0063 - val_acc: 0.9981 - lr: 1.2500e-04\n",
      "Epoch 369/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 5.6907e-04 - acc: 0.9998\n",
      "Epoch 369: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 5.4133e-04 - acc: 0.9998 - val_loss: 0.0016 - val_acc: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 370/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 8.0758e-04 - acc: 0.9997\n",
      "Epoch 370: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.7676e-04 - acc: 0.9997 - val_loss: 0.0036 - val_acc: 0.9983 - lr: 1.2500e-04\n",
      "Epoch 371/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9994\n",
      "Epoch 371: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0111 - val_acc: 0.9957 - lr: 1.2500e-04\n",
      "Epoch 372/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9995\n",
      "Epoch 372: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.0071 - val_acc: 0.9971 - lr: 1.2500e-04\n",
      "Epoch 373/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9994\n",
      "Epoch 373: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0043 - val_acc: 0.9986 - lr: 1.2500e-04\n",
      "Epoch 374/10000\n",
      "197/197 [==============================] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 374: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0053 - val_acc: 0.9979 - lr: 1.2500e-04\n",
      "Epoch 375/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 375: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0078 - val_acc: 0.9976 - lr: 1.2500e-04\n",
      "Epoch 376/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 376: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0019 - val_acc: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 377/10000\n",
      "193/197 [============================>.] - ETA: 0s - loss: 2.9894e-04 - acc: 1.0000\n",
      "Epoch 377: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 2.9499e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9990 - lr: 1.2500e-04\n",
      "Epoch 378/10000\n",
      "192/197 [============================>.] - ETA: 0s - loss: 2.3449e-04 - acc: 1.0000\n",
      "Epoch 378: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 2.2899e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 379/10000\n",
      "190/197 [===========================>..] - ETA: 0s - loss: 7.9937e-05 - acc: 1.0000\n",
      "Epoch 379: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 7.7566e-05 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9988 - lr: 1.2500e-04\n",
      "Epoch 380/10000\n",
      "191/197 [============================>.] - ETA: 0s - loss: 1.5374e-04 - acc: 1.0000\n",
      "Epoch 380: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 1.5239e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9993 - lr: 1.2500e-04\n",
      "Epoch 381/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 2.0074e-04 - acc: 1.0000\n",
      "Epoch 381: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 1.9784e-04 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9983 - lr: 1.2500e-04\n",
      "Epoch 382/10000\n",
      "187/197 [===========================>..] - ETA: 0s - loss: 6.0987e-04 - acc: 0.9998\n",
      "Epoch 382: val_loss did not improve from 0.00037\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 5.8029e-04 - acc: 0.9998 - val_loss: 0.0060 - val_acc: 0.9974 - lr: 1.2500e-04\n",
      "Epoch 383/10000\n",
      "189/197 [===========================>..] - ETA: 0s - loss: 7.7531e-04 - acc: 0.9998\n",
      "Epoch 383: val_loss did not improve from 0.00037\n",
      "\n",
      "Epoch 383: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0023 - val_acc: 0.9990 - lr: 1.2500e-04\n",
      "Epoch 383: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=10000,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")\n",
    "message = {\n",
    "    \"message\" : \"[ ] :     !\"\n",
    "}\n",
    "requests.post(api_url, headers = headers, data = message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[3093,    0],\n",
       "        [   0, 1102]],\n",
       "\n",
       "       [[3227,    0],\n",
       "        [   0,  968]],\n",
       "\n",
       "       [[3052,    0],\n",
       "        [   0, 1143]],\n",
       "\n",
       "       [[3213,    0],\n",
       "        [   0,  982]]], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "# print(y_pred.shape)\n",
    "# print(y_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 104ms/step\n",
      "[0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0]\n",
      "(7, 5)\n",
      "(7, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "test_data = np.load(\"dataset/confusion_matrix/seq_fall-2023-1.npy\")\n",
    "\n",
    "test_data = np.nan_to_num(test_data)\n",
    "test_x = test_data[:, :, :-1]\n",
    "test_y = test_data[:, 0, -1]\n",
    "\n",
    "\n",
    "test_y = to_categorical(test_y, num_classes=len(actions))\n",
    "\n",
    "y_pred = model.predict(test_x)\n",
    "print(np.argmax(y_pred, axis=1))\n",
    "print(np.argmax(test_y, axis=1))\n",
    "\n",
    "print(test_y.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "confusion_matrix(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

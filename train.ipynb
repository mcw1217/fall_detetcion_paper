{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10411, 30, 137)\n"
     ]
    }
   ],
   "source": [
    "actions = [\n",
    "    'fall','stand','walking','lie'\n",
    "]\n",
    "data = np.load(\"./dataset/sampledata/seq_fall-2023-1.npy\")\n",
    "datas = np.load(\"./dataset/sampledata/seq_stand-2023-1.npy\")\n",
    "datas2 = np.load(\"./dataset/sampledata/seq_walking-2023-1.npy\")\n",
    "datas3 = np.load(\"./dataset/sampledata/seq_lie-2023-1.npy\")\n",
    "for i in range(2,107):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/sampledata/seq_fall-2023-{i}.npy')\n",
    "    ], axis=0)  \n",
    "# for i in range(2,3):\n",
    "#     datas = np.concatenate([\n",
    "#         datas,\n",
    "#         np.load(f'./dataset/sampledata/seq_stand-2023-{i}.npy')\n",
    "#     ], axis=0)\n",
    "    \n",
    "for i in range(2,3):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/sampledata/seq_walking-2023-{i}.npy')\n",
    "    ], axis=0)\n",
    "    \n",
    "for i in range(2,5):\n",
    "    data = np.concatenate([\n",
    "        data,\n",
    "        np.load(f'./dataset/sampledata/seq_lie-2023-{i}.npy')\n",
    "    ], axis=0) \n",
    "        \n",
    "data = np.concatenate([data,datas])\n",
    "data = np.concatenate([data,datas2])\n",
    "data = np.concatenate([data,datas3])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load(\"dataset/seq_fall-2023-1.npy\")\n",
    "# datas = np.load(\"dataset/seq_stand-2023-1.npy\")\n",
    "# datas2 = np.load(\"dataset/seq_walking-2023-1.npy\")\n",
    "# datas3 = np.load(\"dataset/seq_lie-2023-1.npy\")\n",
    "\n",
    "\n",
    "# for i in range(2,60):\n",
    "#     data = np.concatenate([\n",
    "#         data,\n",
    "#         np.load(f'dataset/seq_fall-2023-{i}.npy')\n",
    "#     ], axis=0)  \n",
    "# print(data.shape)\n",
    "\n",
    "# # fall = 1569\n",
    "# # walking = 2428\n",
    "# # lie = 5710\n",
    "# for i in range(2,5):\n",
    "#     datas3 = np.concatenate([\n",
    "#         datas3,\n",
    "#         np.load(f'dataset/seq_lie-2023-{i}.npy')\n",
    "#     ], axis=0) \n",
    "\n",
    "# print(datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10411, 30, 136)\n",
      "(10411,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)\n",
    "print(labels[700:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10411, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7287, 30, 136) (7287, 4)\n",
      "(3124, 30, 136) (3124, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.3,random_state=4220)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 136)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_3 (GRU)                 (None, 30)                15120     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 15)                465       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 64        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,649\n",
      "Trainable params: 15,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,GRU,Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(30,dropout=0.2,activation='relu',input_shape=x_train.shape[1:3]),\n",
    "    # GRU(32,dropout=0.3),\n",
    "    # Dropout(0.2),\n",
    "    Dense(15, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9109 - acc: 0.7439\n",
      "Epoch 1: val_loss improved from inf to 0.61478, saving model to models\\model.h5\n",
      "228/228 [==============================] - 2s 6ms/step - loss: 0.9109 - acc: 0.7439 - val_loss: 0.6148 - val_acc: 0.7910 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9032\n",
      "Epoch 2: val_loss improved from 0.61478 to 0.44311, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.2661 - acc: 0.9038 - val_loss: 0.4431 - val_acc: 0.8159 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 0.1308 - acc: 0.9529\n",
      "Epoch 3: val_loss improved from 0.44311 to 0.35311, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.1285 - acc: 0.9542 - val_loss: 0.3531 - val_acc: 0.8627 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9690\n",
      "Epoch 4: val_loss improved from 0.35311 to 0.20303, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0918 - acc: 0.9698 - val_loss: 0.2030 - val_acc: 0.9171 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9840\n",
      "Epoch 5: val_loss did not improve from 0.20303\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0514 - acc: 0.9839 - val_loss: 0.2234 - val_acc: 0.9219 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9849\n",
      "Epoch 6: val_loss did not improve from 0.20303\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0429 - acc: 0.9849 - val_loss: 0.2186 - val_acc: 0.9203 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9895\n",
      "Epoch 7: val_loss did not improve from 0.20303\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0303 - acc: 0.9897 - val_loss: 0.2279 - val_acc: 0.9139 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9897\n",
      "Epoch 8: val_loss improved from 0.20303 to 0.16328, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0280 - acc: 0.9898 - val_loss: 0.1633 - val_acc: 0.9475 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9939\n",
      "Epoch 9: val_loss did not improve from 0.16328\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0175 - acc: 0.9940 - val_loss: 0.1707 - val_acc: 0.9408 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.0156 - acc: 0.9947\n",
      "Epoch 10: val_loss improved from 0.16328 to 0.03541, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0151 - acc: 0.9949 - val_loss: 0.0354 - val_acc: 0.9834 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9909\n",
      "Epoch 11: val_loss improved from 0.03541 to 0.02974, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0285 - acc: 0.9911 - val_loss: 0.0297 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 0.0128 - acc: 0.9960\n",
      "Epoch 12: val_loss did not improve from 0.02974\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0126 - acc: 0.9962 - val_loss: 0.0597 - val_acc: 0.9811 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0149 - acc: 0.9957\n",
      "Epoch 13: val_loss did not improve from 0.02974\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0149 - acc: 0.9957 - val_loss: 0.1237 - val_acc: 0.9709 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.0101 - acc: 0.9963\n",
      "Epoch 14: val_loss did not improve from 0.02974\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0098 - acc: 0.9964 - val_loss: 0.0310 - val_acc: 0.9885 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9958\n",
      "Epoch 15: val_loss did not improve from 0.02974\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0145 - acc: 0.9957 - val_loss: 0.0473 - val_acc: 0.9814 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0100 - acc: 0.9977\n",
      "Epoch 16: val_loss improved from 0.02974 to 0.01907, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0100 - acc: 0.9977 - val_loss: 0.0191 - val_acc: 0.9933 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 17: val_loss did not improve from 0.01907\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0029 - acc: 0.9997 - val_loss: 0.0268 - val_acc: 0.9901 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 0.0092 - acc: 0.9976\n",
      "Epoch 18: val_loss improved from 0.01907 to 0.01157, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0088 - acc: 0.9977 - val_loss: 0.0116 - val_acc: 0.9958 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.0157 - acc: 0.9960\n",
      "Epoch 19: val_loss did not improve from 0.01157\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0180 - acc: 0.9957 - val_loss: 0.0369 - val_acc: 0.9818 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9989\n",
      "Epoch 20: val_loss did not improve from 0.01157\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0212 - val_acc: 0.9952 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9987\n",
      "Epoch 21: val_loss did not improve from 0.01157\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.1296 - val_acc: 0.9533 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0046 - acc: 0.9979\n",
      "Epoch 22: val_loss did not improve from 0.01157\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0046 - acc: 0.9979 - val_loss: 0.0410 - val_acc: 0.9834 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.0086 - acc: 0.9974\n",
      "Epoch 23: val_loss did not improve from 0.01157\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.0676 - val_acc: 0.9766 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9977\n",
      "Epoch 24: val_loss did not improve from 0.01157\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0058 - acc: 0.9977 - val_loss: 0.0300 - val_acc: 0.9910 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.0056 - acc: 0.9984\n",
      "Epoch 25: val_loss did not improve from 0.01157\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0426 - val_acc: 0.9818 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0044 - acc: 0.9986\n",
      "Epoch 26: val_loss improved from 0.01157 to 0.00599, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0060 - val_acc: 0.9984 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.0128 - acc: 0.9962\n",
      "Epoch 27: val_loss did not improve from 0.00599\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0125 - acc: 0.9963 - val_loss: 0.0152 - val_acc: 0.9936 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 0.0130 - acc: 0.9970\n",
      "Epoch 28: val_loss did not improve from 0.00599\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0132 - acc: 0.9967 - val_loss: 0.0273 - val_acc: 0.9920 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9960\n",
      "Epoch 29: val_loss did not improve from 0.00599\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0163 - acc: 0.9960 - val_loss: 0.0298 - val_acc: 0.9936 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "223/228 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 30: val_loss did not improve from 0.00599\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0235 - val_acc: 0.9930 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9980\n",
      "Epoch 31: val_loss improved from 0.00599 to 0.00555, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0055 - val_acc: 0.9984 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 32: val_loss did not improve from 0.00555\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0070 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 5.7265e-04 - acc: 0.9999\n",
      "Epoch 33: val_loss did not improve from 0.00555\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.6874e-04 - acc: 0.9999 - val_loss: 0.0101 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9993\n",
      "Epoch 34: val_loss improved from 0.00555 to 0.00488, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0049 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 0.0063 - acc: 0.9980\n",
      "Epoch 35: val_loss improved from 0.00488 to 0.00338, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0034 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9990\n",
      "Epoch 36: val_loss did not improve from 0.00338\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0023 - acc: 0.9990 - val_loss: 0.0083 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9990  \n",
      "Epoch 37: val_loss did not improve from 0.00338\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0029 - acc: 0.9989 - val_loss: 0.0060 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "223/228 [============================>.] - ETA: 0s - loss: 9.2393e-04 - acc: 0.9996\n",
      "Epoch 38: val_loss did not improve from 0.00338\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.0661e-04 - acc: 0.9996 - val_loss: 0.0051 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9970\n",
      "Epoch 39: val_loss did not improve from 0.00338\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0258 - val_acc: 0.9920 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 40: val_loss did not improve from 0.00338\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0062 - val_acc: 0.9984 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9994   \n",
      "Epoch 41: val_loss did not improve from 0.00338\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0013 - acc: 0.9993 - val_loss: 0.0049 - val_acc: 0.9987 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0284 - acc: 0.9911\n",
      "Epoch 42: val_loss did not improve from 0.00338\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0284 - acc: 0.9911 - val_loss: 0.0209 - val_acc: 0.9946 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9978\n",
      "Epoch 43: val_loss did not improve from 0.00338\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0076 - acc: 0.9978 - val_loss: 0.0053 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 8.2822e-04 - acc: 0.9999\n",
      "Epoch 44: val_loss improved from 0.00338 to 0.00150, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.2765e-04 - acc: 0.9999 - val_loss: 0.0015 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 45: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0010 - acc: 0.9999 - val_loss: 0.0018 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0104 - acc: 0.9977\n",
      "Epoch 46: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0104 - acc: 0.9977 - val_loss: 0.0304 - val_acc: 0.9930 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "215/228 [===========================>..] - ETA: 0s - loss: 0.0071 - acc: 0.9980\n",
      "Epoch 47: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.0167 - val_acc: 0.9968 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 48: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0076 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "215/228 [===========================>..] - ETA: 0s - loss: 0.0069 - acc: 0.9975  \n",
      "Epoch 49: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0067 - acc: 0.9977 - val_loss: 0.0108 - val_acc: 0.9987 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 50: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0577 - val_acc: 0.9840 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9989\n",
      "Epoch 51: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0201 - val_acc: 0.9942 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 52: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0158 - val_acc: 0.9952 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 5.7890e-04 - acc: 0.9999\n",
      "Epoch 53: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.5352e-04 - acc: 0.9999 - val_loss: 0.0027 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 6.1795e-04 - acc: 0.9997\n",
      "Epoch 54: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.8672e-04 - acc: 0.9997 - val_loss: 0.0043 - val_acc: 0.9997 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "215/228 [===========================>..] - ETA: 0s - loss: 7.2662e-05 - acc: 1.0000\n",
      "Epoch 55: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.9940e-05 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.2005e-04 - acc: 1.0000\n",
      "Epoch 56: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1955e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9987 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 1.2680e-04 - acc: 1.0000\n",
      "Epoch 57: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2167e-04 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 4.2608e-05 - acc: 1.0000\n",
      "Epoch 58: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.2032e-05 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "215/228 [===========================>..] - ETA: 0s - loss: 0.0118 - acc: 0.9983\n",
      "Epoch 59: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0112 - acc: 0.9984 - val_loss: 0.0056 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 60: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0046 - acc: 0.9989 - val_loss: 0.0066 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9982\n",
      "Epoch 61: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0128 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 62: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0025 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 8.7980e-05 - acc: 1.0000\n",
      "Epoch 63: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.7980e-05 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9997  \n",
      "Epoch 64: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0043 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0072 - acc: 0.9970\n",
      "Epoch 65: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0072 - acc: 0.9970 - val_loss: 0.0278 - val_acc: 0.9917 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "223/228 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 66: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0082 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "223/228 [============================>.] - ETA: 0s - loss: 2.5101e-04 - acc: 1.0000\n",
      "Epoch 67: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.4643e-04 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9965 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 2.8720e-04 - acc: 1.0000\n",
      "Epoch 68: val_loss did not improve from 0.00150\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.8396e-04 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 69: val_loss improved from 0.00150 to 0.00126, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0013 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.7198e-05 - acc: 1.0000\n",
      "Epoch 70: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.7324e-05 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9996  \n",
      "Epoch 71: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0026 - acc: 0.9996 - val_loss: 0.0073 - val_acc: 0.9984 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9960\n",
      "Epoch 72: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0209 - acc: 0.9960 - val_loss: 0.0332 - val_acc: 0.9926 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9996    \n",
      "Epoch 73: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0028 - acc: 0.9996 - val_loss: 0.0091 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 74: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0042 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.5377e-04 - acc: 1.0000\n",
      "Epoch 75: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.5377e-04 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 9.1542e-05 - acc: 1.0000\n",
      "Epoch 76: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0666e-04 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 9.6093e-05 - acc: 1.0000\n",
      "Epoch 77: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.6093e-05 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9987 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9984\n",
      "Epoch 78: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0019 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 5.7628e-04 - acc: 0.9997\n",
      "Epoch 79: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.7182e-04 - acc: 0.9997 - val_loss: 0.0083 - val_acc: 0.9978 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "215/228 [===========================>..] - ETA: 0s - loss: 0.0257 - acc: 0.9961\n",
      "Epoch 80: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0250 - acc: 0.9962 - val_loss: 0.0497 - val_acc: 0.9942 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 81: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0057 - val_acc: 0.9994 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "215/228 [===========================>..] - ETA: 0s - loss: 0.0060 - acc: 0.9985\n",
      "Epoch 82: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0111 - val_acc: 0.9974 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9987\n",
      "Epoch 83: val_loss did not improve from 0.00126\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0033 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 3.4083e-04 - acc: 1.0000\n",
      "Epoch 84: val_loss improved from 0.00126 to 0.00104, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3977e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 3.9865e-04 - acc: 0.9999\n",
      "Epoch 85: val_loss improved from 0.00104 to 0.00058, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.8678e-04 - acc: 0.9999 - val_loss: 5.7838e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 2.1579e-04 - acc: 1.0000\n",
      "Epoch 86: val_loss improved from 0.00058 to 0.00035, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.1487e-04 - acc: 1.0000 - val_loss: 3.5071e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 6.1814e-05 - acc: 1.0000\n",
      "Epoch 87: val_loss improved from 0.00035 to 0.00027, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.9569e-05 - acc: 1.0000 - val_loss: 2.6735e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 7.1506e-05 - acc: 1.0000\n",
      "Epoch 88: val_loss improved from 0.00027 to 0.00021, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.1506e-05 - acc: 1.0000 - val_loss: 2.1046e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 89: val_loss did not improve from 0.00021\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 5.4708e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.1737e-04 - acc: 1.0000\n",
      "Epoch 90: val_loss improved from 0.00021 to 0.00019, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1633e-04 - acc: 1.0000 - val_loss: 1.8987e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9994   \n",
      "Epoch 91: val_loss did not improve from 0.00019\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0059 - val_acc: 0.9978 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 9.9578e-04 - acc: 0.9997\n",
      "Epoch 92: val_loss did not improve from 0.00019\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0023 - val_acc: 0.9997 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.7224e-04 - acc: 1.0000\n",
      "Epoch 93: val_loss did not improve from 0.00019\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.7171e-04 - acc: 1.0000 - val_loss: 2.9136e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 1.3539e-04 - acc: 1.0000\n",
      "Epoch 94: val_loss improved from 0.00019 to 0.00013, saving model to models\\model.h5\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2867e-04 - acc: 1.0000 - val_loss: 1.3164e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 95: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 7.4772e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 96/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 96: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0054 - val_acc: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 97/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 6.6759e-05 - acc: 1.0000\n",
      "Epoch 97: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.4108e-05 - acc: 1.0000 - val_loss: 2.1297e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 98/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 98: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0045 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 99/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 3.3944e-04 - acc: 0.9997\n",
      "Epoch 99: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3572e-04 - acc: 0.9997 - val_loss: 3.1903e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 100/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 2.6900e-05 - acc: 1.0000\n",
      "Epoch 100: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.6076e-05 - acc: 1.0000 - val_loss: 2.8586e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 101/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.0564e-05 - acc: 1.0000\n",
      "Epoch 101: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.0564e-05 - acc: 1.0000 - val_loss: 2.0285e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 102/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.4455e-05 - acc: 1.0000\n",
      "Epoch 102: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4455e-05 - acc: 1.0000 - val_loss: 1.8673e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 103/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 1.8242e-05 - acc: 1.0000\n",
      "Epoch 103: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.7639e-05 - acc: 1.0000 - val_loss: 1.3752e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 104/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.3190e-05 - acc: 1.0000\n",
      "Epoch 104: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.3020e-05 - acc: 1.0000 - val_loss: 1.4421e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 105/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.0595e-05 - acc: 1.0000\n",
      "Epoch 105: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0519e-05 - acc: 1.0000 - val_loss: 1.4264e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 106/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.1255e-04 - acc: 1.0000\n",
      "Epoch 106: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1219e-04 - acc: 1.0000 - val_loss: 5.5693e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 107/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 4.7982e-05 - acc: 1.0000\n",
      "Epoch 107: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.7982e-05 - acc: 1.0000 - val_loss: 3.0874e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 108/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 0.0076 - acc: 0.9987\n",
      "Epoch 108: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0072 - acc: 0.9988 - val_loss: 9.9111e-04 - val_acc: 0.9997 - lr: 5.0000e-04\n",
      "Epoch 109/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 2.6661e-04 - acc: 0.9999\n",
      "Epoch 109: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.9295e-04 - acc: 0.9999 - val_loss: 0.0041 - val_acc: 0.9981 - lr: 5.0000e-04\n",
      "Epoch 110/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 3.0333e-04 - acc: 0.9999\n",
      "Epoch 110: val_loss did not improve from 0.00013\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.9882e-04 - acc: 0.9999 - val_loss: 2.6755e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 111/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 3.3138e-05 - acc: 1.0000\n",
      "Epoch 111: val_loss improved from 0.00013 to 0.00011, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.2927e-05 - acc: 1.0000 - val_loss: 1.0566e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 112/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.6198e-05 - acc: 1.0000\n",
      "Epoch 112: val_loss improved from 0.00011 to 0.00008, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.5593e-05 - acc: 1.0000 - val_loss: 8.0200e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 113/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0031 - acc: 0.9990   \n",
      "Epoch 113: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0029 - val_acc: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 114/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.6315e-04 - acc: 0.9999\n",
      "Epoch 114: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.6376e-04 - acc: 0.9999 - val_loss: 3.5966e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 115/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 6.5543e-05 - acc: 1.0000\n",
      "Epoch 115: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.3396e-05 - acc: 1.0000 - val_loss: 3.5282e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 116/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9987\n",
      "Epoch 116: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 2.3187e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 117/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 117: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 7.6735e-04 - val_acc: 0.9997 - lr: 5.0000e-04\n",
      "Epoch 118/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 4.3908e-04 - acc: 0.9999\n",
      "Epoch 118: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.3772e-04 - acc: 0.9999 - val_loss: 5.2434e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 119/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 3.7199e-05 - acc: 1.0000\n",
      "Epoch 119: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.6990e-05 - acc: 1.0000 - val_loss: 2.5724e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 120/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 1.5477e-05 - acc: 1.0000\n",
      "Epoch 120: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.5954e-05 - acc: 1.0000 - val_loss: 2.2584e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 121/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 2.7506e-05 - acc: 1.0000\n",
      "Epoch 121: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.6557e-05 - acc: 1.0000 - val_loss: 1.6211e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 122/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 122: val_loss did not improve from 0.00008\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 1.2001e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 123/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9996   \n",
      "Epoch 123: val_loss improved from 0.00008 to 0.00006, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 5.9846e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 124/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 124: val_loss did not improve from 0.00006\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 1.5193e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 125/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 4.8531e-05 - acc: 1.0000\n",
      "Epoch 125: val_loss did not improve from 0.00006\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.6883e-05 - acc: 1.0000 - val_loss: 7.2005e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 126/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.6832e-05 - acc: 1.0000\n",
      "Epoch 126: val_loss improved from 0.00006 to 0.00005, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6832e-05 - acc: 1.0000 - val_loss: 5.2864e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 127/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 1.7395e-05 - acc: 1.0000\n",
      "Epoch 127: val_loss improved from 0.00005 to 0.00005, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.7291e-05 - acc: 1.0000 - val_loss: 4.5937e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 128/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.1575e-05 - acc: 1.0000\n",
      "Epoch 128: val_loss improved from 0.00005 to 0.00004, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1755e-05 - acc: 1.0000 - val_loss: 4.4100e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 129/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 5.4122e-06 - acc: 1.0000\n",
      "Epoch 129: val_loss improved from 0.00004 to 0.00004, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.4613e-06 - acc: 1.0000 - val_loss: 4.1456e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 130/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 6.9152e-06 - acc: 1.0000\n",
      "Epoch 130: val_loss improved from 0.00004 to 0.00004, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.8960e-06 - acc: 1.0000 - val_loss: 3.6877e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 131/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.0528e-05 - acc: 1.0000\n",
      "Epoch 131: val_loss improved from 0.00004 to 0.00004, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0495e-05 - acc: 1.0000 - val_loss: 3.6303e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 132/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.5105e-05 - acc: 1.0000\n",
      "Epoch 132: val_loss improved from 0.00004 to 0.00003, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.5159e-05 - acc: 1.0000 - val_loss: 3.1940e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 133/500\n",
      "215/228 [===========================>..] - ETA: 0s - loss: 1.0444e-05 - acc: 1.0000\n",
      "Epoch 133: val_loss improved from 0.00003 to 0.00002, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0124e-05 - acc: 1.0000 - val_loss: 2.2854e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 134/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 2.8394e-06 - acc: 1.0000\n",
      "Epoch 134: val_loss improved from 0.00002 to 0.00002, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.0167e-06 - acc: 1.0000 - val_loss: 2.1794e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 135/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.1427e-05 - acc: 1.0000\n",
      "Epoch 135: val_loss improved from 0.00002 to 0.00002, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1393e-05 - acc: 1.0000 - val_loss: 2.1521e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 136/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 4.5886e-06 - acc: 1.0000\n",
      "Epoch 136: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.5886e-06 - acc: 1.0000 - val_loss: 2.1566e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 137/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0104 - acc: 0.9981\n",
      "Epoch 137: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0104 - acc: 0.9981 - val_loss: 0.0043 - val_acc: 0.9997 - lr: 5.0000e-04\n",
      "Epoch 138/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9992\n",
      "Epoch 138: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0019 - acc: 0.9992 - val_loss: 5.6262e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 139/500\n",
      "216/228 [===========================>..] - ETA: 0s - loss: 1.7317e-04 - acc: 1.0000\n",
      "Epoch 139: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6794e-04 - acc: 1.0000 - val_loss: 2.9690e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 140/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0467e-04 - acc: 1.0000\n",
      "Epoch 140: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0467e-04 - acc: 1.0000 - val_loss: 2.0719e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 141/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 141: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0011 - val_acc: 0.9997 - lr: 5.0000e-04\n",
      "Epoch 142/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.3060e-04 - acc: 0.9999\n",
      "Epoch 142: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.2208e-04 - acc: 0.9999 - val_loss: 1.2470e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 143/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.4773e-04 - acc: 1.0000\n",
      "Epoch 143: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4249e-04 - acc: 1.0000 - val_loss: 1.4431e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 144/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 4.8109e-05 - acc: 1.0000\n",
      "Epoch 144: val_loss did not improve from 0.00002\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.7754e-05 - acc: 1.0000 - val_loss: 9.2760e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 145/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 145: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 7.6658e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 146/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.2801e-04 - acc: 1.0000\n",
      "Epoch 146: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2309e-04 - acc: 1.0000 - val_loss: 7.1326e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 147/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.4757e-05 - acc: 1.0000\n",
      "Epoch 147: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.4757e-05 - acc: 1.0000 - val_loss: 6.7259e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 148/500\n",
      "215/228 [===========================>..] - ETA: 0s - loss: 1.7205e-05 - acc: 1.0000\n",
      "Epoch 148: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6994e-05 - acc: 1.0000 - val_loss: 6.3506e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 149/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 9.1608e-05 - acc: 1.0000\n",
      "Epoch 149: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.9206e-05 - acc: 1.0000 - val_loss: 6.1196e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 150/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.7703e-05 - acc: 1.0000\n",
      "Epoch 150: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.7689e-05 - acc: 1.0000 - val_loss: 5.3736e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 151/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 4.3618e-05 - acc: 1.0000\n",
      "Epoch 151: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.3618e-05 - acc: 1.0000 - val_loss: 4.7059e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 152/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 9.9044e-06 - acc: 1.0000\n",
      "Epoch 152: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.0282e-05 - acc: 1.0000 - val_loss: 4.4537e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 153/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.0774e-04 - acc: 1.0000\n",
      "Epoch 153: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0647e-04 - acc: 1.0000 - val_loss: 2.8684e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 154/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 154: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 2.6272e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 155/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 2.3269e-05 - acc: 1.0000\n",
      "Epoch 155: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.3316e-05 - acc: 1.0000 - val_loss: 1.9419e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 156/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.3726e-05 - acc: 1.0000\n",
      "Epoch 156: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.3685e-05 - acc: 1.0000 - val_loss: 1.4821e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 157/500\n",
      "223/228 [============================>.] - ETA: 0s - loss: 1.7715e-04 - acc: 0.9999\n",
      "Epoch 157: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.7375e-04 - acc: 0.9999 - val_loss: 1.8254e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 158/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 7.5698e-04 - acc: 0.9997\n",
      "Epoch 158: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 7.5467e-04 - acc: 0.9997 - val_loss: 1.3425e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 159/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 2.2164e-05 - acc: 1.0000\n",
      "Epoch 159: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.1699e-05 - acc: 1.0000 - val_loss: 1.0549e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 160/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 2.8979e-05 - acc: 1.0000\n",
      "Epoch 160: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.8888e-05 - acc: 1.0000 - val_loss: 7.9405e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 161/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.1913e-05 - acc: 1.0000\n",
      "Epoch 161: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.1941e-05 - acc: 1.0000 - val_loss: 6.8516e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 162/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0920e-04 - acc: 0.9999\n",
      "Epoch 162: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0920e-04 - acc: 0.9999 - val_loss: 1.0673e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 163/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.1368e-05 - acc: 1.0000\n",
      "Epoch 163: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.1333e-05 - acc: 1.0000 - val_loss: 1.2898e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 164/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 4.6597e-06 - acc: 1.0000\n",
      "Epoch 164: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.6597e-06 - acc: 1.0000 - val_loss: 1.1697e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 165/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 4.0882e-06 - acc: 1.0000\n",
      "Epoch 165: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.0717e-06 - acc: 1.0000 - val_loss: 1.0522e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 166/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 8.4750e-06 - acc: 1.0000\n",
      "Epoch 166: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.1606e-06 - acc: 1.0000 - val_loss: 1.0135e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 167/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 1.3729e-05 - acc: 1.0000\n",
      "Epoch 167: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.3449e-05 - acc: 1.0000 - val_loss: 6.2140e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 168/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 2.4586e-06 - acc: 1.0000\n",
      "Epoch 168: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.4861e-06 - acc: 1.0000 - val_loss: 5.7917e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 169/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 9.6059e-06 - acc: 1.0000\n",
      "Epoch 169: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.5460e-06 - acc: 1.0000 - val_loss: 3.9655e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 170/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.8817e-06 - acc: 1.0000\n",
      "Epoch 170: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.8817e-06 - acc: 1.0000 - val_loss: 3.5166e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 171/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.8004e-06 - acc: 1.0000\n",
      "Epoch 171: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.1517e-06 - acc: 1.0000 - val_loss: 3.5802e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 172/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.3419e-06 - acc: 1.0000\n",
      "Epoch 172: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.6156e-06 - acc: 1.0000 - val_loss: 3.5958e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 173/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 1.7081e-06 - acc: 1.0000\n",
      "Epoch 173: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6893e-06 - acc: 1.0000 - val_loss: 3.3611e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 174/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.6250e-06 - acc: 1.0000\n",
      "Epoch 174: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.6250e-06 - acc: 1.0000 - val_loss: 3.1909e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 175/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.8341e-06 - acc: 1.0000\n",
      "Epoch 175: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.8078e-06 - acc: 1.0000 - val_loss: 3.1733e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 176/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.7066e-06 - acc: 1.0000\n",
      "Epoch 176: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6569e-06 - acc: 1.0000 - val_loss: 3.0853e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 177/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.5165e-06 - acc: 1.0000\n",
      "Epoch 177: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.5092e-06 - acc: 1.0000 - val_loss: 2.9582e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 178/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.8063e-04 - acc: 0.9997\n",
      "Epoch 178: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.6230e-04 - acc: 0.9997 - val_loss: 1.7871e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 179/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.3527e-04 - acc: 0.9999\n",
      "Epoch 179: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.2659e-04 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9994 - lr: 2.5000e-04\n",
      "Epoch 180/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 3.3492e-06 - acc: 1.0000\n",
      "Epoch 180: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.1648e-06 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9994 - lr: 2.5000e-04\n",
      "Epoch 181/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.7032e-06 - acc: 1.0000\n",
      "Epoch 181: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.6891e-06 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9994 - lr: 2.5000e-04\n",
      "Epoch 182/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 182: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0017 - acc: 0.9997 - val_loss: 6.3712e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 183/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 8.7528e-06 - acc: 1.0000\n",
      "Epoch 183: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.6294e-06 - acc: 1.0000 - val_loss: 5.9721e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 184/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.6136e-06 - acc: 1.0000\n",
      "Epoch 184: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.1206e-06 - acc: 1.0000 - val_loss: 6.6623e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 185/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.4735e-05 - acc: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.4561e-05 - acc: 1.0000 - val_loss: 2.3893e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 186/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 5.9532e-06 - acc: 1.0000\n",
      "Epoch 186: val_loss improved from 0.00002 to 0.00002, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 5.9122e-06 - acc: 1.0000 - val_loss: 1.9430e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 187/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 6.8903e-06 - acc: 1.0000\n",
      "Epoch 187: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.8903e-06 - acc: 1.0000 - val_loss: 2.0645e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 188/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 4.4095e-05 - acc: 1.0000\n",
      "Epoch 188: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.3386e-05 - acc: 1.0000 - val_loss: 2.4361e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 189/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.3340e-05 - acc: 1.0000\n",
      "Epoch 189: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2894e-05 - acc: 1.0000 - val_loss: 3.0844e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 190/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 9.7354e-06 - acc: 1.0000\n",
      "Epoch 190: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.5708e-06 - acc: 1.0000 - val_loss: 3.4327e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 191/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.5769e-06 - acc: 1.0000\n",
      "Epoch 191: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.5769e-06 - acc: 1.0000 - val_loss: 1.0858e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 192/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.0823e-06 - acc: 1.0000\n",
      "Epoch 192: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0447e-06 - acc: 1.0000 - val_loss: 7.6813e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 193/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1771e-05 - acc: 1.0000\n",
      "Epoch 193: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1771e-05 - acc: 1.0000 - val_loss: 7.9295e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 194/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 8.7245e-07 - acc: 1.0000\n",
      "Epoch 194: val_loss did not improve from 0.00002\n",
      "\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.4643e-07 - acc: 1.0000 - val_loss: 6.5497e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 195/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 3.9088e-04 - acc: 0.9999\n",
      "Epoch 195: val_loss did not improve from 0.00002\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.8965e-04 - acc: 0.9999 - val_loss: 1.3057e-04 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 196/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 5.7028e-05 - acc: 1.0000\n",
      "Epoch 196: val_loss improved from 0.00002 to 0.00001, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 5.7028e-05 - acc: 1.0000 - val_loss: 9.5889e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 197/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 6.2198e-07 - acc: 1.0000\n",
      "Epoch 197: val_loss improved from 0.00001 to 0.00001, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.0144e-07 - acc: 1.0000 - val_loss: 8.8302e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 198/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 3.3535e-06 - acc: 1.0000\n",
      "Epoch 198: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3432e-06 - acc: 1.0000 - val_loss: 8.8412e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 199/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 7.8014e-07 - acc: 1.0000\n",
      "Epoch 199: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.7779e-07 - acc: 1.0000 - val_loss: 8.9378e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 200/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 7.1066e-04 - acc: 0.9999\n",
      "Epoch 200: val_loss improved from 0.00001 to 0.00001, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 7.1066e-04 - acc: 0.9999 - val_loss: 6.4596e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 201/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 4.8264e-05 - acc: 1.0000\n",
      "Epoch 201: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.8112e-05 - acc: 1.0000 - val_loss: 3.7509e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 202/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 2.8675e-07 - acc: 1.0000\n",
      "Epoch 202: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.8377e-07 - acc: 1.0000 - val_loss: 3.6825e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 203/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.8486e-07 - acc: 1.0000\n",
      "Epoch 203: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.7001e-07 - acc: 1.0000 - val_loss: 3.5590e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 204/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.7660e-04 - acc: 0.9999\n",
      "Epoch 204: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.6984e-04 - acc: 0.9999 - val_loss: 1.5957e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 205/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.6161e-07 - acc: 1.0000\n",
      "Epoch 205: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.6133e-07 - acc: 1.0000 - val_loss: 1.5870e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 206/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.2761e-07 - acc: 1.0000\n",
      "Epoch 206: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.2761e-07 - acc: 1.0000 - val_loss: 1.5687e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 207/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.8678e-07 - acc: 1.0000\n",
      "Epoch 207: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.7163e-07 - acc: 1.0000 - val_loss: 1.5472e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 208/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.7930e-07 - acc: 1.0000\n",
      "Epoch 208: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.7887e-07 - acc: 1.0000 - val_loss: 1.5335e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 209/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 3.0719e-07 - acc: 1.0000\n",
      "Epoch 209: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.9744e-07 - acc: 1.0000 - val_loss: 1.4879e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 210/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 3.3473e-07 - acc: 1.0000\n",
      "Epoch 210: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.2520e-07 - acc: 1.0000 - val_loss: 1.4676e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 211/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.0282e-07 - acc: 1.0000\n",
      "Epoch 211: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.8236e-07 - acc: 1.0000 - val_loss: 1.4327e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 212/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 5.9136e-06 - acc: 1.0000\n",
      "Epoch 212: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 5.9136e-06 - acc: 1.0000 - val_loss: 1.7140e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 213/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 9.6103e-07 - acc: 1.0000\n",
      "Epoch 213: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.3887e-07 - acc: 1.0000 - val_loss: 1.4967e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 214/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 7.5138e-07 - acc: 1.0000\n",
      "Epoch 214: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.3176e-07 - acc: 1.0000 - val_loss: 1.3906e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 215/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.3323e-07 - acc: 1.0000\n",
      "Epoch 215: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.0222e-06 - acc: 1.0000 - val_loss: 1.3477e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 216/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 3.4109e-07 - acc: 1.0000\n",
      "Epoch 216: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3199e-07 - acc: 1.0000 - val_loss: 1.7511e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 217/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 2.1145e-07 - acc: 1.0000\n",
      "Epoch 217: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.0773e-07 - acc: 1.0000 - val_loss: 1.6662e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 218/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.8100e-07 - acc: 1.0000\n",
      "Epoch 218: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.7513e-07 - acc: 1.0000 - val_loss: 1.5939e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 219/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.9789e-07 - acc: 1.0000\n",
      "Epoch 219: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.9393e-07 - acc: 1.0000 - val_loss: 1.5088e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 220/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 2.4737e-07 - acc: 1.0000\n",
      "Epoch 220: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.5362e-07 - acc: 1.0000 - val_loss: 1.3915e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 221/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 3.4669e-07 - acc: 1.0000\n",
      "Epoch 221: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3612e-07 - acc: 1.0000 - val_loss: 1.3832e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 222/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.9621e-07 - acc: 1.0000\n",
      "Epoch 222: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.8411e-07 - acc: 1.0000 - val_loss: 1.1668e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 223/500\n",
      "217/228 [===========================>..] - ETA: 0s - loss: 1.7316e-06 - acc: 1.0000\n",
      "Epoch 223: val_loss improved from 0.00001 to 0.00001, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6533e-06 - acc: 1.0000 - val_loss: 5.6078e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 224/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 1.7254e-07 - acc: 1.0000\n",
      "Epoch 224: val_loss improved from 0.00001 to 0.00001, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6875e-07 - acc: 1.0000 - val_loss: 5.5097e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 225/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 8.1152e-08 - acc: 1.0000\n",
      "Epoch 225: val_loss improved from 0.00001 to 0.00001, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.8981e-08 - acc: 1.0000 - val_loss: 5.4583e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 226/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.0669e-07 - acc: 1.0000\n",
      "Epoch 226: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.0007e-07 - acc: 1.0000 - val_loss: 5.5187e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 227/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 5.9587e-08 - acc: 1.0000\n",
      "Epoch 227: val_loss did not improve from 0.00001\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.0872e-08 - acc: 1.0000 - val_loss: 5.4818e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 228/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.0638e-07 - acc: 1.0000\n",
      "Epoch 228: val_loss improved from 0.00001 to 0.00001, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.2135e-07 - acc: 1.0000 - val_loss: 5.4491e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 229/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 9.8725e-08 - acc: 1.0000\n",
      "Epoch 229: val_loss improved from 0.00001 to 0.00001, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 9.8725e-08 - acc: 1.0000 - val_loss: 5.2802e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 230/500\n",
      "223/228 [============================>.] - ETA: 0s - loss: 2.0488e-07 - acc: 1.0000\n",
      "Epoch 230: val_loss improved from 0.00001 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.0102e-07 - acc: 1.0000 - val_loss: 4.8976e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 231/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 3.7543e-05 - acc: 1.0000\n",
      "Epoch 231: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.6305e-05 - acc: 1.0000 - val_loss: 3.2658e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 232/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 7.4743e-06 - acc: 1.0000\n",
      "Epoch 232: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.2240e-06 - acc: 1.0000 - val_loss: 1.4736e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 233/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.4470e-06 - acc: 1.0000\n",
      "Epoch 233: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.2589e-06 - acc: 1.0000 - val_loss: 8.8135e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 234/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 9.4518e-04 - acc: 0.9999\n",
      "Epoch 234: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.1729e-04 - acc: 0.9999 - val_loss: 6.0243e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 235/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 9.3278e-07 - acc: 1.0000\n",
      "Epoch 235: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 9.3278e-07 - acc: 1.0000 - val_loss: 5.6394e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 236/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 236: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0012 - acc: 0.9999 - val_loss: 1.5962e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 237/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 3.4130e-07 - acc: 1.0000\n",
      "Epoch 237: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.4163e-07 - acc: 1.0000 - val_loss: 1.5916e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 238/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.7642e-06 - acc: 1.0000\n",
      "Epoch 238: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6969e-06 - acc: 1.0000 - val_loss: 1.8640e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 239/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.1980e-05 - acc: 1.0000\n",
      "Epoch 239: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1470e-05 - acc: 1.0000 - val_loss: 1.8253e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 240/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 4.9077e-06 - acc: 1.0000\n",
      "Epoch 240: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.8922e-06 - acc: 1.0000 - val_loss: 1.2634e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 241/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.1511e-07 - acc: 1.0000\n",
      "Epoch 241: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.3159e-07 - acc: 1.0000 - val_loss: 1.2670e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 242/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 4.3258e-07 - acc: 1.0000\n",
      "Epoch 242: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.3258e-07 - acc: 1.0000 - val_loss: 1.4080e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 243/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.1662e-07 - acc: 1.0000\n",
      "Epoch 243: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.1536e-07 - acc: 1.0000 - val_loss: 1.4116e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 244/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.1262e-07 - acc: 1.0000\n",
      "Epoch 244: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 244: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.0671e-07 - acc: 1.0000 - val_loss: 1.3693e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 245/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 3.3595e-07 - acc: 1.0000\n",
      "Epoch 245: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3344e-07 - acc: 1.0000 - val_loss: 1.3309e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 246/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1150e-07 - acc: 1.0000\n",
      "Epoch 246: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1150e-07 - acc: 1.0000 - val_loss: 1.3292e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 247/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.3463e-07 - acc: 1.0000\n",
      "Epoch 247: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.3463e-07 - acc: 1.0000 - val_loss: 1.3230e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 248/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 7.3555e-04 - acc: 0.9999\n",
      "Epoch 248: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.0739e-04 - acc: 0.9999 - val_loss: 2.2800e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 249/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.0044e-05 - acc: 1.0000\n",
      "Epoch 249: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.8513e-05 - acc: 1.0000 - val_loss: 1.4810e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 250/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.2076e-06 - acc: 1.0000\n",
      "Epoch 250: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.0301e-06 - acc: 1.0000 - val_loss: 1.4878e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 251/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.0322e-07 - acc: 1.0000\n",
      "Epoch 251: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.0322e-07 - acc: 1.0000 - val_loss: 1.4859e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 252/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 3.0762e-06 - acc: 1.0000\n",
      "Epoch 252: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.9705e-06 - acc: 1.0000 - val_loss: 1.9584e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 253/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 9.6329e-08 - acc: 1.0000\n",
      "Epoch 253: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.6077e-08 - acc: 1.0000 - val_loss: 1.9556e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 254/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 4.7286e-07 - acc: 1.0000\n",
      "Epoch 254: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.6107e-07 - acc: 1.0000 - val_loss: 2.0198e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 255/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 9.9612e-08 - acc: 1.0000\n",
      "Epoch 255: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.9021e-08 - acc: 1.0000 - val_loss: 2.0181e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 256/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.5731e-07 - acc: 1.0000\n",
      "Epoch 256: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.5494e-07 - acc: 1.0000 - val_loss: 2.0397e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 257/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 3.3967e-07 - acc: 1.0000\n",
      "Epoch 257: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.2742e-07 - acc: 1.0000 - val_loss: 2.0740e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 258/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 8.4887e-08 - acc: 1.0000\n",
      "Epoch 258: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 8.4887e-08 - acc: 1.0000 - val_loss: 2.0679e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 259/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 8.0332e-08 - acc: 1.0000\n",
      "Epoch 259: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 8.2122e-08 - acc: 1.0000 - val_loss: 2.0531e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 260/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.4551e-07 - acc: 1.0000\n",
      "Epoch 260: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.4551e-07 - acc: 1.0000 - val_loss: 2.1002e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 261/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.1538e-07 - acc: 1.0000\n",
      "Epoch 261: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.1418e-07 - acc: 1.0000 - val_loss: 2.1017e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 262/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 8.2483e-08 - acc: 1.0000\n",
      "Epoch 262: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.0895e-08 - acc: 1.0000 - val_loss: 2.0913e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 263/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.7973e-07 - acc: 1.0000\n",
      "Epoch 263: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.7707e-07 - acc: 1.0000 - val_loss: 2.0712e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 264/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.7051e-07 - acc: 1.0000\n",
      "Epoch 264: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6643e-07 - acc: 1.0000 - val_loss: 2.0637e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 265/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 6.5207e-08 - acc: 1.0000\n",
      "Epoch 265: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.5207e-08 - acc: 1.0000 - val_loss: 2.0633e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 266/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.5975e-05 - acc: 1.0000\n",
      "Epoch 266: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.5785e-05 - acc: 1.0000 - val_loss: 1.8822e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 267/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 9.7847e-08 - acc: 1.0000\n",
      "Epoch 267: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 9.7120e-08 - acc: 1.0000 - val_loss: 1.8950e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 268/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.0312e-07 - acc: 1.0000\n",
      "Epoch 268: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0281e-07 - acc: 1.0000 - val_loss: 1.9513e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 269/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.3592e-08 - acc: 1.0000\n",
      "Epoch 269: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.2288e-08 - acc: 1.0000 - val_loss: 1.9554e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 270/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 4.7132e-08 - acc: 1.0000\n",
      "Epoch 270: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.7081e-08 - acc: 1.0000 - val_loss: 1.9444e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 271/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.3231e-07 - acc: 1.0000\n",
      "Epoch 271: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.3231e-07 - acc: 1.0000 - val_loss: 1.9757e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 272/500\n",
      "222/228 [============================>.] - ETA: 0s - loss: 3.7756e-07 - acc: 1.0000\n",
      "Epoch 272: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.7967e-07 - acc: 1.0000 - val_loss: 1.6421e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 273/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 6.8597e-08 - acc: 1.0000\n",
      "Epoch 273: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 6.8380e-08 - acc: 1.0000 - val_loss: 1.6257e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 274/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 2.6193e-04 - acc: 0.9999\n",
      "Epoch 274: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.6111e-04 - acc: 0.9999 - val_loss: 9.9768e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 275/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 4.4118e-06 - acc: 1.0000\n",
      "Epoch 275: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.3785e-06 - acc: 1.0000 - val_loss: 1.0058e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 276/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 2.7382e-07 - acc: 1.0000\n",
      "Epoch 276: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.7210e-07 - acc: 1.0000 - val_loss: 9.5987e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 277/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.5314e-07 - acc: 1.0000\n",
      "Epoch 277: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.5314e-07 - acc: 1.0000 - val_loss: 9.6575e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 278/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.8676e-07 - acc: 1.0000\n",
      "Epoch 278: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.8010e-07 - acc: 1.0000 - val_loss: 9.5888e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 279/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.0716e-07 - acc: 1.0000\n",
      "Epoch 279: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.0320e-07 - acc: 1.0000 - val_loss: 9.5670e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 280/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 8.2040e-08 - acc: 1.0000\n",
      "Epoch 280: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.1729e-08 - acc: 1.0000 - val_loss: 9.5327e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 281/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 7.0638e-08 - acc: 1.0000\n",
      "Epoch 281: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 7.0638e-08 - acc: 1.0000 - val_loss: 9.4808e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 282/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 3.2898e-08 - acc: 1.0000\n",
      "Epoch 282: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.4027e-08 - acc: 1.0000 - val_loss: 9.4549e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 283/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 8.3969e-08 - acc: 1.0000\n",
      "Epoch 283: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.3969e-08 - acc: 1.0000 - val_loss: 9.3866e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 284/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.6459e-06 - acc: 1.0000\n",
      "Epoch 284: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.6459e-06 - acc: 1.0000 - val_loss: 9.1557e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 285/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.1990e-07 - acc: 1.0000\n",
      "Epoch 285: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.1990e-07 - acc: 1.0000 - val_loss: 9.0580e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 286/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 2.3205e-07 - acc: 1.0000\n",
      "Epoch 286: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.2659e-07 - acc: 1.0000 - val_loss: 9.1294e-07 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 287/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 5.3943e-06 - acc: 1.0000\n",
      "Epoch 287: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.2358e-06 - acc: 1.0000 - val_loss: 1.2622e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 288/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.0329e-08 - acc: 1.0000\n",
      "Epoch 288: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.1143e-08 - acc: 1.0000 - val_loss: 1.2538e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 289/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.3569e-07 - acc: 1.0000\n",
      "Epoch 289: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.2658e-07 - acc: 1.0000 - val_loss: 1.2490e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 290/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 8.8230e-08 - acc: 1.0000\n",
      "Epoch 290: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.2835e-08 - acc: 1.0000 - val_loss: 1.2386e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 291/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.6901e-06 - acc: 1.0000\n",
      "Epoch 291: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.5758e-06 - acc: 1.0000 - val_loss: 1.1548e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 292/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.3501e-07 - acc: 1.0000\n",
      "Epoch 292: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.2925e-07 - acc: 1.0000 - val_loss: 1.1396e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 293/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 3.9514e-06 - acc: 1.0000\n",
      "Epoch 293: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.8008e-06 - acc: 1.0000 - val_loss: 1.9544e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 294/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.4492e-06 - acc: 1.0000\n",
      "Epoch 294: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 294: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.3882e-06 - acc: 1.0000 - val_loss: 1.9710e-06 - val_acc: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 295/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.0355e-07 - acc: 1.0000\n",
      "Epoch 295: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4753e-07 - acc: 1.0000 - val_loss: 1.9432e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 296/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 2.3173e-07 - acc: 1.0000\n",
      "Epoch 296: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.3103e-07 - acc: 1.0000 - val_loss: 1.8704e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 297/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.5163e-06 - acc: 1.0000\n",
      "Epoch 297: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.5163e-06 - acc: 1.0000 - val_loss: 1.5466e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 298/500\n",
      "223/228 [============================>.] - ETA: 0s - loss: 8.3141e-08 - acc: 1.0000\n",
      "Epoch 298: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 8.1892e-08 - acc: 1.0000 - val_loss: 1.4498e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 299/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.2962e-07 - acc: 1.0000\n",
      "Epoch 299: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2674e-07 - acc: 1.0000 - val_loss: 1.4473e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 300/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 8.2276e-08 - acc: 1.0000\n",
      "Epoch 300: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.0550e-08 - acc: 1.0000 - val_loss: 1.4400e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 301/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 7.0010e-08 - acc: 1.0000\n",
      "Epoch 301: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.4187e-08 - acc: 1.0000 - val_loss: 1.4283e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 302/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 4.7751e-08 - acc: 1.0000\n",
      "Epoch 302: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.7408e-08 - acc: 1.0000 - val_loss: 1.4035e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 303/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.2816e-06 - acc: 1.0000\n",
      "Epoch 303: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2406e-06 - acc: 1.0000 - val_loss: 1.5267e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 304/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 3.8235e-08 - acc: 1.0000\n",
      "Epoch 304: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.5282e-08 - acc: 1.0000 - val_loss: 1.5084e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 305/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 5.0645e-06 - acc: 1.0000\n",
      "Epoch 305: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.8487e-06 - acc: 1.0000 - val_loss: 1.3736e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 306/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.7352e-08 - acc: 1.0000\n",
      "Epoch 306: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.7352e-08 - acc: 1.0000 - val_loss: 1.3717e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 307/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 7.2979e-08 - acc: 1.0000\n",
      "Epoch 307: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.2714e-08 - acc: 1.0000 - val_loss: 1.3559e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 308/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.3015e-06 - acc: 1.0000\n",
      "Epoch 308: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2581e-06 - acc: 1.0000 - val_loss: 1.2009e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 309/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 7.0132e-08 - acc: 1.0000\n",
      "Epoch 309: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.8494e-08 - acc: 1.0000 - val_loss: 1.2028e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 310/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 3.7213e-07 - acc: 1.0000\n",
      "Epoch 310: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.6647e-07 - acc: 1.0000 - val_loss: 1.3080e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 311/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.8919e-07 - acc: 1.0000\n",
      "Epoch 311: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.8919e-07 - acc: 1.0000 - val_loss: 1.3764e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 312/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.3716e-08 - acc: 1.0000\n",
      "Epoch 312: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3716e-08 - acc: 1.0000 - val_loss: 1.3695e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 313/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.9285e-07 - acc: 1.0000\n",
      "Epoch 313: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.9285e-07 - acc: 1.0000 - val_loss: 1.2258e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 314/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.5190e-08 - acc: 1.0000\n",
      "Epoch 314: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4789e-08 - acc: 1.0000 - val_loss: 1.2207e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 315/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 3.5687e-08 - acc: 1.0000\n",
      "Epoch 315: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.4615e-08 - acc: 1.0000 - val_loss: 1.2148e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 316/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 5.1380e-08 - acc: 1.0000\n",
      "Epoch 316: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.1380e-08 - acc: 1.0000 - val_loss: 1.2156e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 317/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 1.1102e-07 - acc: 1.0000\n",
      "Epoch 317: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1001e-07 - acc: 1.0000 - val_loss: 1.2310e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 318/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 8.6226e-06 - acc: 1.0000\n",
      "Epoch 318: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.4076e-06 - acc: 1.0000 - val_loss: 7.3993e-07 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 319/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.9788e-07 - acc: 1.0000\n",
      "Epoch 319: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.9788e-07 - acc: 1.0000 - val_loss: 7.3585e-07 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 320/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 4.0531e-07 - acc: 1.0000\n",
      "Epoch 320: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.0237e-07 - acc: 1.0000 - val_loss: 7.4138e-07 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 321/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0041e-07 - acc: 1.0000\n",
      "Epoch 321: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0041e-07 - acc: 1.0000 - val_loss: 7.4108e-07 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 322/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.3620e-07 - acc: 1.0000\n",
      "Epoch 322: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.3055e-07 - acc: 1.0000 - val_loss: 7.4062e-07 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 323/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 9.7129e-08 - acc: 1.0000\n",
      "Epoch 323: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 9.6822e-08 - acc: 1.0000 - val_loss: 7.4142e-07 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 324/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.6968e-06 - acc: 1.0000\n",
      "Epoch 324: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.4117e-06 - acc: 1.0000 - val_loss: 1.1650e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 325/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.5992e-08 - acc: 1.0000\n",
      "Epoch 325: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.5356e-08 - acc: 1.0000 - val_loss: 1.3181e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 326/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 3.5827e-07 - acc: 1.0000\n",
      "Epoch 326: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.4574e-07 - acc: 1.0000 - val_loss: 1.3269e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 327/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.8661e-07 - acc: 1.0000\n",
      "Epoch 327: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.7662e-07 - acc: 1.0000 - val_loss: 1.1931e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 328/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.9157e-08 - acc: 1.0000\n",
      "Epoch 328: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.9157e-08 - acc: 1.0000 - val_loss: 1.1877e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 329/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.1590e-08 - acc: 1.0000\n",
      "Epoch 329: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.1103e-08 - acc: 1.0000 - val_loss: 1.1766e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 330/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 4.2434e-08 - acc: 1.0000\n",
      "Epoch 330: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.3690e-08 - acc: 1.0000 - val_loss: 1.1679e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 331/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.8084e-08 - acc: 1.0000\n",
      "Epoch 331: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.8072e-08 - acc: 1.0000 - val_loss: 1.1568e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 332/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.4918e-08 - acc: 1.0000\n",
      "Epoch 332: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4772e-08 - acc: 1.0000 - val_loss: 1.1524e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 333/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 1.9141e-07 - acc: 1.0000\n",
      "Epoch 333: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.8596e-07 - acc: 1.0000 - val_loss: 1.1389e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 334/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.5560e-07 - acc: 1.0000\n",
      "Epoch 334: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.4846e-07 - acc: 1.0000 - val_loss: 1.1727e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 335/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 2.3295e-08 - acc: 1.0000\n",
      "Epoch 335: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.3344e-08 - acc: 1.0000 - val_loss: 1.1636e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 336/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.6384e-08 - acc: 1.0000\n",
      "Epoch 336: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.6305e-08 - acc: 1.0000 - val_loss: 1.1433e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 337/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 3.4871e-08 - acc: 1.0000\n",
      "Epoch 337: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3683e-08 - acc: 1.0000 - val_loss: 1.1423e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 338/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 5.1964e-08 - acc: 1.0000\n",
      "Epoch 338: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.0302e-08 - acc: 1.0000 - val_loss: 1.1376e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 339/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.7068e-08 - acc: 1.0000\n",
      "Epoch 339: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.6076e-08 - acc: 1.0000 - val_loss: 1.1141e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 340/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 9.0936e-08 - acc: 1.0000\n",
      "Epoch 340: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.7137e-08 - acc: 1.0000 - val_loss: 1.0909e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 341/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 1.7413e-08 - acc: 1.0000\n",
      "Epoch 341: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6981e-08 - acc: 1.0000 - val_loss: 1.0820e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 342/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 2.7032e-08 - acc: 1.0000\n",
      "Epoch 342: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.7041e-08 - acc: 1.0000 - val_loss: 1.0621e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 343/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 8.1110e-09 - acc: 1.0000\n",
      "Epoch 343: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.9996e-09 - acc: 1.0000 - val_loss: 1.0566e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 344/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 3.4343e-08 - acc: 1.0000\n",
      "Epoch 344: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 344: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3404e-08 - acc: 1.0000 - val_loss: 1.0123e-06 - val_acc: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 345/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.5212e-08 - acc: 1.0000\n",
      "Epoch 345: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.3546e-08 - acc: 1.0000 - val_loss: 1.0163e-06 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 346/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 8.1003e-08 - acc: 1.0000\n",
      "Epoch 346: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.8036e-08 - acc: 1.0000 - val_loss: 9.7159e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 347/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.3411e-08 - acc: 1.0000\n",
      "Epoch 347: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.3038e-08 - acc: 1.0000 - val_loss: 9.6533e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 348/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.4461e-08 - acc: 1.0000\n",
      "Epoch 348: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4461e-08 - acc: 1.0000 - val_loss: 9.5957e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 349/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.5476e-06 - acc: 1.0000\n",
      "Epoch 349: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 6.2817e-06 - acc: 1.0000 - val_loss: 7.1473e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 350/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 9.7480e-08 - acc: 1.0000\n",
      "Epoch 350: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 9.5921e-08 - acc: 1.0000 - val_loss: 6.8405e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 351/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.6267e-08 - acc: 1.0000\n",
      "Epoch 351: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.6267e-08 - acc: 1.0000 - val_loss: 6.7951e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 352/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 6.8730e-08 - acc: 1.0000\n",
      "Epoch 352: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 6.8211e-08 - acc: 1.0000 - val_loss: 6.8295e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 353/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 2.3715e-08 - acc: 1.0000\n",
      "Epoch 353: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.3360e-08 - acc: 1.0000 - val_loss: 6.7902e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 354/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 5.5942e-08 - acc: 1.0000\n",
      "Epoch 354: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.5044e-08 - acc: 1.0000 - val_loss: 6.7078e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 355/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 8.0241e-08 - acc: 1.0000\n",
      "Epoch 355: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.6947e-08 - acc: 1.0000 - val_loss: 6.8890e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 356/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.4818e-08 - acc: 1.0000\n",
      "Epoch 356: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.4162e-08 - acc: 1.0000 - val_loss: 6.6955e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 357/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.9108e-08 - acc: 1.0000\n",
      "Epoch 357: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.8268e-08 - acc: 1.0000 - val_loss: 6.6421e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 358/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.2590e-08 - acc: 1.0000\n",
      "Epoch 358: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.1839e-08 - acc: 1.0000 - val_loss: 6.6093e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 359/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 6.6791e-08 - acc: 1.0000\n",
      "Epoch 359: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.4953e-08 - acc: 1.0000 - val_loss: 6.5276e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 360/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.8917e-08 - acc: 1.0000\n",
      "Epoch 360: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.8191e-08 - acc: 1.0000 - val_loss: 6.3830e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 361/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.0026e-07 - acc: 1.0000\n",
      "Epoch 361: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.9994e-08 - acc: 1.0000 - val_loss: 6.4742e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 362/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.9819e-08 - acc: 1.0000\n",
      "Epoch 362: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.9135e-08 - acc: 1.0000 - val_loss: 6.5451e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 363/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.9377e-08 - acc: 1.0000\n",
      "Epoch 363: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.8416e-08 - acc: 1.0000 - val_loss: 6.4692e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 364/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 9.5258e-09 - acc: 1.0000\n",
      "Epoch 364: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.5865e-09 - acc: 1.0000 - val_loss: 6.3730e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 365/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.4081e-08 - acc: 1.0000\n",
      "Epoch 365: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.4036e-08 - acc: 1.0000 - val_loss: 6.2223e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 366/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.2864e-07 - acc: 1.0000\n",
      "Epoch 366: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2587e-07 - acc: 1.0000 - val_loss: 5.6233e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 367/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 6.6214e-08 - acc: 1.0000\n",
      "Epoch 367: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.3973e-08 - acc: 1.0000 - val_loss: 5.3215e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 368/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 5.1282e-08 - acc: 1.0000\n",
      "Epoch 368: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 5.1282e-08 - acc: 1.0000 - val_loss: 4.5774e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 369/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.4151e-07 - acc: 1.0000\n",
      "Epoch 369: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.2550e-07 - acc: 1.0000 - val_loss: 4.4801e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 370/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.0356e-07 - acc: 1.0000\n",
      "Epoch 370: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.9338e-08 - acc: 1.0000 - val_loss: 4.3614e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 371/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.2030e-08 - acc: 1.0000\n",
      "Epoch 371: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.1348e-08 - acc: 1.0000 - val_loss: 4.3774e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 372/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 2.2466e-08 - acc: 1.0000\n",
      "Epoch 372: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.2346e-08 - acc: 1.0000 - val_loss: 4.4827e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 373/500\n",
      "224/228 [============================>.] - ETA: 0s - loss: 2.6192e-08 - acc: 1.0000\n",
      "Epoch 373: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.5765e-08 - acc: 1.0000 - val_loss: 4.5342e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 374/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.2302e-07 - acc: 1.0000\n",
      "Epoch 374: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.2302e-07 - acc: 1.0000 - val_loss: 6.0436e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 375/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.3244e-08 - acc: 1.0000\n",
      "Epoch 375: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.3234e-08 - acc: 1.0000 - val_loss: 5.8033e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 376/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 3.7253e-09 - acc: 1.0000\n",
      "Epoch 376: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.6481e-09 - acc: 1.0000 - val_loss: 5.7911e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 377/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.6777e-08 - acc: 1.0000\n",
      "Epoch 377: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.5650e-08 - acc: 1.0000 - val_loss: 5.8147e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 378/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.3547e-09 - acc: 1.0000\n",
      "Epoch 378: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.2697e-09 - acc: 1.0000 - val_loss: 5.7815e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 379/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.4793e-06 - acc: 1.0000\n",
      "Epoch 379: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4162e-06 - acc: 1.0000 - val_loss: 5.9166e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 380/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 8.4062e-05 - acc: 1.0000\n",
      "Epoch 380: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.0475e-05 - acc: 1.0000 - val_loss: 7.9353e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 381/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 3.2790e-08 - acc: 1.0000\n",
      "Epoch 381: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.1407e-08 - acc: 1.0000 - val_loss: 8.0051e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 382/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 5.0969e-09 - acc: 1.0000\n",
      "Epoch 382: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.1858e-09 - acc: 1.0000 - val_loss: 7.9891e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 383/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 7.9268e-09 - acc: 1.0000\n",
      "Epoch 383: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.8851e-09 - acc: 1.0000 - val_loss: 7.9422e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 384/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 5.0753e-09 - acc: 1.0000\n",
      "Epoch 384: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.9568e-09 - acc: 1.0000 - val_loss: 7.9285e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 385/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.5278e-09 - acc: 1.0000\n",
      "Epoch 385: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.8054e-09 - acc: 1.0000 - val_loss: 7.8834e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 386/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.3277e-06 - acc: 1.0000\n",
      "Epoch 386: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2770e-06 - acc: 1.0000 - val_loss: 7.2727e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 387/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 4.5969e-09 - acc: 1.0000\n",
      "Epoch 387: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.5969e-09 - acc: 1.0000 - val_loss: 7.2669e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 388/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.7205e-07 - acc: 1.0000\n",
      "Epoch 388: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.7205e-07 - acc: 1.0000 - val_loss: 6.7622e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 389/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.5140e-08 - acc: 1.0000\n",
      "Epoch 389: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4707e-08 - acc: 1.0000 - val_loss: 6.6317e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 390/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.0236e-08 - acc: 1.0000\n",
      "Epoch 390: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.8481e-09 - acc: 1.0000 - val_loss: 6.5859e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 391/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 9.4407e-09 - acc: 1.0000\n",
      "Epoch 391: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.1283e-09 - acc: 1.0000 - val_loss: 6.5737e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 392/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.8553e-08 - acc: 1.0000\n",
      "Epoch 392: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.9723e-08 - acc: 1.0000 - val_loss: 6.5920e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 393/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.5961e-04 - acc: 0.9999\n",
      "Epoch 393: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.5961e-04 - acc: 0.9999 - val_loss: 4.1649e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 394/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 7.8418e-09 - acc: 1.0000\n",
      "Epoch 394: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "\n",
      "Epoch 394: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.7542e-09 - acc: 1.0000 - val_loss: 4.1584e-07 - val_acc: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 395/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 5.7133e-07 - acc: 1.0000\n",
      "Epoch 395: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.4701e-07 - acc: 1.0000 - val_loss: 4.1420e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 396/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 3.4704e-08 - acc: 1.0000\n",
      "Epoch 396: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3402e-08 - acc: 1.0000 - val_loss: 4.1378e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 397/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.7670e-09 - acc: 1.0000\n",
      "Epoch 397: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.3453e-09 - acc: 1.0000 - val_loss: 4.1367e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 398/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 5.6703e-09 - acc: 1.0000\n",
      "Epoch 398: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 5.6276e-09 - acc: 1.0000 - val_loss: 4.1329e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 399/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.0145e-09 - acc: 1.0000\n",
      "Epoch 399: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.9262e-09 - acc: 1.0000 - val_loss: 4.1298e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 400/500\n",
      "223/228 [============================>.] - ETA: 0s - loss: 1.0057e-08 - acc: 1.0000\n",
      "Epoch 400: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 9.9300e-09 - acc: 1.0000 - val_loss: 4.1291e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 401/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1271e-08 - acc: 1.0000\n",
      "Epoch 401: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1271e-08 - acc: 1.0000 - val_loss: 4.1279e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 402/500\n",
      "225/228 [============================>.] - ETA: 0s - loss: 5.0829e-09 - acc: 1.0000\n",
      "Epoch 402: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 9.7991e-09 - acc: 1.0000 - val_loss: 4.1237e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 403/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.0270e-08 - acc: 1.0000\n",
      "Epoch 403: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0094e-08 - acc: 1.0000 - val_loss: 4.1233e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 404/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 7.1277e-04 - acc: 0.9999\n",
      "Epoch 404: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.8235e-04 - acc: 0.9999 - val_loss: 3.3747e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 405/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 6.9370e-08 - acc: 1.0000\n",
      "Epoch 405: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.9370e-08 - acc: 1.0000 - val_loss: 3.3766e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 406/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.3757e-08 - acc: 1.0000\n",
      "Epoch 406: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.3001e-08 - acc: 1.0000 - val_loss: 3.3736e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 407/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1648e-08 - acc: 1.0000\n",
      "Epoch 407: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.1648e-08 - acc: 1.0000 - val_loss: 3.3713e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 408/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 8.2125e-09 - acc: 1.0000\n",
      "Epoch 408: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.9505e-09 - acc: 1.0000 - val_loss: 3.3713e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 409/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.7018e-08 - acc: 1.0000\n",
      "Epoch 409: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.2150e-08 - acc: 1.0000 - val_loss: 3.3701e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 410/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.0398e-06 - acc: 1.0000\n",
      "Epoch 410: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.9553e-07 - acc: 1.0000 - val_loss: 3.3827e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 411/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 4.3191e-08 - acc: 1.0000\n",
      "Epoch 411: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.2267e-08 - acc: 1.0000 - val_loss: 3.3816e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 412/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 6.2165e-09 - acc: 1.0000\n",
      "Epoch 412: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.2165e-09 - acc: 1.0000 - val_loss: 3.3808e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 413/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 5.8846e-09 - acc: 1.0000\n",
      "Epoch 413: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 5.8402e-09 - acc: 1.0000 - val_loss: 3.3801e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 414/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.0121e-08 - acc: 1.0000\n",
      "Epoch 414: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1042e-08 - acc: 1.0000 - val_loss: 3.3789e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 415/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 5.3339e-09 - acc: 1.0000\n",
      "Epoch 415: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.1695e-09 - acc: 1.0000 - val_loss: 3.3770e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 416/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.0580e-08 - acc: 1.0000\n",
      "Epoch 416: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0208e-08 - acc: 1.0000 - val_loss: 3.3770e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 417/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 3.0312e-07 - acc: 1.0000\n",
      "Epoch 417: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.9165e-07 - acc: 1.0000 - val_loss: 3.3766e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 418/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 3.1989e-08 - acc: 1.0000\n",
      "Epoch 418: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 3.0820e-08 - acc: 1.0000 - val_loss: 3.3694e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 419/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 6.2854e-09 - acc: 1.0000\n",
      "Epoch 419: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 6.2656e-09 - acc: 1.0000 - val_loss: 3.3663e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 420/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 9.9284e-09 - acc: 1.0000\n",
      "Epoch 420: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.5046e-09 - acc: 1.0000 - val_loss: 3.3644e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 421/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.6661e-08 - acc: 1.0000\n",
      "Epoch 421: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.5999e-08 - acc: 1.0000 - val_loss: 3.3640e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 422/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.3687e-08 - acc: 1.0000\n",
      "Epoch 422: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.3687e-08 - acc: 1.0000 - val_loss: 3.3610e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 423/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.2013e-08 - acc: 1.0000\n",
      "Epoch 423: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1860e-08 - acc: 1.0000 - val_loss: 3.3610e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 424/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.0164e-07 - acc: 1.0000\n",
      "Epoch 424: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.7611e-07 - acc: 1.0000 - val_loss: 3.3656e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 425/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 8.8339e-09 - acc: 1.0000\n",
      "Epoch 425: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.8339e-09 - acc: 1.0000 - val_loss: 3.3637e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 426/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0411e-07 - acc: 1.0000\n",
      "Epoch 426: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0411e-07 - acc: 1.0000 - val_loss: 3.3698e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 427/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.0796e-08 - acc: 1.0000\n",
      "Epoch 427: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.9958e-08 - acc: 1.0000 - val_loss: 3.3690e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 428/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 5.0922e-07 - acc: 1.0000\n",
      "Epoch 428: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.8750e-07 - acc: 1.0000 - val_loss: 3.3140e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 429/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 8.9985e-09 - acc: 1.0000\n",
      "Epoch 429: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.7194e-09 - acc: 1.0000 - val_loss: 3.3099e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 430/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 3.8566e-09 - acc: 1.0000\n",
      "Epoch 430: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.0898e-09 - acc: 1.0000 - val_loss: 3.3091e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 431/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 8.5543e-08 - acc: 1.0000\n",
      "Epoch 431: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.5273e-08 - acc: 1.0000 - val_loss: 3.3148e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 432/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 5.8443e-09 - acc: 1.0000\n",
      "Epoch 432: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.9711e-09 - acc: 1.0000 - val_loss: 3.3144e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 433/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 1.7025e-08 - acc: 1.0000\n",
      "Epoch 433: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6817e-08 - acc: 1.0000 - val_loss: 3.3129e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 434/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.0988e-08 - acc: 1.0000\n",
      "Epoch 434: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0633e-08 - acc: 1.0000 - val_loss: 3.3072e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 435/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.1447e-08 - acc: 1.0000\n",
      "Epoch 435: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 1.1124e-08 - acc: 1.0000 - val_loss: 3.3060e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 436/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.5742e-08 - acc: 1.0000\n",
      "Epoch 436: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.5742e-08 - acc: 1.0000 - val_loss: 3.3095e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 437/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.0151e-09 - acc: 1.0000\n",
      "Epoch 437: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.8729e-09 - acc: 1.0000 - val_loss: 3.3015e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 438/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 2.5583e-08 - acc: 1.0000\n",
      "Epoch 438: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.4702e-08 - acc: 1.0000 - val_loss: 3.3007e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 439/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.2030e-08 - acc: 1.0000\n",
      "Epoch 439: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1664e-08 - acc: 1.0000 - val_loss: 3.2984e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 440/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.0072e-08 - acc: 1.0000\n",
      "Epoch 440: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 2.0072e-08 - acc: 1.0000 - val_loss: 3.2953e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 441/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 7.7752e-09 - acc: 1.0000\n",
      "Epoch 441: val_loss improved from 0.00000 to 0.00000, saving model to models\\model.h5\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.7378e-09 - acc: 1.0000 - val_loss: 3.2908e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 442/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.3732e-07 - acc: 1.0000\n",
      "Epoch 442: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.3206e-07 - acc: 1.0000 - val_loss: 3.3678e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 443/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 1.5877e-05 - acc: 1.0000\n",
      "Epoch 443: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.5339e-05 - acc: 1.0000 - val_loss: 3.3598e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 444/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 3.5817e-05 - acc: 1.0000\n",
      "Epoch 444: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 444: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.4603e-05 - acc: 1.0000 - val_loss: 4.1943e-07 - val_acc: 1.0000 - lr: 7.8125e-06\n",
      "Epoch 445/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.4690e-08 - acc: 1.0000\n",
      "Epoch 445: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4690e-08 - acc: 1.0000 - val_loss: 4.1909e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 446/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1909e-08 - acc: 1.0000\n",
      "Epoch 446: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1909e-08 - acc: 1.0000 - val_loss: 4.1871e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 447/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.4609e-08 - acc: 1.0000\n",
      "Epoch 447: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.4609e-08 - acc: 1.0000 - val_loss: 4.1882e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 448/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.8058e-07 - acc: 1.0000\n",
      "Epoch 448: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.5181e-07 - acc: 1.0000 - val_loss: 4.1894e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 449/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 7.3151e-09 - acc: 1.0000\n",
      "Epoch 449: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.1326e-09 - acc: 1.0000 - val_loss: 4.1882e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 450/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 9.5333e-09 - acc: 1.0000\n",
      "Epoch 450: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.2429e-09 - acc: 1.0000 - val_loss: 4.1890e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 451/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 4.6296e-09 - acc: 1.0000\n",
      "Epoch 451: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.6296e-09 - acc: 1.0000 - val_loss: 4.1882e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 452/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 8.9648e-09 - acc: 1.0000\n",
      "Epoch 452: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.9648e-09 - acc: 1.0000 - val_loss: 4.1871e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 453/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 7.2454e-08 - acc: 1.0000\n",
      "Epoch 453: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.0349e-08 - acc: 1.0000 - val_loss: 4.1836e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 454/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.8612e-08 - acc: 1.0000\n",
      "Epoch 454: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.6866e-08 - acc: 1.0000 - val_loss: 4.1749e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 455/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.7621e-08 - acc: 1.0000\n",
      "Epoch 455: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.7488e-08 - acc: 1.0000 - val_loss: 4.1768e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 456/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 3.5373e-09 - acc: 1.0000\n",
      "Epoch 456: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.4354e-09 - acc: 1.0000 - val_loss: 4.1764e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 457/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.4397e-09 - acc: 1.0000\n",
      "Epoch 457: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.2697e-09 - acc: 1.0000 - val_loss: 4.1753e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 458/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 6.8552e-09 - acc: 1.0000\n",
      "Epoch 458: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.1326e-09 - acc: 1.0000 - val_loss: 4.1730e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 459/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 8.1650e-09 - acc: 1.0000\n",
      "Epoch 459: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.3595e-09 - acc: 1.0000 - val_loss: 4.1699e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 460/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 4.4703e-09 - acc: 1.0000\n",
      "Epoch 460: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.3515e-09 - acc: 1.0000 - val_loss: 4.1680e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 461/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 8.8176e-09 - acc: 1.0000\n",
      "Epoch 461: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.6213e-09 - acc: 1.0000 - val_loss: 4.1642e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 462/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 6.6208e-09 - acc: 1.0000\n",
      "Epoch 462: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.5600e-09 - acc: 1.0000 - val_loss: 4.1650e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 463/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 5.0877e-09 - acc: 1.0000\n",
      "Epoch 463: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.0877e-09 - acc: 1.0000 - val_loss: 4.1630e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 464/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2728e-06 - acc: 1.0000\n",
      "Epoch 464: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2728e-06 - acc: 1.0000 - val_loss: 3.9524e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 465/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.3547e-09 - acc: 1.0000\n",
      "Epoch 465: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.3515e-09 - acc: 1.0000 - val_loss: 3.9410e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 466/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 2.7512e-09 - acc: 1.0000\n",
      "Epoch 466: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.6502e-09 - acc: 1.0000 - val_loss: 3.9402e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 467/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 2.3790e-08 - acc: 1.0000\n",
      "Epoch 467: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.3475e-08 - acc: 1.0000 - val_loss: 3.9402e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 468/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.9892e-09 - acc: 1.0000\n",
      "Epoch 468: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.5906e-09 - acc: 1.0000 - val_loss: 3.9398e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 469/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 3.4027e-09 - acc: 1.0000\n",
      "Epoch 469: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.4027e-09 - acc: 1.0000 - val_loss: 3.9394e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 470/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 5.7234e-09 - acc: 1.0000\n",
      "Epoch 470: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.5621e-09 - acc: 1.0000 - val_loss: 3.9391e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 471/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.5634e-08 - acc: 1.0000\n",
      "Epoch 471: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.5634e-08 - acc: 1.0000 - val_loss: 3.9387e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 472/500\n",
      "220/228 [===========================>..] - ETA: 0s - loss: 8.0093e-09 - acc: 1.0000\n",
      "Epoch 472: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.9342e-09 - acc: 1.0000 - val_loss: 3.9387e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 473/500\n",
      "226/228 [============================>.] - ETA: 0s - loss: 3.4012e-07 - acc: 1.0000\n",
      "Epoch 473: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.3759e-07 - acc: 1.0000 - val_loss: 3.9707e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 474/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 8.2537e-09 - acc: 1.0000\n",
      "Epoch 474: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.3431e-09 - acc: 1.0000 - val_loss: 3.9688e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 475/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.7346e-07 - acc: 1.0000\n",
      "Epoch 475: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.5360e-07 - acc: 1.0000 - val_loss: 4.1115e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 476/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.0897e-08 - acc: 1.0000\n",
      "Epoch 476: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0862e-08 - acc: 1.0000 - val_loss: 4.1081e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 477/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 8.0629e-09 - acc: 1.0000\n",
      "Epoch 477: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 8.1141e-09 - acc: 1.0000 - val_loss: 4.1070e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 478/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 6.7952e-08 - acc: 1.0000\n",
      "Epoch 478: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.5085e-08 - acc: 1.0000 - val_loss: 4.1165e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 479/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1836e-06 - acc: 1.0000\n",
      "Epoch 479: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.1836e-06 - acc: 1.0000 - val_loss: 4.2558e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 480/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 1.2264e-08 - acc: 1.0000\n",
      "Epoch 480: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.2057e-08 - acc: 1.0000 - val_loss: 4.2535e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 481/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0928e-08 - acc: 1.0000\n",
      "Epoch 481: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0928e-08 - acc: 1.0000 - val_loss: 4.2531e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 482/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.7051e-08 - acc: 1.0000\n",
      "Epoch 482: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6997e-08 - acc: 1.0000 - val_loss: 4.2561e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 483/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 7.0234e-09 - acc: 1.0000\n",
      "Epoch 483: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.8545e-09 - acc: 1.0000 - val_loss: 4.2500e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 484/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 7.4115e-08 - acc: 1.0000\n",
      "Epoch 484: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.1359e-08 - acc: 1.0000 - val_loss: 4.1810e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 485/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 9.5182e-09 - acc: 1.0000\n",
      "Epoch 485: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 9.3247e-09 - acc: 1.0000 - val_loss: 4.1730e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 486/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 2.7827e-07 - acc: 1.0000\n",
      "Epoch 486: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 2.7739e-07 - acc: 1.0000 - val_loss: 3.8990e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 487/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 6.3744e-08 - acc: 1.0000\n",
      "Epoch 487: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 6.1827e-08 - acc: 1.0000 - val_loss: 3.8063e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 488/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 7.4506e-09 - acc: 1.0000\n",
      "Epoch 488: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.1817e-09 - acc: 1.0000 - val_loss: 3.7998e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 489/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 4.0863e-09 - acc: 1.0000\n",
      "Epoch 489: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.0734e-09 - acc: 1.0000 - val_loss: 3.7964e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 490/500\n",
      "219/228 [===========================>..] - ETA: 0s - loss: 4.7289e-09 - acc: 1.0000\n",
      "Epoch 490: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.7442e-09 - acc: 1.0000 - val_loss: 3.7941e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 491/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.1309e-07 - acc: 1.0000\n",
      "Epoch 491: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.0846e-07 - acc: 1.0000 - val_loss: 3.8093e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 492/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 1.6661e-08 - acc: 1.0000\n",
      "Epoch 492: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 1.6064e-08 - acc: 1.0000 - val_loss: 3.8158e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 493/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 4.3161e-09 - acc: 1.0000\n",
      "Epoch 493: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 4.3025e-09 - acc: 1.0000 - val_loss: 3.8120e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 494/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 3.9987e-09 - acc: 1.0000\n",
      "Epoch 494: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 494: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.9916e-09 - acc: 1.0000 - val_loss: 3.8074e-07 - val_acc: 1.0000 - lr: 3.9063e-06\n",
      "Epoch 495/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 4.1520e-09 - acc: 1.0000\n",
      "Epoch 495: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.1389e-09 - acc: 1.0000 - val_loss: 3.8055e-07 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 496/500\n",
      "228/228 [==============================] - ETA: 0s - loss: 8.9157e-09 - acc: 1.0000\n",
      "Epoch 496: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 8.9157e-09 - acc: 1.0000 - val_loss: 3.8013e-07 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 497/500\n",
      "218/228 [===========================>..] - ETA: 0s - loss: 4.2721e-09 - acc: 1.0000\n",
      "Epoch 497: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 4.3188e-09 - acc: 1.0000 - val_loss: 3.7983e-07 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 498/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 3.8918e-08 - acc: 1.0000\n",
      "Epoch 498: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 3.7786e-08 - acc: 1.0000 - val_loss: 3.8448e-07 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 499/500\n",
      "227/228 [============================>.] - ETA: 0s - loss: 7.5818e-09 - acc: 1.0000\n",
      "Epoch 499: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 7.5579e-09 - acc: 1.0000 - val_loss: 3.8437e-07 - val_acc: 1.0000 - lr: 1.9531e-06\n",
      "Epoch 500/500\n",
      "221/228 [============================>.] - ETA: 0s - loss: 5.8323e-09 - acc: 1.0000\n",
      "Epoch 500: val_loss did not improve from 0.00000\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 5.7748e-09 - acc: 1.0000 - val_loss: 3.8402e-07 - val_acc: 1.0000 - lr: 1.9531e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=500,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[2618,    1],\n",
       "        [   4,  799]],\n",
       "\n",
       "       [[3127,    0],\n",
       "        [   0,  295]],\n",
       "\n",
       "       [[2702,    4],\n",
       "        [   0,  716]],\n",
       "\n",
       "       [[1814,    0],\n",
       "        [   1, 1607]]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
